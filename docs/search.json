[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction à l’analyse biographique des durées",
    "section": "",
    "text": "1 Présentation - Bibliographie - Outils"
  },
  {
    "objectID": "index.html#le-support",
    "href": "index.html#le-support",
    "title": "Introduction à l’analyse biographique des durées",
    "section": "Le support",
    "text": "Le support\nMISE A JOUR DU SUPPORT EN COURS\n\nLe support est maintenant intégralement disponible en format pdf en cliquant sur l’icône au dessus de la barre de recherche.\nD’ici la fin septembre 2023:\n\nUne introduction du support à cet endroit.\nQuelques ajouts:\n\nUn nouveau chapitre sur la manipulation des données [OK].\nPar l’exemple, un warning assez soutenu sur des utilisations douteuses de données prospectives (on visera l’EDP - Echantillon Démographique Permanent).\nJ’avais retiré la courte présentation des fonctions de lien probit et surtout complémentaire-loglog dans les modèles à durée discrète….maivaise idée que j’ai eu (voir point précédent).\nJe ne pense pas avoir le temps mais on sait jamais: une première présentation des modèles à pseudo observations dans la section annexe (sinon en fin d’année ou à l’été prochain).\n\n\n\nVersion pdf:\nPour le chapitre programmation, seulement la section dédiée à R a été ajoutée. Pour les 3 autres applications, seulement la version HTML est disponible."
  },
  {
    "objectID": "index.html#bibliographie",
    "href": "index.html#bibliographie",
    "title": "Introduction à l’analyse biographique des durées",
    "section": "Bibliographie",
    "text": "Bibliographie\nLes éléments bibliographiques qui figurent ci-dessous proviennent du champ des sciences sociales. Elle est courte, mais efficace. Quelle que soit la langue, le nombre de cours ou support sont très nombreux en médecine, qui est ici l’espace privilégié de l’ingénierie méthodologique. On trouve également de (trop) nombreux tutoriels à dominante mise en pratique avec R.\nAccès en ligne\n\nCours Gilbert Colletaz (Université d’Orléans - Master d’économétrie).\n\nLe cours est mis à jour tous les ans, applications uniquement avec Sas.\nDernière version 2020: lien\n\nDocument de travail de Simon Quantin (Insee).\n\nCouvre l’ensemble des techniques de base d’analyse des durées en durée dite continue. Il propose surement la meilleure introduction en langue française à la problématique de la fragilité.\nApplication en R seulement (attention au passage de la v3 du package survival)\n2019 - pas de mise à jour: lien\n\nLes notes de cours de German Rodriguez (en)\n\nDémographe à l’université de Princeton.\nLes dernières mises à jour doivent dater de 2017-2018: lien\n\n\nOuvrage de référence en démographie:\n\nL’analyse démographique des biographies de Daniel Courgeau et Eva Lelièvre (Edition de l’Ined - 1989). Malheureusement cet ouvrage ne dispose pas de version epub ou pdf disponible en ligne 1."
  },
  {
    "objectID": "index.html#outils",
    "href": "index.html#outils",
    "title": "Introduction à l’analyse biographique des durées",
    "section": "Outils",
    "text": "Outils\n\nSupport réalisé sous Rstudio avec l’outil d’édition Quarto\nLangages utilisés pour la partie programmation:\n\nR\nStata v18\nSas\nPython"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introduction à l’analyse biographique des durées",
    "section": "",
    "text": "Pour les résident.e.s du campus Condorcet, l’ouvrage est disponible au GED [lien]↩︎"
  },
  {
    "objectID": "01-presentation.html#questions",
    "href": "01-presentation.html#questions",
    "title": "2  L’analyse biographique des durées",
    "section": "2.1 Questions",
    "text": "2.1 Questions\nOn dispose de données dites “longitudinales”, et on cherche à appréhender l’occurence d’un évènement au sein d’une population. Les problématiques se basent sur les questions suivantes:\n\nObserve-t-on la survenue de l’évènement pour l’ensemble des individus?\nQuelle est la durée jusqu’à la survenue de l’évènement?\nQuels sont les facteurs qui favorisent la survenue de cet évènement? Facteurs fixes ou facteurs pouvant apparaitre/changer au cours de la période d’observation: variables dynamiques (TVC: Time Varying Covariate)"
  },
  {
    "objectID": "01-presentation.html#terminologies",
    "href": "01-presentation.html#terminologies",
    "title": "2  L’analyse biographique des durées",
    "section": "2.2 Terminologies",
    "text": "2.2 Terminologies\n\n\n\nFrançais\nAnglais\n\n\n\n\nAnalyse des durées\nDuration analysis\n\n\nAnalyse de survie/séjour\nSurvival analysis\n\n\nAnalyse de fiabilité\nFailure time data analysis\n\n\nAnalyse des transitions\nEvent-history analysis\n\n\n\nPour ce support, le choix de son titre me pose toujours problème pour éviter qu’il soit trop à rallonge. Si j’avais à trancher, il devrait un peu s’éterniser sous l’appelation Introduction à l’analyse biographique des durées en présence de données censurée (à droite1)."
  },
  {
    "objectID": "01-presentation.html#exemples-danalyse",
    "href": "01-presentation.html#exemples-danalyse",
    "title": "2  L’analyse biographique des durées",
    "section": "2.3 Exemples d’analyse",
    "text": "2.3 Exemples d’analyse\n\nNuptialité, Mise en couple: cohabiter, décohabiter, se marier, Rompre une union …\nLogement: Changement de statut (locataire &lt;=&gt; propriétaire), mobilité résidentielle/migration …\nEmploi: Trouver un 1er emploi, changer d’emploi, entrée ou sortie du chômage …\nFécondité: Avoir un premier enfant, avoir un nouvel enfant …\nMortalité: Décéder après diagnostic, survivre après l’administration un traitement, rechute…"
  },
  {
    "objectID": "01-presentation.html#elements-nécessaire-à-lanalyse",
    "href": "01-presentation.html#elements-nécessaire-à-lanalyse",
    "title": "2  L’analyse biographique des durées",
    "section": "2.4 Elements nécessaire à l’analyse",
    "text": "2.4 Elements nécessaire à l’analyse\n\nUn processus temporel\n\nUne échelle de mesure ou métrique temprelle: minutes, heures, jours, mois, années….\nUne origine commune définissant un évènement de départ 2: naissance, mariage si on analyse la séparation, …..\nUne définition précise de l’évènement d’étude.\n\nUne durée entre le début et la fin de la période d’observation, si nécessaire avec la fin de la période d’exposition au risque. Cette durée doit être généralement calculée à l’aide des informations de datation.\n\nUne population soumise au risque de connaître l’évènement (Risk Set)\nDes variables explicatives ou covariables\n\nFixes: sexe/genre, génération, niveau de diplôme le plus élevé,……\nDynamiques (TVC: Time varying covariates):\n\nMesurées à tout moment entre le début et la sortie de l’observation: statut matrimonial, taille du ménage, statut d’activité…\nPour les modèles à l’exception du semi-paramétrique de Cox, en présence de données censurées la durée ou une transformation de celle-ci est une variable dynamique introduite comme variable indépendante pour assurer le bon ajustement des données. L’introduction directe d’une fonction de la durée comme variable dépendante seule ne peut se faire qu’en absence d’observation censurée, en particulier à droite. Quelle que soit leur forme c’est une caractéristique propre aux modèles pleinement paramétriques."
  },
  {
    "objectID": "01-presentation.html#footnotes",
    "href": "01-presentation.html#footnotes",
    "title": "2  L’analyse biographique des durées",
    "section": "",
    "text": "ce qui est déjà bien suffisant↩︎\nAttention, dans le cadre des données prospectives ou de suivi, cela ne peut pas être le moment de l’inclusion à la base données↩︎"
  },
  {
    "objectID": "02-donnees.html#données-prospectives-et-rétrospectives",
    "href": "02-donnees.html#données-prospectives-et-rétrospectives",
    "title": "3  Les Données",
    "section": "3.1 Données prospectives et rétrospectives",
    "text": "3.1 Données prospectives et rétrospectives\n\n3.1.1 Les données prospectives\n\nIndividus suivis à des dates successives. On parle souvent de données de stock mises à jour à intervalle de temps plus ou moins réguliers.\nInstrument de mesure identique à chaque vague (si possible).\nAvantages:\nQualité des données et techniquement l’absence de biais de mémoire1.\nSi le suivi est pérenne un même analyse peut être répliquée à intervalles réguliés.\nInconvénients:\nDélais pour les exploiter dans une analyse. Mais à minima deux points d’observations permettent déjà sur une exposition certes très courte, de présenter des résultats.\n\nMêmes hypothèses entre deux passages pas forcément respectées\nAttrition, censure ou troncature à gauche liés aux âges d’inclusion. C’est sans aucun doute le plus gros problème, et ces phénomènes demande une vigilance extrême. Sans connaissance des principes de base en analyse des durées ou de survie, on peut être amené à réaliser, évaluer ou lire des études que l’on pourrait qualifier ici de Canada dry.\n\nA noter l’exploitation croissante des données administratives qui peuvent regorger d’informations biographiques. Déjà disponibles, le problème du coût de collecte peut-être est contourné2. Ce type de données comprend par exemple les informations issues des fichiers des Ressources Humaines des entreprises, qui ont été exploitées à l’Ined dans le cadre du projet « worklife » (https://worklife.site.ined.fr/). Une des sources de plus en plus utilisée en France est maintenant l’EDP3. Elles engendrent en revanche des questionnements techniques liés à l’inférence (on ne travaille directement pas sur des échantillons), et à une présence potentiellement massive de problèmes de censures à gauche ou par intervalles, ou de troncature à gauche4.\nUn exemple de mauvaise pratique avec ce type de données sera développé plus loin dans la section relative aux censures et aux troncatures.\n\n\n3.1.2 Les données rétrospectives\n\nIndividus interrogés une seule fois.\nRecueil de biographies thématiques depuis une origine jusqu’au moment de l’enquête.\nRecueil d’informations complémentaires à la date de l’enquête (âge, sexe, csp au moment de l’enquête et/ou csp représentative).\nAvantages: Information longitudinale immédiatement disponible.\nInconvénients: questionnaire long, informations datées qui font appel à la mémoire de l’enquêté.e. A de rares exceptions (enfant, mariage), il est difficile d’aller chercher des datations trop fines avec une retrospectivité assez longue.\n\nLes deux types de recueil peuvent être mixés avec des enquêtes à passages répétés comprenant des informations retrospectives entre 2 vagues. Par exemple la cohorte Elfe de l’Ined-Inserm ou la Millenium-Cohort-Study en Grande Bretagne5."
  },
  {
    "objectID": "02-donnees.html#grille-ageven",
    "href": "02-donnees.html#grille-ageven",
    "title": "3  Les Données",
    "section": "3.2 Grille AGEVEN",
    "text": "3.2 Grille AGEVEN\nPour recueillir des informations biographiques retrospectives, on utilise généralement la méthode des grilles AGEVEN.\nIl s’agit d’une grille âge-évènement, de type chronologique, avec des repères temporels en ligne (âge, année). En colonne, sont complétés de manière progressive et relative, les évènements relatifs à des domaines, par exemple la biographie professionnelle, familiale, résidentielle…\n\n\n\n\n\n\nRéférences\n\n\n\n\nAntoine P., X. Bry and P.D. Diouf, 1987 “La fiche Ageven : un outil pour la collecte des données rétrospectives”, Statistiques Canada 13(2).\nVivier G, “Comment collecter des biographies ? De la fiche Ageven aux grilles biographiques, Principes de collecte et Innovations récentes”, Acte des colloques de l’AIDELF, 2006.\nGRAB, 1999, “Biographies d’enquêtes : bilan de 14 collectes biographiques”, Paris, INED.\n\nExemple grille Ageven page 121: &lt;http://retro.erudit.org/livre/aidelf/2006/001404co.pdf&lt;"
  },
  {
    "objectID": "02-donnees.html#enregistrement-des-données",
    "href": "02-donnees.html#enregistrement-des-données",
    "title": "3  Les Données",
    "section": "3.3 Enregistrement des données",
    "text": "3.3 Enregistrement des données\nLa question du format des fichiers biographiques mis à disposition n’est pas neutre, en particulier au niveau des manipulations pour créer le fichier d’analyse, opération qui pourra s’avérer particulièrement chronophage et complexe si plusieurs modules doivent être appariés. On distingue trois formats d’enregistrement.\n\n3.3.1 Large [format individu]\nUne ligne par individu, qui renseigne sur une même ligne tous les évènements liés à un domaine : les datations et les caractéristiques des évènements.\nExemple: domaine : unions - échelle temporelle: année - fin de l’observation en 1986:\n\n\n\nid\ndebut1\nfin1\ncause1\ndébut2\nfin2\ncause2\n\n\n\n\nA\n1979\n1982\ndécès conjoint\n1985\n.\n.\n\n\nB\n1983\n1984\nSéparation\n.\n.\n.\n\n\n\nInconvénients: peut générer beaucoup de vecteurs colonnes avec de nombreuses valeurs manquantes. Le nombre de colonnes va dépendre du nombre maximum d’évènements. Si ce nombre concerne un seul individu, on va multiplier le nombre de colonnes pour un niveau d’information très limité. Situation classique, le nombre d’enfants, où les naissances de rang élevé deviennent de plus en plus rares.\n\n\n3.3.2 Semi-long [format individu-évènements]\nC’est le format le plus courant de mise à disposition des enquêtes biographiques. Si les transitions sont de type continu, par exemple le lieu de résidence (on habite toujours quelque part), la date de fin de la séquence correspond à la date de début de la séquence suivante. Les dates de fin ne sont pas forcément renseignées sur une ligne pour des trajectoires continues, l’information peut être donnée sur la ligne suivante avec la date de début.\nPour la séquence qui se déroule au moment de l’enquête, la date de fin est souvent une valeur manquante, une fin de séquence pouvant se produire juste avant l’enquête au cours d’une même année. Il est également possible d’avoir une information qui ne s’est pas encore produite au moment de l’enquête, mais qui aura lieu peu de temps après (personne enceinte, donc une naissance probable la même année).\nExemple précédent (trajectoires discontinues):\n\n\n\nid\ndebut\nfin\ncause\nNumero séquence\n\n\n\n\nA\n1979\n1982\ndécès conjoint\n1\n\n\nA\n1985\n.\n.\n2\n\n\nB\n1983\n1984\nSéparation\n1\n\n\n\n\n\n3.3.3 Long [format individu-périodes]\nTypique des recueils prospectifs. Ils engendrent des lignes sans information supplémentaire par rapport à la ligne précédente.\nExemple précédent:\n\n\n\nid\nAnnée\ncause\nNumero séquence\n\n\n\n\nA\n1979\n.\n1\n\n\nA\n1980\n.\n1\n\n\nA\n1981\n.\n1\n\n\nA\n1982\nDécès conjoint\n1\n\n\nA\n1985\n.\n2\n\n\nA\n1986\n.\n2\n\n\nB\n1983\n.\n1\n\n\nB\n1984\nSéparation\n1\n\n\n\nIci les trajectoires ne sont pas continues. Une forme continue présenterait toute la trajectoire, avec l’ajout d’un statut du type être en couple ou non. Pour ID=A, en 1983 et 1984, deux lignes « pas couple » (cohabitant ou non) pourraient être insérées avec au total 3 séquences.\n Remarque : pour certaines analyses (par exemple analyse en temps discret), on doit transformer passer d’un format large ou semi-long à un format long, sur les durées observées ou sur des intervalles de durées construits."
  },
  {
    "objectID": "02-donnees.html#exemples-de-mise-à-disposition",
    "href": "02-donnees.html#exemples-de-mise-à-disposition",
    "title": "3  Les Données",
    "section": "3.4 Exemples de mise à disposition",
    "text": "3.4 Exemples de mise à disposition\nDeux enquêtes biographiques de type rétrospectives produite par l’Ined, avec un fichier qui fournit des informations générales sur les individus (une ligne par individu), et une série de modules biographiques en format individus-évènements.\n\n3.4.1 Enquête biographie et entourage (Ined)\nhttps://grab.site.ined.fr/fr/enquetes/france/biographie_entourage/\n\n\n\nBiographie et entourage: base caractéristiques individuelle\n\n\n\n\n\nBiographie et entourage: base biographique logements\n\n\n\n\n3.4.2 Enquête MAFE (Ined)\n\n\n\nMAFE: base caractéristiques individuelles\n\n\n\n\n\nMAFE: base biographique logement\n\n\nQuelques éléments de manipulation de ce type de données biographiques sont présentés dans le chapitre compléments6 [lien]"
  },
  {
    "objectID": "02-donnees.html#footnotes",
    "href": "02-donnees.html#footnotes",
    "title": "3  Les Données",
    "section": "",
    "text": "Cet avantage peut se trouver contrebalancé par des phénomènes de censure par intervalles, donc de trous tout aussi problématique que ceux liés à la mémoire↩︎\nJe ne suis par forcément à l’aise avec cet argument souvent avancé. La maintenance et l’alimentation de ce type de données peut être également coûteuse, ne se faisant pas par magie, comme pour l’EDP de l’Insee. Malheureusement cet argument est sûrement une des raisons qui expliquent les problèmes de financement des enquêtes retrospective↩︎\nEchantillon Démographique Permanent. Un bon exemple de données administrative dont le coût de production est loin d’être négligeable↩︎\nSe reporter par exemple à la présentation du très rigoureux guide de l’utilisateur de l’EDP: Les informations disponibles : des informations à géométrie variable et à trous↩︎\nPour avoir exploité ces données, cette cohorte souffre contrairement à Elfe de changement assez récurrent d’un passage à un autre (parait-il lié à ses changements de direction, chacune souhaitant laisser sa “patte”). En revanche elle a le mérite de s’être investi sur la récupération de l’attrition au cours du temps. Cela rend les données difficile à exploiter dans un cadre longitudinal, mais cela à le mérite de donner une certaine stabilisation de l’échantillon de départ↩︎\nIl s’agit d’un premier jet réalisé pour la version 2023 et qui ne peut pas viser l’exhaustivité↩︎"
  },
  {
    "objectID": "03-theorie.html",
    "href": "03-theorie.html",
    "title": "4  lightbox: auto",
    "section": "",
    "text": "5 La théorie\nL’analyse des durées peut être vue comme l’étude d’une variable aléatoire \\(T\\) qui décrit la durée d’attente jusqu’à l’occurence d’un évènement.\nLa principale caractéristique de l’analyse des durées est le traitement des informations dites censurées, lorsque la durée d’observation est plus courte que la durée d’exposition au risque."
  },
  {
    "objectID": "03-theorie.html#temps-et-durée",
    "href": "03-theorie.html#temps-et-durée",
    "title": "4  La théorie",
    "section": "4.1 Temps et durée",
    "text": "4.1 Temps et durée\nLe temps est une dimension (la quatrième), la durée est sa mesure. La durée est tout simplement calculée par la différence, pour une échelle temporelle donnée, entre la fin et le début d’une période d’exposition ou d’observation.\nOn distingue généralement deux types de mesure de la durée : continue et discrète/groupée. Ces deux notions ne possèdent pas réellement de définition, la différence s’explique plutôt par la présence ou non de simultanéité dans l’occurrence des évènements. Le temps étant intrasèquement continu car deux évènements ne peuvent pas avoir lieu en « même temps ». C’est donc l’échelle temporelle choisie ou imposée par l’analyse et les données qui pourra rendre cette mesure continue ou discrète/groupée. Pour un physicien travaillant sur la théorie de la relativité avec des horloges atomiques, une minute (voire une seconde) est une mesure très groupée pour ne pas dire grossière du temps, alors que pour un géologue c’est une mesure continue. Pour ces deux disciplines, cette échelle de mesure n’est pas adaptée à leur domaine. Le choix de l’échelle temporelle doit être pertinent par rapport aux objectifs de l’analyse même si on dispose des informations très fines (dates de naissance exactes). Etudier la fécondité avec une métrique journalière n’aurait pas de sens.\nIl existe des situations où les durées sont par nature discrète, lorsqu’un évènement ne peut avoir lieu qu’à un moment précis (date d’anniversaire des contrats pour l’analyse des résiliation). Généralement dans les sciences sociales avec un recueil de données de type rétrospectif, les mesures dites discrètes sont plutôt de nature groupées. Pour une même année, on considèrera indifféremment des évènements qui se produiront un premier janvier et un 31 décembre d’une même année.\n\n\n\n\n\n\nImportant\n\n\n\n\nDurée continue : absence (ou très peu) d’évènements mesurés simultanément\n\nDurée discrète/groupée : présence constante et/ou en grand nombre d’évènements simultanés"
  },
  {
    "objectID": "03-theorie.html#le-risk-set",
    "href": "03-theorie.html#le-risk-set",
    "title": "4  La théorie",
    "section": "4.2 Le Risk Set",
    "text": "4.2 Le Risk Set\n\n\nIl s’agit de la population soumise ou exposée au risque lorsque \\(T=t_i\\).\n\n\nCette population varie dans le temps car:\n\n\nCertaines personnes ont connu l’évènement, donc ne peuvent plus être soumises au risque (ex: décès si on analyse la mortalité).\nCertaines personnes sortent de l’observation sans avoir (encore) observé l’évènement: décès si on analyse un autre type d’évènement, perdu.e.s de vue, fin de l’observation dans un recueil rétrospectif."
  },
  {
    "objectID": "03-theorie.html#la-censure",
    "href": "03-theorie.html#la-censure",
    "title": "4  La théorie",
    "section": "4.3 La Censure",
    "text": "4.3 La Censure\n\n\n\n\n\n\nImportant\n\n\n\nUne observation est dite censurée lorsque la durée d’observation est inférieure à la durée d’exposition au risque\n\n\n\n4.3.1 Censure à droite\nDéfinition\nCertains individus n’auront pas (encore) connu l’évènement à la date de l’enquête après une certaine durée d’exposition. On a donc besoin d’un marqueur permettant de déterminer que les individus n’ont pas observé l’évènement sur la période d’étude.\nPourquoi une information est-elle censurée (à droite)?\n\nFin de l’étude, date de l’enquête.\nPerdu de vue, décès si autre évènement étudié.\n\nEn pratique (important)\n\nNe pas exclure ces observations, sinon on surestime la survenue de l’évènement.\nNe pas les considérer a-priori comme sorties de l’exposition sans avoir connu l’évènement. Elles peuvent connaître l’évènement après la date de l’enquête ou en étant perdues de vue. Sinon on sous-estime la durée moyenne de survenue de l’évènement.\n\nExemple\nOn effectue une enquête auprès de femmes : On souhaite mesurer l’âge à la première naissance. Au moment de l’enquête, une femme est âgée de 29 ans n’a pas (encore) d’enfant.\nCette information sera dite «censurée».\nElle est clairement encore soumise au risque après la date de l’enquête. Au niveau de l’analyse, elle sera soumise au risque à partir de ses premières règles jusqu’au moment de l’enquête.\nHypothèse fondamentale\nLes observations censurées ont vis à vis du phénomène observé le même comportement que les observations non censurées. On dit que la censure est non informative. Elle ne dépend pas de l’évènement analysé. Normalement le problème ne se pose pas dans les recueil retrospectif.\nProblème posé par la censure informative\nPar exemple en analysant des décès avec un recueil prospectif, si un individu est perdu de vue en raison d’une dégradation de son état de santé, l’indépendance entre la cause de la censure et le décès ne peut plus être assurée.\n\nA l’Ined l’exploitation du registre des personnes atteintes de mucoviscidose (G.Belis) donne une autre illustration de ce phénomène. Chaque année un nombre significatif de personnes sortent du registre (pas de résultats aux examens annuels). Si certain.e.s perdu.e.s de vue s’expliquent par des déménagements, émigration ou par un simple problème d’enregistrement des informations, on note qu’ils/elles sont nombreu.se.s à présenter une forme « légère » de la maladie. L’information pouvant être donnée ici par la mutation du gène. Comme il n’est pas recommandé de supprimer ou de traiter ces observations comme des censures à droite non informatives, on peut les appréhender comme un risque concurrent au décès ou à tout autre évènement analysé à partir de ce registre (voir section dédiée).\n\n\nLes graphiques suivant représentent, en temps calendaire et après sa transformation en durée, la logique des censures à droite. Le recueil des informations est ici de nature prospectives, et bien évidemment on suppose que le début de l’observation correspond à un début d’exposition cohérent avec l’analyse réalisée (année de diagnostic d’une maladie, début d’une séquence d’emploi, de lieu de résidence, de couple ou de célibat strict etc….).\n\nTrait plein : durée observée\nPointillés : durée censurée\nBulle : moment de l’évènement\n\n\n\n\n\n\nSchéma évènement/censure en temps calendaire\n\n\n\n\n\nSchéma évènement/censure sous forme de durée\n\n\n\n\n\n\n4.3.2 Censure à gauche, troncature et censure par intervalle\nCensure à gauche\nL’évènement a pu se produire avant le début période d’observation, mais on est pas en mesure de savoir s’il s’est produit, et si on sait qu’il s’est produit on est pas en mesure de savoir quand. Typique des données prospectives, de type registre, avec par exemple des âges à l’inclusion différenciés. La présence de ce type de censure ne permet de définir lors de la création de l’échantillon d’analyse des durées d’exposition cohérente pour l’ensemble de observations de départ. Même si elle ne sont pas traitée dans ce support, il existe des méthodes pour obtenir des résultats en précense de ce type de censure, mais à la condition qu’elle ne soit pas trop nombreuses. On peut également filtré l’échantillon en amont en conservent seulement celles dont le début d’exposition est clairement défini.\nCensure par intervalle\nTraditionnellement la censure par intervalle est définie sur l’impossibilité de dater exactement la survenue d’un évènement dans un intervalle de temps1. Dans ce sens, on pourrait donc affirmer qu’elle est une caractéristique propre aux temporalité groupées dites à durée discrète2. On peut sans problème généraliser ce phénomène de censure à l’occurence: un évènement peut se produire entre 2 temps d’observations sans qu’on puisse l’observer. Un exemple classique en criminologie est celle de la récidive d’un délit entre deux arrestations ou deux condamnation: on sait qu’une personne a récidivé en raison de son arrestation, mais on est pas en mesure de savoir s’il a récidivé plus tôt….Pas vu pas pris. A notr également, toujours dans un recueil prospectif, qu’un phénomène de censure à droite lié aux perdu.e de vue peut se transformer en censure par intervalle lorsque la personne “réapparait” et est de nouveau incluse à l’échantillon3.\nTroncature\nPar l’exemple, on analyse la survie d’une population. Seule la survie des individus vivants à l’inclusion peut être analysée (troncature à gauche). On peut donc rencontré un phénomène de sélection difficement contrôlable. On peut également avoir de la troncature à droite lorsqu’on mesure la durée à partir ou jusqu’à un certain seuil niveau. Celle-ci n’est pas forcément problématique.\nCes situations sont généralement plutôt bien contrôlées dans les recueils rétrospectifs, en particulier pour la censure ou la troncature à gauche. Elles sont en revanche assez courantes lorsque le recueil est de type prospectif, et certaines études passent complètement à travers. En annexe de ce support, on trouvera un examen critique d’une études publiées récemment dans Population qui ressemble, par le vocabulaire utilisé, à une analyse de type durée/survie mais qui en est pas une [lien]. Je ne conseille pas de lire cette section à ce moment du support\nDurée d’observation supérieure à la durée d’exposition\nA l’inverse de la censure, des individus peuvent sortir de l’exposition avant la fin de la période d’observation, et il convient donc de corriger la durée de cette sortie.\n\nSi au moment de l’enquête une femme sans enfant a 70 ans, cela n’a pas de sens de continuer de l’exposer au risque au-delà d’un certain âge. Si on ne dispose pas d’information sur l’âge à la ménopause on peut tronquer la durée un peu au-delà de l’âge le plus élevé à la première naissance observée dans les données.\nSituation traitée dans le TP de la formation: on analyse la durée de la première séquence d’emploi ou d’une suite de séquence d’emploi sans rupture (chômage, maladie, sortie du marché du travail etc…). Il conviendra pour les personnes qui n’on pas connu de rupture (d’au moins un an par exemple) de faire sortir certaines personnes au moment de la retraite et non au moment de l’enquête, si elle sont déjà retraitées à ce moment. On pourra considérer ces âges à la retraite censures comme des censures à droites non informatives4"
  },
  {
    "objectID": "03-theorie.html#les-grandeurs",
    "href": "03-theorie.html#les-grandeurs",
    "title": "4  La théorie",
    "section": "4.4 Les grandeurs",
    "text": "4.4 Les grandeurs\n\n4.4.1 Les grandeurs utilisées\n\nLa fonction de survie: \\(S(t)\\)\nLa fonction de répartition: \\(F(t)\\)\nLa fonction de densité: \\(f(t)\\)\nLe risque instantané: \\(h(t)\\)\nLe risque instantané cumulé: \\(H(t)\\)\n\nRemarques:\n\nToutes ces grandeurs sont mathématiquement liées les unes par rapport aux autres. En connaître une permet d’obtenir les autres.\n\nAu niveau formel on se placera ici du point de vue où la durée mesurée est strictement continue. Cela se traduit, entre autre, par l’absence d’évènements dits “simultanés”. En présence de durée discrète/groupé, il est a noté que les expressions se simplifie, en particulier pour la densite ou le risque dit instantané.\nLes expressions qui vont suivre ne sont pas des estimateurs, mais des grandeurs dont on précisera les propriétés. Les techniques d’estimations devront respecter ces propriétés .\n\n\n\n\n4.4.2 La fonction de Survie \\(S(t)\\)\nDans ce type d’analyse, il est courant d’analyser la courbe dite de survie. Hors contexte de mortalité on peut préférer la notion de courbe de de séjour (Courgeau, Lelièvre).\nLa fonction de survie donne la proportion de la population qui n’a pas encore connue l’évènement après une certaine durée \\(t\\). Elle y a donc “survécu”.\nFormellement, la fonction de survie est la probabilité de survivre au-delà de \\(t\\), soit:\n\n\\[S(t) = P(T&gt;t)\\]\nPropriétés:\n\n\\(S(0)=1\\)\n\\(\\lim\\limits_{t\\to{\\propto}}S(t)=0\\)\n\nLa fonction de survie est donc strictement non croissante.\n\n\n4.4.3 La fonction de répartition \\(F(t)\\)\nC’est la probabilité de connaitre l’évènement jusqu’en \\(t\\), soit:\n\n\\[F(t)=P(T\\leq{t})\\]\nSoit: \\(F(t) = 1 - S(t)\\)\nLa fonction de survie et la fonction de répartition sont donc deux grandeurs strictement complémentaires et décrivent la même information.\nPropriétés:\n\n\\(F(0)=0\\)\n\\(\\lim\\limits_{t\\to{\\propto}}F(t)=1\\)\n\n\n\n4.4.4 La fonction de densité \\(f(t)\\)\n\nPour une valeur de \\(t\\) donnée, la fonction de densité de l’évènement donne la distribution des moments où les évènement ont eu lieu. Elle est donnée dans un premier temps par la probabilité de connaitre l’évènement dans un petit intervalle de temps \\(dt\\). Si \\(dt\\) est proche de 0 (temps continu) alors cette probabilité tend également vers 0. On norme donc cette probabilité par \\(dt\\). Rappel: on est toujours ici dans la théorie.\n\nEn temps continu, la fonction de densité est donnée par la dérivée de la fonction de répartition: \\(f(t)=F'(t)=-S'(t)\\).\n\nFormellement la fonction de densité \\(f(t)\\) s’écrit:\n\\[f(t)=\\lim\\limits_{dt\\to{0}}  \\frac{P(t\\leq{T}&lt; t + dt)}{dt}\\]"
  },
  {
    "objectID": "03-theorie.html#le-risque-instantané-ht",
    "href": "03-theorie.html#le-risque-instantané-ht",
    "title": "4  La théorie",
    "section": "4.5 Le risque instantané \\(h(t)\\)",
    "text": "4.5 Le risque instantané \\(h(t)\\)\nConcept fondamental de l’analyse des durées:\n\\[h(t)=\\lim\\limits_{dt\\to{0}} \\frac{P(t\\leq{T}&lt; t + dt | T\\geq{t})}{dt}\\]\n\n\\(P(t\\leq{T}&lt; t + dt | T\\geq{t})\\) donne la probabilité de survenue de l’évènement sur l’intervalle \\([t,t+dt[\\) conditionnellement à la survie au temps \\(t\\).\n\nEn divisant par \\(dt\\), La quantité obtenue donne alors un nombre moyen d’évènements que connaîtrait un individu durant un très court intervalle de temps.\nA priori cette quantité n’est pas une probabilité. C’est la nature de l’évènement, en particulier sa non récurrence, et la métrique temporelle choisie ou disponible qui peut la rendre assimilable à une probabilité. Tout comme la densité, on est plutôt dans la définition d’un taux (d’où l’expression hazard rate en anglais).\n\nOn peut également écrire:\n\\(h(t)=\\frac{f(t)}{S(t)}=\\frac{F'(t)}{S(t)}=-\\frac{S'(t)}{S(t)}\\) 5\nOn remarque que cette fonction de risque (ou hazard rate) n’est pas une probabilité car \\(\\frac{f(t)}{S(t)}\\) ne peut pas contraindre ici la valeur obtenue à ne pas être supérieure à 1.\n\n\n\n\n\n\nGrandeurs avec des durées discrètes/groupées\n\n\n\nLes expressions de la densité et du risque dit instantané se simplifie: \\(f(t)=P(t\\leq{T}&lt; t + dt)\\) et \\(h(t)=P(t\\leq{T}&lt; t + dt | T\\geq{t})\\). Ces deux grandeurs ne sont plus des taux tel qu’ils ont été définit précedemment, mais des probabilités.\nNéanmoins les relations entre les grandeurs, en particulier celle qui lie risque, densité et survie reste toujours valable. Ceci est fondamental, car elle permet de définir la fonction de vraisemblance sur laquelle repose le calcul de tous les estimateurs\n\n\n\n4.5.1 Le risque cumulé \\(H(t)\\)\nLe risque cumulé est égal à :\n\\[H(t)=\\int_{0}^{t} h(u)du = -log(S(t))\\]\nOn peut alors réécrire toutes les autres quantités à partir de celle-ci:\n\n\\(S(t)=e^{-H(t)}\\)\n\n\\(F(t)=1-e^{-H(t)}\\)\n\n\\(f(t)=h(t)\\times{e^{-H(t)}}\\)\n\nExemple avec la loi exponentielle (risque constant)\nSi on pose que le risque est strictement constant au cours du temps: \\(h(t)=a\\). Cette forme du risque suit une loi exponentielle. Cette situation est, par exemple, typique des processus dits sans mémoire comme la durée de vie des ampoules:\n\n\\(h(t)=a\\)\n\n\\(H(t)=a\\times{t}\\)\n\n\\(S(t)=e^{-a\\times{t}}\\)\n\n\\(F(t)=1-e^{-a\\times{t}}\\)\n\n\\(f(t)=a\\times{e^{-a\\times{t}}}\\)\n\n\n\n\nGrandeurs de la loi exponentielle avec h(t)=0.04\n\n\nApplication: risque et échelles temporelles:\nAttention on sort ici très clairement de la durée continue, il s’agit seulement de manipuler les concepts et de voir la dépendance de la mesure du risque à l’échelle temporelle choisie ou disponible.\n\nDurant les mois d’hiver, entre le 1er janvier et le 1er avril [3 mois], la probabilité d’attraper un rhume chaque mois est de 48% (il s’agit bien d’un risque). Quelle est le risque d’attraper le rhume durant la saison froide?\n\\(\\frac{0.48}{1/3}=1.44\\). On peut donc s’attendre a attraper 1.44 rhume durant la période d’hiver.\nOn passe une année en vacances dans une région où la probabilité de décéder chaque mois est évaluée à 33%. Quelle est le risque de décéder pendant cette année sabbatique? \\(\\frac{0.33}{1/12}=3.96\\)\n\nLe risque peut donc être supérieur à 1. C’est donc plutôt un taux tel qu’il est défini généralement. En soit cela ne pose pas de problème comme il s’agit d’un nombre moyen d’évènements espérés (exemple: taux de fécondité), mais pour des évènements qui ne peuvent pas se répéter, évènements dits absorbants, l’interprétation n’est pas très intuitive.\nLe risque étant constant, on peut prendre son inverse qui mesure la durée moyenne (espérée) jusqu’à l’occurence de l’évènement.\nOn retrouve donc un concept classique en analyse démographique comme l’espérance de vie (survie): la question n’est pas de savoir si on va mourir ou non, ce risque indépendamment de la durée étant par définition égal à 1, mais jusqu’à quand on peut espérer survivre.\n\nPour le rhume, la durée moyenne est de \\((1.44^{-1}=0.69\\) du trimestre hivernal, soit approximativement le début du mois de mars.\nPour l’année sabbatique, la durée moyenne de survie est de \\(3.96^{-1}=0.25\\) d’une année soit 3 mois après l’arrivée dans la région.\n\nExercice\n\nOn a une population de 100 cochons d’Inde.\nOn analyse leur mortalité (naturelle).\n\nIci l’analyse est en temps discret.\nLa durée représente le nombre d’année de vie.\nIl n’y a pas de censure à droite.\n\n\n\n\nDurée\nNombre de décès\n\n\n\n\n1\n1\n\n\n2\n1\n\n\n3\n3\n\n\n4\n9\n\n\n5\n30\n\n\n6\n40\n\n\n7\n10\n\n\n8\n3\n\n\n9\n2\n\n\n10\n1\n\n\n\nN=100\nA quel âge le risque de mourir des cochons d’Inde est-il le plus élevé? Quelle est la valeur de ce risque?"
  },
  {
    "objectID": "03-theorie.html#remarques-complémentaires",
    "href": "03-theorie.html#remarques-complémentaires",
    "title": "4  La théorie",
    "section": "4.6 Remarques complémentaires",
    "text": "4.6 Remarques complémentaires\n\n4.6.1 Formes typiques de la fonction de survie\nUne des propriétés de la fonction de survie ou de séjour est qu’elles tendent vers 0. A la lecture du graphique suivant, cela peut correspondre à la forme de la courbe S2, bien que le % de survivant tend à baisser de moins en moins à mesure que la durée augmente. Deux cas limites doivent être considéré.\n\n\n\nFonction de survie: 3 situtation typiques\n\n\n\nS1: très peu d’évènements et la fonction de séjour suit une asymptote nettement supérieur à 0 ( \\(\\lim\\limits_{t\\to{\\propto}}S(t)=a\\) avec \\(a&gt;0\\)). La question est plus délicate car on interroge l’exposition au risque d’une partie de l’échantillon ou, dit autrement on peut penser qu’une fraction est immunisé au risque. Cette problématique est rapidement posée en fin de formation.\nS2: la situation attendue\nS3: La survie tombe à 0 très/trop rapidement: il n’y a donc pas ou presque pas de durée (par exemple presque tout l’échantillon observe l’évènement la première année de l’exposition). Les méthodes en temps continue ne sont a priori pas adaptées à ce genre de situation. Si on dispose d’une information plus fine pour dater les évènements, la fonction de séjour pourra reprendre une forme plus “standard”. Dans le graphique, \\(S(t=1)=0.4\\) , \\(S(t=2)=0.025\\), mais si on dispose par exemple de 10 points d’observations supplémentaires dans chaque durée groupée:\n\n\n\n\nFonction de survie et modification de la métrique temporelle\n\n\n\n\n4.6.2 Absence de censures à droites\nLes méthodes qui vont être présentées plus tard gèrent la présence de censures à droite. S’il n’y en a pas, elle restent néanmoins parfaitement valables. L’absence de censure facilite certaines analyses, par exemple celles des fonctions de séjour où le calcul direct des durées moyennes est rendu possible.\n\n\n4.6.3 Utilisation des pondérations dans un schema retrospectif avec des biographies longues\nUne question assez récurrente concerne l’utilisation des poids de sondage dans les analyses de durées avec longueurs biographiques souvent assez longues. Leur utilisation ne me semble pas recommandée voire à exclure sauf exceptions. En effet les pondérations sont générées au moment de l’enquête, alors que les évènements étudiés peuvent remonter dans un passé plus ou moins lointain pour une partie de la population analysée. Si on regarde de plus près, la création de poids longitudinaux ne résoudrait pas grand chose , les pondérations devant être recalculées à chaque moment d’observation ou à chaque moment où des évènements se produisent. Par ailleurs on mélangerait régulièrement à un instant donné des personnes issues de générations différentes ce qui rend impossible tout calage sur des caractéristiques d’un population. Supposons une personne âgée de 25 ans et un personne âgée de 70 ans au moment de l’enquête en 2022, avec un début d’observation à l’âge de 18 ans . A 20 ans (\\(t=2\\)), pour la première personne les caractéristiques de la population sont celles de 2017, pour celle de 70 ans celles de 1972. On fait comment??????"
  },
  {
    "objectID": "03-theorie.html#footnotes",
    "href": "03-theorie.html#footnotes",
    "title": "4  La théorie",
    "section": "",
    "text": "Garder en mémoire que l’analyse des durées ou de survie a été très largement développé dans un carde à durée continue↩︎\nSi on utilisee une mesure de la durée sur l’âge, on ne sait pas si l’évènement à eu lieu le lendemain de l’anniversaire ou la veille de l’anniversaire suivant↩︎\nVoir exemple plus haut sur le registre de la mucoviscidose↩︎\n\nLa relation \\(h(t)=\\frac{f(t)}{S(t)}\\) et donc \\(f(t)= h(t) \\times S(t)\\) est intéressante et importante car elle permet d’écrire la vraisemblance du processus probabiliste permettant d’estimer les paramètres des différentes analyses. On voit déjà sa proximité avec la fonction de masse de Bernouilli: \\(f(y_i) = p^{y_i} \\times (1-p)^{1-y_i}\\). Se reporté à la section qui décrit qui la vraisemblance partielle de Cox pour s’en faire une idée plus précise.↩︎"
  },
  {
    "objectID": "04-fsurvie.html#les-fonctions-de-survieséjour",
    "href": "04-fsurvie.html#les-fonctions-de-survieséjour",
    "title": "5  Estimations des fonctions de survie",
    "section": "5.1 Les fonctions de survie/séjour",
    "text": "5.1 Les fonctions de survie/séjour\n\n5.1.1 Les variables d’analyse\nOn a un échantillon aléatoire de \\(n\\) individus avec:\n\nDes indicateurs de fin d’épisode \\(e_1,e_2,....,e_k\\) avec \\(e_i=0\\) si censure à droite et \\(e_i=1\\) si évènement observé pendant la période d’observation.\nDes durées d’exposition au risque \\(t_1,t_2,....,t_k\\) jusqu’à l’évènement ou la censure.\nEn théorie, il ne peut pas y avoir d’évènement en \\(t=0\\).\n\n\n\n5.1.2 Calcul de la fonction de survie\nRappel: La fonction de survie donne la probabilité que l’évènement survienne après \\(t_i\\), soit \\(S(t_i)=P(T&gt;t_i)\\). Pour survivre en \\(t_i\\), il faut donc avoir survécu en \\(t_{i-1}\\), \\(t_{i-2}\\), …., \\(t_{1}\\).\nLa fonction de survie renvoie donc des probabilités conditionnelles: on survit en \\(t_i\\) conditionnellement au fait d’y avoir survécu avant. Il s’agit donc d’un produit de probabilités.\nSoit \\(d_i=\\sum e_i\\) le nombre d’évènements observé en \\(t_i\\) et \\(r_i\\) la population encore soumise au risque en \\(i\\). On peut mesurer l’intensité de l’évènement en \\(t_i\\) en calculant le quotient \\(q(t_i)=\\frac{d_i}{r_i}\\).\nSi le temps est strictement continu on devrait toujours avoir \\(q(t_i)=\\frac{1}{r_i}\\).\n\\(S(t_i) = (1 - \\frac{d_i}{r_i})\\times{S(t_{i-1})} = S(t_i) = (1 - q(t_i))\\times{S(t_{i-1})}\\). En remplaçant \\(S(t_{i-1})\\) par sa valeur: \\(S(t_i) = (1 - \\frac{d_i}{r_i})\\times(1 - \\frac{d_{i-1}}{r_{i-1}})\\times{S(t_{i-2})}\\).\nAu final, en remplaçant toutes les expressions de la survie jusqu’en \\(t_0\\) (\\(S(0)=1\\)):\n\\[S(t_i)=\\displaystyle \\prod_{t_i\\leq{k}} (1-q(t_i))\\]\n\n\n\n\n\n\nApplication pour la suite du support\n\n\n\n\nOn va analyser le risque de décéder (la survie) de personnes souffrant d’une insuffisance cardiaque. Le début de l’exposition est leur inscription dans un registre d’attente pour une greffe du coeur.\nLes covariables sont dans un premier temps toutes fixes: l’année (year) et l’âge (age) à l’entrée dans le registre, et le fait d’avoir été opéré pour un pontage aorto-coronarien avant l’inscription (surgery).\nLe début de l’exposition au risque est l’entrée dans le registre, la durée est mesurée en jour (stime). La variable évènement/censure est le décès (died). Les durées de la variable stime ont été regroupée par période de 30 jours pour réaliser des analyses à durée discrete. Cette nouvelle variable de durée a été appelé mois.\nL’introduction d’une dimension dynamique, la greffe, est donnée par les informations contenues dans les variables transplant et wait.\nLa variable compet est une information simulée pour réaliser des analyses en risques concurrents.\nLes bases en format .csv, .sas7bdat et .dta sont disponibles dans ce dépôt [lien]\n\nExtrait de la base:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nyear\nage\ndied\nstime\nsurgery\ntransplant\nwait\nmois\ncompet\n\n\n\n\n15\n68\n53\n1\n1\n0\n0\n0\n1\n1\n\n\n43\n70\n43\n1\n2\n0\n0\n0\n1\n1\n\n\n61\n71\n52\n1\n2\n0\n0\n0\n1\n1\n\n\n75\n72\n52\n1\n2\n0\n0\n0\n1\n1\n\n\n102\n74\n40\n0\n11\n0\n0\n0\n1\n0\n\n\n74\n72\n29\n1\n17\n0\n1\n5\n1\n2"
  },
  {
    "objectID": "04-fsurvie.html#la-méthode-actuarielle",
    "href": "04-fsurvie.html#la-méthode-actuarielle",
    "title": "5  Estimations des fonctions de survie",
    "section": "5.2 La méthode actuarielle",
    "text": "5.2 La méthode actuarielle\n\nEstimation sur des intervalles définies par l’utilisateur.\nMéthode dite «continue», estimation en milieu d’intevalle.\nMéthode apropriée lorsque la durée est mesurée de manière discrète/groupée.\nMéthode, hélas, quasiment abandonnée dans les sciences sociales où les durées sont plus rarement mesurées de manière exacte. L’absence de test de comparaison des fonctions de survie n’y est pas étranger, tout comme le lien de la méthode suivante (Kaplan-Meier) avec le modèle de Cox.\nContrairement à la méthode de Cox, la méthode actuarielle permet de calculer les quantiles de la durée.\n\n\n5.2.1 Estimation\nEchelle temporelle\nLa durée est divisée en \\(J\\) intervalles, en choisissant \\(J\\) points: \\(t_0&lt;t_1&lt;...&lt;t_J\\) avec \\(t_{J+1}=\\infty\\).\nCalcul du Risk set\n\nA \\(t_{min}=0\\), \\(n_0=n\\) individus soumis au risque: \\(r_0=n_0\\).\nLe nombre d’exposé.e.s au risque sur un intervalle est calculé en soustrayant la moitié des cas censurés sur la longueur de l’intervalle: \\(r_i=n_i- 0.5\\times{c_i}\\), avec \\(n_i\\) le nombre de personnes soumises au risque au début de l’intervalle et \\(c_i\\) le nombre d’observations censurées sur la longueur de l’intervalle. On suppose donc que les observations censurées \\(c_i\\) sont sorties de l’observation uniformément sur l’intervalle. Les cas censurés le sont en moyenne au millieu de l’intervalle. \n\nCalcul de \\(S(t_i)\\)\nOn applique la méthode de la section précédente avec:\n\\[q(t_i)=\\frac{d_i}{n_i - 0.5\\times c_i}\\]\nCalcul de la durée médiane (ou autre quantiles)\nRappel: en raison de la présence de censures à droite, le dernier intervalle étant ouvert jusqu’à la dernière sortie d’observation, il n’est pas conseillé de calculer des durées moyennes. On préfère utiliser la médiane ou tout autre quantile lorsqu’ils sont calculables.\nDéfinition: il s’agit de la durée telle que \\(S(t_i)=0.5\\).\nCalcul: Comme on applique une méthode continue et monotone à l’intérieur d’intervalles, on ne peut pas calculer directement un point de coupure qui correspond à 50% de survivants. On doit donc trouver ce point par interpolation linéaire dans l’intervalle \\([t_i;t_{i+1}[\\) avec \\(S(t_{i+1})\\leq0.5\\) et \\(S(t_{i})&gt;0.5\\).\nR-Stata-Sas-Python\n\nRStataSasPython\n\n\nLes fonctions de survie avec la méthode dite actuarielle sont estimables avec le package discSurv. Avec le temps, il s’est étoffé, on peut maintenant paramatrer des intervalles (programmation pénible), mais les quantiles de la durée ne sont toujours pas estimables, ce qui est bien dommage.\n\n\nCommande ltable, avec en option la paramétrisation des intervalles de durées. Voir la commande externe qlt (MT) qui calcule les durées médianes (+ autres quartiles) et qui recalcule la fonction de séjour avec une définition des intervalles de durées identique à celle de SAS.\n\n\nSous une proc lifetest avec en option method=lifetable. On peut paramétrer les intervalles d’estimation avec l’option width.\n\n\nA l’heure actuelle, aucune fonction à ma connaissance\n\n\n\n\n\n5.2.2 Application\nLes résultats qui suivent ont été estimés avec Stata en retenant la définition des bornes de Sas, plus pertinente à mon sens, avec des intervalles fixes de 30 jours.\n\n\nAfficher le tableau des estimateurs\n     +----------------------------------------------+\n     |   t0     t1   survival CI 95% low  CI 95% up |\n     |----------------------------------------------|\n  1. |    0     30          1          .          . |\n  2. |   30     60   .7853659   .6925991   .8530615 |\n  3. |   60     90   .6461871   .5449008   .7304808 |\n  4. |   90    120    .525027   .4232338   .6170507 |\n  5. |  120    150   .4740535   .3737563   .5677139 |\n     |----------------------------------------------|\n  6. |  150    180   .4636348   .3637283   .5575485 |\n  7. |  180    210   .4425605   .3435417   .5368989 |\n  8. |  210    240   .4105681   .3132064   .5052779 |\n  9. |  240    270   .3997637   .3030412   .4945301 |\n 10. |  270    300   .3888113   .2927645   .4836136 |\n     |----------------------------------------------|\n 11. |  300    330   .3665935   .2720434   .4613676 |\n 12. |  330    360   .3554846   .2617823   .4501585 |\n 13. |  360    390   .3216289   .2308275   .4157428 |\n 14. |  390    420   .3216289   .2308275   .4157428 |\n 15. |  420    450   .3216289   .2308275   .4157428 |\n     |----------------------------------------------|\n 16. |  480    510   .3216289   .2308275   .4157428 |\n 17. |  510    540   .3216289   .2308275   .4157428 |\n 18. |  540    570   .3216289   .2308275   .4157428 |\n 19. |  570    600   .3216289   .2308275   .4157428 |\n 20. |  600    630   .3059397   .2154747   .4009653 |\n     |----------------------------------------------|\n 21. |  660    690   .3059397   .2154747   .4009653 |\n 22. |  720    750   .2884574   .1981834   .3848506 |\n 23. |  840    870   .2704288   .1806664   .3680736 |\n 24. |  900    930   .2517786   .1628919   .3505543 |\n 25. |  930    960   .2517786   .1628919   .3505543 |\n     |----------------------------------------------|\n 26. |  960    990   .2517786   .1628919   .3505543 |\n 27. |  990   1020   .2288896   .1404089   .3303913 |\n 28. | 1020   1050   .2060007   .1191749   .3093143 |\n 29. | 1140   1170   .1831117   .0991601   .2873401 |\n 30. | 1320   1350   .1831117   .0991601   .2873401 |\n     |----------------------------------------------|\n 31. | 1380   1410   .1831117   .0991601   .2873401 |\n 32. | 1560   1590   .1464894   .0645215   .2602391 |\n 33. | 1770   1800   .1464894   .0645215   .2602391 |\n 34. | 1800      .   .1464894   .0645215   .2602391 |\n     +----------------------------------------------+\n\n\n\nQuantiles de la fonction de séjour type actuarielle - Bornes Sas\n\n\nS(t)\nt\n\n\n\n\n0.90\n13.977\n\n\n0.75\n37.623\n\n\n0.50\n104.729\n\n\n0.25\n906.993\n\n\n0.10\n.\n\n\n\n\n\n\nCourbe de survie: estimation méthode actuarielle\n\n\nLecture des résultats: 102 jours après leur inscription dans le registre d’attente pour une greffe, 50% des malades sont toujours en vie. Au bout de 914 jours, 75% sont décédés."
  },
  {
    "objectID": "04-fsurvie.html#la-méthode-de-kaplan-meier",
    "href": "04-fsurvie.html#la-méthode-de-kaplan-meier",
    "title": "5  Estimations des fonctions de survie",
    "section": "5.3 La méthode de Kaplan-Meier",
    "text": "5.3 La méthode de Kaplan-Meier\n\nL’approche qui exploite toute l’information disponible est celle dite de Kaplan-Meier (KM).\nIl y a autant d’intervalles que de durées où l’on observe au moins un évènement.\nAu lieu d’utiliser des intervalles prédéterminés, l’estimateur KM va définir un intervalle entre chaque évènement enregistré.\nLa fonction de survie estimée par la méthode KM est une fonction en escalier (stairstep), d’où une estimation dite “discrète”.\nPour chaque intervalle, on compte le nombe d’évènements et le nombre de censures.\nMéthode adaptée pour une mesure de la durée de type continue.\n\n\n5.3.1 Estimation\nDéfinition du Risk Set (\\(r_i\\))\nS’il y a à la fois des évènements et des censures à une durée \\(t_i\\), les observations censurées sont considérées comme exposées au risque à ce moment, comme si elles étaient censurées très rapidement après. C’est la principale caractéristique de cette méthode, appelé également l’estimateur product-limit\n\\[r_i=r_{i-1}-d_{i-1}-c_{i-1}\\]\n\nCalcul de \\(q_i\\)\n On applique la méthode de la section précédente avec:\n\\[q_i=\\frac{d_i}{r_{i-1}-d_{i-1}-c_{i-1}}\\] Remarque: la variance de l’estimateur est obtenu par la méthode dite de Greenwood. Il n’y a pas d’intérêt particulier de la décrire dans ce support.\nRécupération de la médiane\nIl n’y a pas de méthode pour calculer directement la durée médiane (ou tout autre quantile) contrairement à l’approche actuarielle.\nLa définition retenue est conventionnelle. On va prendre la valeur de la durée qui se situe juste en dessous de 50% de survivant.e.s. Elle est donc définie tel que \\(S(t)\\leq0.5\\). Attention, il n’est pas impossible que le % de survivant.e.s soit bien en deçà de 50% pour l’obtention cette durée médiane.\nR-Stata-Sas-Python\n\nRStataSASPython\n\n\nLes estimateurs sont obtenus avec fonction survfit du package survival. On peut obtenir des rendus graphiques de meilleures qualité avec le package survminer (fonction ggsurvplot)\n\n\nAprès avoir appelé les variables de durée et de censure en mode survival avec stset), le tableau des estimateurs est obtenu avec la commande sts list et le graphique avec sts graph.\n\n\nL’estimation de Kaplan-Meier est affichée par défaut par la proc lifetest. Warning : le tableau affiché par SAS est particulièrement pénible à lire voire illisible, en particulier lorsque le nombre de censures est élevé, une ligne étant ajoutée pour chaque observation censurée. Je conseille de ne pas afficher cette partie de l’output (se reporter à la section SAS du chapitre programmation). On récupère pour le reste de l’output les valeurs de la durée pour S(t) =(.75,.5,.25) ainsi que le graphique, ce qui est suffisant.\n\n\nLes resultats sont donnés dans la librairie lifeline par des fonctions au nom interminable. Je conseille plutôt l’utilisation de la librairie statmodels (se reporter à la section dédiée à Python).\n\n\n\n\n\n5.3.2 Application\nOn reprend l’exemple précédent.\n\n\nAfficher le tableau des estimateurs\nTime    Total   Fail   Lost           Function     Error     [95% Conf. Int.]\n-------------------------------------------------------------------------------\n     1      103      1      0             0.9903    0.0097     0.9331    0.9986\n     2      102      3      0             0.9612    0.0190     0.8998    0.9852\n     3       99      3      0             0.9320    0.0248     0.8627    0.9670\n     5       96      2      0             0.9126    0.0278     0.8388    0.9535\n     6       94      2      0             0.8932    0.0304     0.8155    0.9394\n     8       92      1      0             0.8835    0.0316     0.8040    0.9321\n     9       91      1      0             0.8738    0.0327     0.7926    0.9247\n    11       90      0      1             0.8738    0.0327     0.7926    0.9247\n    12       89      1      0             0.8640    0.0338     0.7811    0.9171\n    16       88      3      0             0.8345    0.0367     0.7474    0.8937\n    17       85      1      0             0.8247    0.0375     0.7363    0.8857\n    18       84      1      0             0.8149    0.0383     0.7253    0.8777\n    21       83      2      0             0.7952    0.0399     0.7034    0.8614\n    28       81      1      0             0.7854    0.0406     0.6926    0.8531\n    30       80      1      0             0.7756    0.0412     0.6819    0.8448\n    31       79      0      1             0.7756    0.0412     0.6819    0.8448\n    32       78      1      0             0.7657    0.0419     0.6710    0.8363\n    35       77      1      0             0.7557    0.0425     0.6603    0.8278\n    36       76      1      0             0.7458    0.0431     0.6495    0.8192\n    37       75      1      0             0.7358    0.0436     0.6388    0.8106\n    39       74      1      1             0.7259    0.0442     0.6282    0.8019\n    40       72      2      0             0.7057    0.0452     0.6068    0.7842\n    43       70      1      0             0.6956    0.0457     0.5961    0.7752\n    45       69      1      0             0.6856    0.0461     0.5855    0.7662\n    50       68      1      0             0.6755    0.0465     0.5750    0.7572\n    51       67      1      0             0.6654    0.0469     0.5645    0.7481\n    53       66      1      0             0.6553    0.0472     0.5541    0.7390\n    58       65      1      0             0.6452    0.0476     0.5437    0.7298\n    61       64      1      0             0.6352    0.0479     0.5333    0.7206\n    66       63      1      0             0.6251    0.0482     0.5230    0.7113\n    68       62      2      0             0.6049    0.0487     0.5026    0.6926\n    69       60      1      0             0.5948    0.0489     0.4924    0.6832\n    72       59      2      0             0.5747    0.0493     0.4722    0.6643\n    77       57      1      0             0.5646    0.0494     0.4621    0.6548\n    78       56      1      0             0.5545    0.0496     0.4521    0.6453\n    80       55      1      0             0.5444    0.0497     0.4422    0.6357\n    81       54      1      0             0.5343    0.0498     0.4323    0.6261\n    85       53      1      0             0.5243    0.0499     0.4224    0.6164\n    90       52      1      0             0.5142    0.0499     0.4125    0.6067\n    96       51      1      0             0.5041    0.0499     0.4027    0.5969\n   100       50      1      0             0.4940    0.0499     0.3930    0.5872\n   102       49      1      0             0.4839    0.0499     0.3833    0.5773\n   109       48      0      1             0.4839    0.0499     0.3833    0.5773\n   110       47      1      0             0.4736    0.0499     0.3733    0.5673\n   131       46      0      1             0.4736    0.0499     0.3733    0.5673\n   149       45      1      0             0.4631    0.0499     0.3632    0.5571\n   153       44      1      0             0.4526    0.0499     0.3531    0.5468\n   165       43      1      0             0.4421    0.0498     0.3430    0.5364\n   180       42      0      1             0.4421    0.0498     0.3430    0.5364\n   186       41      1      0             0.4313    0.0497     0.3327    0.5258\n   188       40      1      0             0.4205    0.0497     0.3225    0.5152\n   207       39      1      0             0.4097    0.0495     0.3123    0.5045\n   219       38      1      0             0.3989    0.0494     0.3022    0.4938\n   263       37      1      0             0.3881    0.0492     0.2921    0.4830\n   265       36      0      1             0.3881    0.0492     0.2921    0.4830\n   285       35      2      0             0.3660    0.0488     0.2714    0.4608\n   308       33      1      0             0.3549    0.0486     0.2612    0.4496\n   334       32      1      0             0.3438    0.0483     0.2510    0.4383\n   340       31      1      1             0.3327    0.0480     0.2409    0.4270\n   342       29      1      0             0.3212    0.0477     0.2305    0.4153\n   370       28      0      1             0.3212    0.0477     0.2305    0.4153\n   397       27      0      1             0.3212    0.0477     0.2305    0.4153\n   427       26      0      1             0.3212    0.0477     0.2305    0.4153\n   445       25      0      1             0.3212    0.0477     0.2305    0.4153\n   482       24      0      1             0.3212    0.0477     0.2305    0.4153\n   515       23      0      1             0.3212    0.0477     0.2305    0.4153\n   545       22      0      1             0.3212    0.0477     0.2305    0.4153\n   583       21      1      0             0.3059    0.0478     0.2156    0.4008\n   596       20      0      1             0.3059    0.0478     0.2156    0.4008\n   620       19      0      1             0.3059    0.0478     0.2156    0.4008\n   670       18      0      1             0.3059    0.0478     0.2156    0.4008\n   675       17      1      0             0.2879    0.0483     0.1976    0.3844\n   733       16      1      0             0.2699    0.0485     0.1802    0.3676\n   841       15      0      1             0.2699    0.0485     0.1802    0.3676\n   852       14      1      0             0.2507    0.0487     0.1616    0.3497\n   915       13      0      1             0.2507    0.0487     0.1616    0.3497\n   941       12      0      1             0.2507    0.0487     0.1616    0.3497\n   979       11      1      0             0.2279    0.0493     0.1394    0.3295\n   995       10      1      0             0.2051    0.0494     0.1183    0.3085\n\n[Résultats non reportés à partir de t=1000 ]\n-------------------------------------------------------------------------------\n\n\nLa durée durée médiane de survie est \\(t=100\\). Elle correspond à \\(S(t)=0.4940\\).\n\nQuantiles de la fonction de séjour type Kaplan-Meier\n\n\nS(t)\nt\n\n\n\n\n0.90\n6\n\n\n0.75\n36\n\n\n0.50\n100\n\n\n0.25\n979\n\n\n0.1\n.\n\n\n\n\n\n\n\n\nCourbe de survie: estimation méthode Kaplan-Meier\n\n\n\n\n\nCourbe de survie: méthode Kaplan-Meier + CI\n\n\n\n\n\n\n5.3.3 Quantités associées à l’estimateur Kaplan-Meier..\nLe risque cumulé: estimateur de Nelson AAlen\nIl est simplément égal à:\n\\[H(t)=\\sum_{t_i\\leq k}q(t_i)\\]\n\n\n\nRisque cumulé: estimateur Nelson-Aalen\n\n\nLe risque ou taux de hasard instantané\nNécessite l’estimateur de risque cumulé de Nelson-Aalen. Le risque est obtenu en lissant les différences - toujours positive - entre \\(H(t)\\) par la méthode dite du kernel (cf estimation de la densité des distributions). Elle permet d’obtenir une fonction continue avec la durée (paramétrables sur les largeurs des fenêtres de lissage). D’autres méthodes de lissage sont maintenant possibles, et de plus en plus utilisées, en particulier celles utilisant des splines.\n\n\n\nRisque instantané: estimateur du Kernel\n\n\n\n\n\n\n\n\nNote\n\n\n\nIl n’est pas inutile de noter qu’il n’y a pas de formule toute faite pour obtenir des valeurs du risque instantané. Ce type de méthode par lissage est pleinement paramétrable, par exemple sa fenêtre, ce qui implique que son profil varie d’un paramétrage à l’autre. Le graphique précédent a été fait avec Stata, si on utilisait le package muhaz les différences de paramétrage par défaut font que les courbes ne se confondent pas."
  },
  {
    "objectID": "05-fsurvie_comp.html#tests-du-log-rank",
    "href": "05-fsurvie_comp.html#tests-du-log-rank",
    "title": "6  Tests de comparaison",
    "section": "6.1 Tests du log-rank",
    "text": "6.1 Tests du log-rank\nIl s’agit d’une série de tests qui répondent à la même logique, la seule différence réside dans le poids accordé au début ou à la fin de la période d’observation. Par ailleurs ces différents tests sont plus ou moins sensibles à la distribution des censures à droites entre les sous échantillons et à la non proportionalité des risques.\nDans leur logique, ces tests entrent dans le cadre des tests d’indépendance du Khi2, même si formellement ils relèvent des techniques dites de rang.\nIl s’agira donc de comparer des effectifs observés à des effectifs espérés à chaque moment d’évènement. La principale différence réside dans le calcul de la variance de la statistique du test qui, ici, suit assez logiquement une loi hypergéométrique [proche loi binomiale mais avec tirage avec remise].\n\n6.1.1 Principe de calcul de la statistique de test\n\nEffectifs observés en \\(t_i\\): \\(o_{i1}\\) et \\(o_{i2}\\) sont égaux à \\(d_{i1}\\) et \\(d_{i2}\\), et leur somme pour tous les temps d’évènement à \\(O_1\\) et \\(O_2\\).\nEffectifs expérés (hypothèse nulle \\(H_0\\)): comme pour une statistique du \\(\\chi^2\\) on se base sur les marges, avec le risque set (\\(R_i\\)) en \\(t_i\\) pour dénombrer les effectifs, soit \\(e_{i1}=R_{i1}\\times\\frac{d_i}{R_i}\\) et \\(e_{i2}=R_{i2}\\times\\frac{d_2}{R_i}\\). Leur somme pour tous les temps d’évènement est égale à \\(E_1\\) et \\(E_2\\). Le principe de calcul des effectifs observés reposent donc sur l’hypothèse d’un rapport des risques toujours égal à 1 au cours du temps (hypothèse fondamentale de risques proportionnels).\nStatistique du log-rank: \\((O_1 - E_1) = -(O_2 - E_2)\\).\nStatistique de test: sous \\(H_0\\), \\(\\frac{(O_1 - E_1)^2}{\\sum{v_i}}\\), avec \\(v_i\\) la variance de \\((o_{i1} - e_{i2})\\), suis un \\(\\chi^2(1)\\). Si on teste simultanément la différence de \\(g\\) fonctions de survie, ce qui n’est pas une bonne idée en passant, la statistique de test suis un \\(\\chi^2(g-1)\\).\n\n\n\n6.1.2 Les principaux tests log-rank\nLe principe de construction des effectifs observés et espérés reste le même dans chaque test, les différences résident dans les pondérations (\\(w_i\\)) qui prennent en compte, de manière différente, la taille de la population soumise au risque à chaque durée où au moins un évènement est observé.\n\nTest du log-rank: \\(w_i=1\\)\nIl accorde le même poids à toutes les durées d’évènement. C’est le test standard, le plus utilisé.\nTest de Wilconxon-Breslow-Grehan: \\(w_i=R_i\\)\nLes écarts entre effectifs observés et espérés sont pondérés par la population soumise à risque en \\(t_i\\). Le test accorde plus de poids au début de la période analysée, et il est sensible aux différences de distributions entre les strates des observations censurées.\nTest de Tarone-Ware: \\(w_i=\\sqrt{R_i}\\)\nVariante du test précédent, il atténue le poids accordé aux évènements au début de la période d’observation. Il est par ailleurs moins sensible au problème de la distribution des censures entre les strates.\nTest de Peto-Peto : \\(w_i=S_i\\)\nLa pondération est une variante de la fonction de survie KM (avec \\(R_i=R_i+1\\)). Le test n’est pas sensible au problème de distribution des censures.\nTest de Fleming-Harington: \\(w_i=(S_i)^p\\times(1-S_i)^{q}\\) avec \\(0\\leq{p}\\leq{1}\\) Il permet de paramétrer le poids accordé au début où à la fin de temps d’observation. Si \\(p=q=0\\) on retrouve le test de base non pondéré.\n\nEn pratique/remarques:\n\nLes tests du log-rank sont sensibles à l’hypothèse de risques proportionnels (voir modèle semi-paramétrique de Cox). En pratique si des courbes de séjours se croisent, il est fortement déconseillé de les utiliser. Cela ne signifie pas que si les courbes ne se croisent pas, l’hypothèse de proportionalité des risques est respectée : des rapports de risque peuvent au cours du temps s’intensifier, se réduire ou, le cas échant s’inverser, ce qui est typique d’un croisement.\nEffectuer un test global (multiple/omnibus) sur un nombre important de groupes (ou &gt;2) peut rendre le test très facilement significatif. Il peut être intéressant de tester des courbes deux à deux (idem qu’une régression avec covariable discrète), en conservant un seul degré de liberté. Des méthodes de correction du test multiple sont possibles ou disponibles si on utilise R.\n\nR-Stata-Sas-Python\n\nRStataSasPython\n\n\nOn utilise la fonction survdiff de la librairie survival. Le résultat du test de Peto-Peto est affiché par défaut (rho=1). Si on souhaite utiliser le test non pondéré, on ajoute l’option rho=0. Pour obtenir le résultat d’un test multiple corrigé (plus d’un degré de liberté), on peut utiliser la fonction pairwise_survdiff du package survminer. Cette fonction permet également d’obtenir des tests 2 à 2.\nJe conseille de rester sur l’option Peto-Peto et dans le cas d’une variable à plus de deux modalités, d’utiliser la fonction de survminer pairwise_survdiff.\n\n\nOn utilise la commande sts test avec le nom de la version du test: peto, wilcoxon . Sans préciser le nom de la variante, le test non pondéré est exécuté.\n\n\nLe test non pondéré et la version Wilcoxon sont données avec l’option strata de la proc lifetest. Attention : ne jamais utiliser la version LR Test qui est biaisée. Pour obtenir d’autres versions du test du log-rank, on ajoute /test=all à l’option strata.\n\n\nAvec la librairie lifelines, on utilise la fonction logrank_test. Quatre variantes sont disponibles (Wilcoxon, Tarone-Ware, Peto-Peto et Fleming-Harrigton). On peut également utiliser la fonction duration.survdiff de statmodels (non pondéré, Wilcoxon - appelé ici Breslow- et Tarone-Ware).\n\n\n\n\n\n6.1.3 Application\nOn compare ici l’effet du pontage coronarien sur le risque de décéder depuis l’inscription dans le registre de greffe.\n\n\n\n\n\n\nRésultats des tests du logrank\n\n\nTest\ndf\nChi2\nP&gt;Chi2\n\n\n\n\nNon pondéré\n1\n6.59\n0.0103\n\n\nWilcoxon (Breslow)\n1\n8.99\n0.0027\n\n\nTarone-Ware\n1\n8.46\n0.0036\n\n\nPeto-Peto\n\n8.66\n0.0033\n\n\n\nLes résultats font apparaître que l’opération permet d’augmenter la durée de survie des personnes. Il apparait que la p-value est plus élevée pour test non pondérée. Cela peut-il s’expliquer en regardant les deux courbes de séjours? Qu’en est-il de la proportionalité des risques ???? …. Réponse pendant la formation."
  },
  {
    "objectID": "05-fsurvie_comp.html#comparaison-des-rmst",
    "href": "05-fsurvie_comp.html#comparaison-des-rmst",
    "title": "6  Tests de comparaison",
    "section": "6.2 Comparaison des RMST",
    "text": "6.2 Comparaison des RMST\nRMST: Restricted Mean of Survival Time\nLa comparaison des RMST est une alternative pertinente aux tests du log-rank car elle ne repose pas sur des hypothèses contraignantes (proportionnalité des risques, distribution des censures), et permet une lecture vivante basée sur des espérances de séjour et non sur la lecture d’une simple p-value traduisant l’homogénéité ou non des fonctions de séjour. Par ailleurs les comparaisons sont souples, on peut choisir un ou plusieurs points d’horizon pour alimenter l’analyse.\nPrincipe\n\nL’aire sous la fonction de survie représente la durée moyenne d’attente jusqu’à l’évènement, soit une espérance de survie.\nEn présence de censure à droite, il faut borner la durée maximale \\(t^*&lt;\\infty\\). L’espérance de survie s’interprète donc sur un horizon fini. On est très proche d’une mesure en analyse démographique type « espérance de vie partielle ».\n\\(RMST =\\int_0^{t^*}S(t)dt\\).\nOn peut facilement comparer les RMST de deux groupes, en termes de différence ou de ratio.\nPar défaut on définit généralement \\(t^*\\) à partir le temps du dernier évènement observé. Il est néanmoins possible de calculer le RMST sur des intervalles plus court, ce qui lui permet une véritable souplesse au niveau de l’analyse.\n\nR-Stata-Sas-Python\nAttention, selon les logiciels la durée max par défaut n’est pas la même. Pour R et Sas, il s’agit du dernier évènement observé sur l’ensemble de l’échantillon, alors que Stata prend la durée qui correspond au dernier évènement observé le plus court des deux groupes . Cela affectera légèrement la valeur des Rmst estimées par défaut.\nPour l’exemple, la durée maximale utilisée par R est de 1407 jours alors que pour Stata elle est de 995 jours.\n\nRStataSASPython\n\n\nLibrairie SurvRm2. Programmée par les mêmes personnes que la commande Stata, la fonction proposée n’est pas très souple.\n\n\nCommande externe strmst2. La plus ancienne fonction proposée par les logiciels. Au final plus limitée que la solution Sas. J’ai programmé une commande, diffrmst, qui représente graphiquement les estimations des Rmst pour chaque temps d’évènement, leurs différences et les p-value issues des comparaisons.\n\n\nDisponible depuis la version 15.1 de SAS/Stat (fin 2018). Les estimations et le résultat du test de comparaison sont récupérables très simplement dans une proc lifetest, avec en option **plots=(rmst)** . Bien que sortie tardivement par rapport Stata et R, les résultats sont particulièrement complets.\n\n\nEstimation un peu pénible. A partir de l’estimateur KM obtenu avec la fonction KaplanMeierFitter de lifelines, on peut obtenir les RMST avec la fonction restricted_mean_survival_time. On peut tracer les fonctions, en revanche le test de comparaison n’est pas implémenté.\n\n\n\nApplication\nAvec \\(tmax=1407\\):\n\nEstimation des Rmst pour la variable surgery\n\n\nGroupes\nRMST\nStd. Err\n95% CI\n\n\n\n\n\\(surgery=1\\)\n884.576\n187.263\n517.546 - 1251.605\n\n\n\\(surgery=0\\)\n379.148\n61.667\n258.282 - 500.014\n\n\n\n\nDifférences entre Rmst pour la variable surgery\n\n\n\n\n\n\n\n\nTypes de contraste\nEcarts RMST\nP&gt;|z|\n95% CI\n\n\n\n\n\\(Rmst(surgery1 - surgery0)\\)\n505.428\n0.010\n517.546 - 1251.605\n\n\n\\(Rmst\\left(\\frac{surgery1}{surgery0}\\right)\\)\n2.333\n0.002\n1.383 - 3.937\n\n\n\nIci \\(t^*\\) est égal à 1407 jours, soit la durée qui correspond au dernier décès observé.\nSur un horizon de 1407 jours, ces individus opérés d’un pontage peuvent espérer vivre 884 jours en moyenne, contre 379 jours pour les autres. La durée moyenne de survie est donc 2.3 fois plus importante pour les personnes opérées (rapport des Rmst = 2.3 ), ce qui correspond à une différence de 379 jours.\nLe tableau et le graphique suivant donnent les valeurs des Rmst et les écarts de la variable surgery en faisant varier \\(tmax\\) sur chaque jour où au moins un décès a été observé. Il a été réalisé avec Stata, la durée maximale utilisée a été paramétrée à 1407 jours (idem R, Sas).\nComme le premier décès observé pour les personnes opéré se situe le 165eme jours, il est tout à fait normal que pour ce groupe de personnes la valeur de la Rmst soit identique au jour de décès des individus non opérés.\n\n\n\n\n\n\nNote\n\n\n\nPour la version pdf, seulement une dizaine de points a été sélectionné en raison de la longueur du tableau\n\n\n\n\nAfficher le tableau\n  +--------------------------------------------------------------------------+\n  | _time     _rmst1     _rmst0      _diff    95%CI lower 95%CI upper  pvalue|\n  |--------------------------------------------------------------------------|\n  |--------------------------------------------------------------------------|\n  |     1          1          1          0           0          0          . |\n  |     2          2   1.989011    .010989   -.0104304   .0324084   .3146368 |\n  |     3          3   2.945055   .0549451   -.0009099      .1108   .0538507 |\n  |     5          5   4.791209   .2087912    .0549289   .3626535   .0078217 |\n  |     6          6   5.692307   .3076923    .0995576   .5158269   .0037617 |\n  |--------------------------------------------------------------------------|\n  |     8          8    7.45055   .5494505    .2224352   .8764658   .0009908 |\n  |     9          9   8.318682   .6813186    .2913915   1.071246   .0006156 |\n  |    11         11   10.03297   .9670329    .4461406   1.487925   .0002741 |\n  |    12         12   10.89011    1.10989    .5212656   1.698514   .0002193 |\n  |    16         16   14.27415   1.725846    .8588567   2.592834   .0000956 |\n  |--------------------------------------------------------------------------|\n  |    17         17   15.08677    1.91323    .9769751   2.849484    .000062 |\n  |    18         18   15.88825   2.111745     1.10533   3.118161   .0000391 |\n  |    21         21   18.25931   2.740688    1.516228   3.965148   .0000115 |\n  |    28         28   23.63593   4.364065    2.598741   6.129389   1.26e-06 |\n  |    30         30   25.14985    4.85015    2.924714   6.775586   7.93e-07 |\n  |--------------------------------------------------------------------------|\n  |    31         31   25.89568   5.104324    3.098862   7.109787   6.08e-07 |\n  |    32         32    26.6415   5.358499    3.272218   7.444779   4.80e-07 |\n  |    35         35   28.84508   6.154923    3.825102   8.484744   2.24e-07 |\n  |    36         36    29.5683   6.431698    4.020457   8.842939   1.71e-07 |\n  |    37         37   30.28023   6.719774    4.227545   9.212004   1.26e-07 |\n  |--------------------------------------------------------------------------|\n  |    39         39   31.68147   7.318526    4.664121   9.972931   6.52e-08 |\n  |    40         40    32.3708   7.629202    4.893653   10.36475   4.60e-08 |\n  |    43         43   34.37097   8.629034    5.651719   11.60635   1.34e-08 |\n  |    45         45   35.68181   9.318189    6.177435   12.45894   6.07e-09 |\n  |    50         50   38.90242   11.09758    7.539261    14.6559   9.80e-10 |\n  |--------------------------------------------------------------------------|\n  |    51         51   39.53524   11.46476    7.822028   15.10748   6.89e-10 |\n  |    53         53   40.77829   12.22171    8.410756   16.03267   3.27e-10 |\n  |    58         58   43.82939   14.17061    9.933216     18.408   5.58e-11 |\n  |    61         61   45.62615   15.37385    10.87721   19.87049   2.07e-11 |\n  |    66         66   48.56425   17.43575    12.50275   22.36874   4.28e-12 |\n  |--------------------------------------------------------------------------|\n  |    68         68   49.71689   18.28311      13.175   23.39121   2.30e-12 |\n  |    69         69   50.27061   18.72939    13.53629   23.92248   1.56e-12 |\n  |    72         72   51.89787   20.10213    14.65526   25.54901   4.71e-13 |\n  |    77         77   54.49696   22.50304    16.63758   28.36851   5.51e-14 |\n  |    78         78   55.00547   22.99453    17.04528   28.94377   3.57e-14 |\n  |--------------------------------------------------------------------------|\n  |    80         80   55.99991   24.00009    17.88509   30.11509   1.44e-14 |\n  |    81         81   56.48582   24.51418    18.31722   30.71113   8.88e-15 |\n  |    85         85   58.38429   26.61571    20.09202    33.1394   1.33e-15 |\n  |    90         90   60.70087   29.29913    22.36311   36.23515   2.22e-16 |\n  |    96         96   63.41296   32.58704    25.15034   40.02373          0 |\n  |--------------------------------------------------------------------------|\n  |   100        100   65.17583   34.82418    27.05229   42.59606          0 |\n  |   102        102   66.03465   35.96535     28.0274   43.90329          0 |\n  |   109        109   68.96146   40.03855     31.5203   48.55679          0 |\n  |   110        110   69.37957   40.62043     32.0176   49.22326          0 |\n  |   131        131   77.91607   53.08393    42.67269   63.49517          0 |\n  |--------------------------------------------------------------------------|\n  |   149        149   85.23307   63.76693    51.71735   75.81651          0 |\n  |   153        153   86.81125   66.18875     53.7781    78.5994          0 |\n  |   165        165   91.40231   73.59769    60.11884   87.07655          0 |\n  |   180   178.6364   97.14113   81.49523    66.43188   96.55859          0 |\n  |   186   184.0909   99.43666   84.65425     68.8452   100.4633          0 |\n  |--------------------------------------------------------------------------|\n  |   188   185.7273   100.2018   85.52544    69.46069   101.5902          0 |\n  |   207   201.2727   107.2365    94.0362    75.10156   112.9708          0 |\n  |   219   211.0909   111.5314   99.55952    78.49335   120.6257          0 |\n  |   263   247.0909   126.7362   120.3547     90.2882   150.4212   4.22e-15 |\n  |   265   248.7273   127.4026   121.3246    90.82352   151.8258   6.44e-15 |\n  |--------------------------------------------------------------------------|\n  |   285   265.0909   134.0671   131.0238     96.0849   165.9628   1.98e-13 |\n  |   308   283.9091   141.1416   142.7675    102.6688   182.8662   2.99e-12 |\n  |   334   305.1818   148.8057   156.3761    110.3439   202.4082   2.77e-11 |\n  |   340   310.0909   150.4975   159.5934    112.1866   207.0003   4.16e-11 |\n  |   342   311.7273   151.0358   160.6915    112.8294   208.5536   4.69e-11 |\n  |--------------------------------------------------------------------------|\n  |   370   332.0909   158.5717   173.5192    119.5852   227.4532   2.87e-10 |\n  |   397   351.7273   165.8385   185.8887    125.7134   246.0641   1.41e-09 |\n  |   427   373.5454   173.9127   199.6327    132.2074    267.058   6.51e-09 |\n  |   445   386.6364   178.7573   207.8791    135.9845   279.7736   1.45e-08 |\n  |   482   413.5454   188.7155     224.83    143.5408   306.1191   5.93e-08 |\n  |--------------------------------------------------------------------------|\n  |   515   437.5454   197.5971   239.9483    150.1031   329.7935   1.65e-07 |\n  |   545   459.3636   205.6714   253.6923    155.9623   351.4222   3.62e-07 |\n  |   583        487   215.8987   271.1013    163.2743   378.9283   8.32e-07 |\n  |   596   494.8788   219.3976   275.4812    164.6331   386.3293   1.11e-06 |\n  |   620   509.4243   225.8569   283.5673     166.951   400.1836   1.88e-06 |\n  |--------------------------------------------------------------------------|\n  |   670   539.7273    239.314   300.4133    171.1152   429.7114   5.27e-06 |\n  |   675   542.7576   240.6597   302.0979    171.4897   432.7061   5.80e-06 |\n  |   733   577.9091    254.969   322.9401     176.861   469.0191   .0000147 |\n  |   841   643.3636   279.1917   364.1719    187.8796   540.4643   .0000515 |\n  |   852   650.0303   281.6588   368.3715    188.9058   547.8372   .0000575 |\n  |--------------------------------------------------------------------------|\n  |   915   688.2121   294.2187   393.9934    196.3119   591.6749   .0000937 |\n  |   941   703.9697   299.4022   404.5675    199.2493   609.8857   .0001125 |\n  |   979        727    306.978    420.022    203.4389   636.6051   .0001441 |\n  |   995   734.7576   310.1678   424.5898    204.0643   645.1152   .0001609 |\n  |  1032   748.2121   317.5443   430.6678    202.7468   658.5889   .0002127 |\n  |--------------------------------------------------------------------------|\n  |  1141   787.8485   335.6531   452.1953    200.7097    703.681   .0004248 |\n  |  1321    853.303   365.5577   487.7454    191.5434   783.9473   .0012492 |\n  |  1386   876.9394   376.3565   500.5829    186.9499   814.2158   .0017585 |\n  |  1400   882.0303   378.2173    503.813    186.4392   821.1869   .0018625 |\n  |  1407   884.5757   379.1476   505.4281    186.1745   824.6817   .0019162 |\n  +--------------------------------------------------------------------------+\n\n\n\n\n\nComparaison des Rmst à chaque jour où au moins un décès est observé"
  },
  {
    "objectID": "06-intro_mod.html",
    "href": "06-intro_mod.html",
    "title": "7  Introduction",
    "section": "",
    "text": "8 Proprortionalité des risques\nLa spécification usuelle d’un modèle à risque proportionnel est:\n\\[h(t)=h_0(t)\\times e^{X^{'}b}\\]\nLe risque de base\n\\(h(t)=h_0(t)\\) donc \\(e^{X^{'}b}=1\\). Observations pour lesquelles \\(X=0\\)\nRisques proportionnels\nCette hypothèse stipule l’invariance dans la durée du rapport des risques (hazard ratio).\nExemple:\nAvec une seule covariable \\(X\\) introduite au modèle, et 2 observations disons \\(A\\) et \\(B\\):\nLe rapport des risques entre \\(A\\) et \\(B\\) est simplement égal à:\n\\[\\frac{h_A(t)}{h_B(t)}= \\frac{e^{bX_A}}{e^{bX_B}}=e^{b(X_A-X_B)}\\]\nAutrement dit, cette proportionnalité des risques est la traduction d’une absence d’interaction entre les rapports de risques estimés par un modèle à risque proportionnel et la durée (ou une fonction de celle-ci).\nSi on part d’un modèle tel que \\(h_0(t)=0.1\\) quelque soit \\(t\\) (baseline à risque constant).\nSi \\(h_1(t)\\) est lui même constant, le rapport entre \\(h_1(t)\\) et \\(h_0(t)\\) sera lui même constant dans la durée. On dit que les risques sont proportionnels. Ici, \\(h_1(t)=0.2\\) quel que soit \\(t\\), le rapport des risques est toujours égal à \\(\\frac{0.2}{0.1}=2=e^{b}\\). Le paramètre estimé par un modèle à risque proportionnel sera égal à \\(log(2)=0.69\\).\nPour \\(h_{1b}(t)\\), le risque augmente de manière à un rythme constant (linéaire): \\(h_{1b}(1)=0.15\\) et \\(h_{1b}(1000)=0.25\\). Comme \\(h_0(t)*\\) est constant, le rapport des risques s’accroît également. On dit que les risques ne sont pas proportionnels.\nSi on est dans le deuxième cas de figure, un modèle à risque proportionnel estimera un rapport toujours égal à 2. Il estimera un rapport moyen sur la période d’observation."
  },
  {
    "objectID": "06-intro_mod.html#footnotes",
    "href": "06-intro_mod.html#footnotes",
    "title": "7  Introduction",
    "section": "",
    "text": "On rappelera qu’en durée continue, seule positivité du risque doit être assurée, d’où l’expression hazard rate↩︎"
  },
  {
    "objectID": "07-cox.html#le-modèle-semi-paramétrique-de-cox",
    "href": "07-cox.html#le-modèle-semi-paramétrique-de-cox",
    "title": "8  Le modèle de Cox",
    "section": "8.1 Le modèle semi-paramétrique de Cox",
    "text": "8.1 Le modèle semi-paramétrique de Cox\n\n8.1.1 La vraisemblance partielle et estimation des paramètres\nOn se situe dans une situation où la durée est mesurée sur une échelle strictement continue. Il ne peut donc y avoir qu’un seul évènement observé en \\(t_i\\) (idem pour la censure).\nOn peut représenter le processus aléatoire d’une analyse de survie en présence de censure à droite, avec l’équation de vraisemblance suivante:\n\\[L_i=f(t_i)^{\\delta_i}S(t_i)^{1-\\delta_i}\\]\n\n\\(f(t_i)\\) est la valeur de la fonction de densité en \\(t_i\\)\n\\(S(t_i)\\) est la valeur de la fonction de survie en \\(t_i\\)\n\\(\\delta_i=1\\) si l’évènement est observé: \\(L_i=f(t_i)\\)\n\\(\\delta_i=0\\) si l’observation est censurée: \\(L_i=S(t_i)\\)\n\nVraisemblance partielle de Cox\nComme \\(f(t_i)=h(t_i)\\times S(t_i)\\) 1, on obtient: \\(L_i=[h(t_i)S(t_i)]^{\\delta_i}S(t_i)^{1-\\delta_i} = h(t_i)^{\\delta_i}S(t_i)\\).\nPour \\(i=1,2,.....,n\\), la vraisemblance s’ecrit donc: \\(L_i=\\prod_{i=1}^{n}h(t_i)^{\\delta_i}S(t_i)\\).\nOn peut réécrire cette vraisemblance en la multipliant et en la divisant par: \\(\\sum_{j\\in R_i}h(t_i)\\), où \\(j\\in R_i\\) est l’ensemble des observations soumises au risque en \\(t_i\\).\n\\[L=\\prod_{i=1}^{n}\\left[h(t_i)\\frac{\\sum_{j\\in R}h(t_i)}{\\sum_{j\\in R}h(t_i)}\\right]^{\\delta_i}S(t_i)= \\prod_{i=1}^{n}\\left[\\frac{h(t_i)}{\\sum_{j\\in R_i}h(t_i)}\\right]^{\\delta_i}\\sum_{j\\in R_i}h(t_i)^{\\delta_i}S(t_i)\\]\nLa vraisemblance partielle retient seulement le premier terme de la vraisemblance, soit:\n\\[PL=\\prod_{i=1}^{n}\\left[\\frac{h(t_i)}{\\sum_{j\\in R}h(t_i)}\\right]^{\\delta_i}\\]\nUne fois remplacée la valeur de \\(h(t_i)\\) par son expression en tant que modèle à risques proportionnels, la vraisemblance partielle ne dépendra plus de la durée. Mais elle va dépendre de l’ordre d’arrivée des évènements, c’est à dire leur rang.\nRemarque: pour les observations censurées(\\(\\delta_i=0\\)), \\(PL=1\\). Toutefois, ces censures à droite entrent dans l’expression \\(\\sum_{j\\in R}h(t_i)\\) tant qu’elles sont soumises au risque.\nEn remplaçant donc \\(h(t_i)\\) par l’expression \\(h_0(t)e^{X_i^{'}b}\\):\n\\[PL=\\prod_{i=1}^{n}\\left[\\frac{h_0(t)e^{X_{i}^{'}b}}{\\sum_{j\\in R_i}h_0(t)e^{X_{j}^{'}b}}\\right]^{\\delta_i} =\\prod_{i=1}^{n}\\left[\\frac{e^{X_i^{'}b}}{\\sum_{j\\in R_i}e^{X_{j}^{'}b}}\\right]^{\\delta_i}\\]\nL’expression \\(\\frac{e^{Xb}}{\\sum_{j\\in R}e^{Xb}}\\) est donc bien une probabilité, et la vraisemblance partielle est donc bien un produit de probabilités. Pour un individu ayant connu l’évènement, la contribution à la vraisemblance partielle est la probabilité que l’individu observe l’évènement en \\(t_i\\) sachant qu’un évènement (et un seul) s’est produit.\n\nSi \\(\\delta_i = 0\\): \\(PL_i = 1\\)\nSi \\(\\delta_i = 1\\): \\(PL_i =\\frac{e^{X_i^{'}b}}{\\sum_{j\\in R_i}e^{X_{j}^{'}b}}\\)\n\nCondition nécessaire: pas d’évènement simultané: en présence d’évènements mesurés simultanément, l’estimation de la vraisemblance doit faire l’objet d’une correction.\nCorrection de la vraisemblance avec des évènements simultanés:\n\nLa méthode dite exacte: Comme il ne doit pas y avoir d’évènement simultané, on va introduire à la vraisemblance partielle toutes les permutations possibles des évènements observés au même moment. Bien qu’en \\(t_i\\) on observe au même moment l’évènement pour 2 observations (A,B) une métrique temporelle plus précise permettrait de savoir si A s’est produit avant B ou B s’est produit avant A (2 permutations). Comme le nombre de permutations est calculé à l’aide d’une factorielle 2, avec 3 évènements mesurés simultanément, on obtient 6 permutations (\\(3\\times2\\times1\\)). Problème: le nombre de permutations pour chaque \\(t_i\\) peut devenir très vite particulièrement élevé. Par exemple pour 10 évènements simultanés, le nombre de permutations est égal à 3,628,800. Le temps de calcul devient extrêmement long, et ce type de correction totalement inopérant.\nLa méthode dite de Breslow: il s’agit d’une approximation de la méthode exacte permettant de ne pas avoir à intégrer chaque permutation. Cette approximation est utilisée par défaut par les logiciels Sas et Stata.\nLa méthode dite d’Efron: elle corrige l’approximation de Breslow, et est jugée plus proche de la méthode exacte. C’est la méthode utilisée par défaut avec le logiciel R, et elle est disponible avec les autres applications.\n\n\n\n8.1.2 Estimation des paramètres\nOn utilise la méthode habituelle, à savoir la maximisation de la log-vraisemblance (ici partielle).\n\nConditions de premier ordre: calcul des équations de score à partir des dérivées partielles. Solution: \\(\\frac{\\partial log(PL)}{\\partial{b_k}}=0\\). On ne peut pas obtenir de solution numérique directe.\n\nRemarque: les équations de score sont utilisées pour tester la validité de l’hypothèse de constance des rapports de risque pour calculer les résidus de Schoenfeld (voir plus loin).\n\nConditions de second ordre: calcul des dérivées secondes qui permettent d’obtenir la matrice d’information de Fisher et la matrice des variances-covariances des paramètres.\nComme il n’y a pas de solution numérique directe, on utilise un algorithme d’optimisation (ex: Newton-Raphson) à partir des équations de score et de la matrice d’information de Fisher.\n\nEléments de calcul\nEn logarithme (sans évènement simultané), la vraisemblance partielle s’ecrit:\n\\[pl(b)=\\sum_{i=1}^n\\delta_i\\left(log(e^{X_{i}^{'}b})-log\\sum_{j\\in R_i}e^{X_{j}^{'}b}\\right)\\]\n\\[pl(b)=\\sum_{i=1}^n\\delta_i\\left(X_{i}^{'}b-log\\sum_{j\\in R_i}e^{X_{j}^{'}b}\\right)\\]\nCalcul de l’équation de score pour une covariable \\(X_k\\):\n\\[\\frac{\\partial pl(b)}{\\partial{b_k}}=\\sum_{i=1}^n\\delta_i\\left(X_{ik}-\\sum_{j\\in R_i}X_{jk}\\frac{e^{X_{j}^{'}b}}{\\sum_{j\\in R_i}e^{X_{j}^{'}b}}\\right)\\]\nComme \\(\\frac{e^{X_{j}b}}{\\sum_{j\\in R}e^{X_{j}b}}\\) est une probabilité, et \\(\\sum_{j\\in R}X_{ik}\\times p_i\\) est l’espérance (la moyenne) \\(E(X_k)\\) d’avoir la caractéristique \\(X_k\\) lorsqu’un évènement a été observé. Au final:\n\\[\\frac{\\partial lp(b)}{\\partial{b_k}}= \\sum_{i=1}^n\\delta_i\\left(X_{ik} - E(X_{j\\in R_i,k}) \\right)\\]\nCette expression va permettre d’analyser le respect ou non de l’hypothèse de risques proportionnels via les résidus de Schoenfeld.\n\n\n8.1.3 Lecture des résultats\nComme il s’agit d’un modèle à risque proportionnel, les rapports de risques sont constants pendant toute la période d’observation. Il s’agit d’une propriété de l’estimation.\nCovariable binaire (indicatrice) \\(X=(0,1)\\): \\(RR=\\frac{h(t\\ |\\ X=1)}{h(t\\ |\\ X=0)}=e^b\\).\nA chaque moment de la durée \\(t\\), le risque d’observer l’évènement est \\(e^b\\) fois plus important/plus faible pour \\(X=1\\) que pour \\(X=0\\).\nCovariable quantitative (fixe dans le temps)\n\\(RR=\\frac{h(t\\ |\\ X=a+c)}{h(t\\ |\\ X=a)}=e^{c \\times{b}}\\). On prendra pour illustrer une variable type âge au début de l’exposition au risque (a) et un delta de comparaison avec un âge inférieur c.\nSi \\(c=1\\) (résultat de l’estimation): A un âge donnée, le risque de connaitre l’évènement est \\(e^b\\) fois inférieur/supérieur à celui d’une personne qui a un an de moins.\nExemple pour les insuffisances cardiaques\n\nCorrection de la vraisemblance: méthode d’Efron\nNombre d’observations: 103\nNombre de décès: 75\nLog-Vraisemblance: -289.30639\n\n\nCox: log Hazard Ratio (Risks Ratio)\n\n\nVariables\nlogRR\nStd.Err\nz\n\\(P&gt;|z|\\)\n95% IC\n\n\n\n\nyear\n-0.119\n0.0673\n-1.78\n0.076\n-0.2516;+0.0124\n\n\nage\n+0.0296\n0.0135\n2.19\n0.029\n+0.0031;+0.0561\n\n\nsurgery\n-0.9873\n0.4363\n-2.26\n0.024\n-1.8424;-0.1323\n\n\n\n\nCox: Hazard Ratio (Risks Ratio)\n\n\nVariables\nRR\nStd.Err\nz\n\\(P&gt;|z|\\)\n95%CI\n\n\n\n\nyear\n0.8872\n0.0597\n-1.78\n0.076\n0.7775; 1.0124\n\n\nage\n1.0300\n0.0139\n2.19\n0.029\n1.0031; 1.0577\n\n\nsurgery\n0.3726\n0.1625\n-2.26\n0.024\n0.1584; 0.8761\n\n\n\nOn retrouve les des tests non paramétriques pour l’opération, à savoir qu’un pontage réduit les risques journaliers de décès pendant la période d’observation (augmente la durée de survie).\nDe la même manière, plus on entre à un âge élevé dans la liste d’attente plus le risque de décès augmente. La variable year, qui traduit des progrès en médecine, renvoie à une réduction plutôt modérée du risque journalier de décès durant l’attente d’une greffe.\nR-Stata-Sas-Python\n\nR\n\n\nLe modèle est estimé avec la fonction coxph de la librairie survival. Hors options, la syntaxe est identiques aux fonctions survfit et survdif.\n\n8.1.3.1 Stata\nLe modèle est estimé avec la commande stcox.\n\n\n8.1.3.2 SAS\nLe modèle est estimé avec la proc phreg.\n\n\n8.1.3.3 Python\nAvec la librairie lifelines, le modèle est estimé avec la fonction CoxPHFitter. Avec la librairie statmodels, il est estimé avec la fonction smf.phreg."
  },
  {
    "objectID": "07-cox.html#analyse-de-la-constance-des-rapports-de-risque",
    "href": "07-cox.html#analyse-de-la-constance-des-rapports-de-risque",
    "title": "8  Le modèle de Cox",
    "section": "8.2 Analyse de la constance des rapports de risque",
    "text": "8.2 Analyse de la constance des rapports de risque\n\nLes rapports de risque (RR) estimés par le modèle sont contraints à être constant sur toute la période d’observation. C’est une hypothèse forte.\nLe respect de cette hypothèse doit être analysé, en particulier pour le modèle de Cox où la baseline du risque est habituellement estimée à l’aide de ces rapports (par exemple la méthode dite de Breslow, non traitée). En post-estimation, les valeurs estimées du risque pourront présenter des valeurs aberrantes si on dévie trop de constance, en particulier en obtenant des négatives des taux de risque.\nAnalyser cette hypothèse revient à introduire une interaction entre les rapports et la durée ou plutôt précisément une fonction de la durée).\nPlusieurs méthodes disponibles, on traitera celles basées sur les résidus de Schoenfeld, et l’introduction directe d’une intéraction entre une fonction la durée et les covariables du modèle. Cette dernière fait également office de méthode de correction lorsque la violation de l’hypothèse est jugée trop importante ou problématique du point de vue des résultats obtenus.\nSi on regarde les courbes de Kaplan-Meier, leurs croisement non tardif impliquera nécessairement un problème sur cette hypothèse.\n\n\n8.2.1 Test de Grambsch-Therneau sur les résidus de Schoenfeld\nCe test a été proposé par P.Grambsch et T.Therneau 3 dans un cadre à durée strictement continue. Il repose originellement sur une régression linéaire estimé avec les moindres carrés généralisés (GLS) correction de l’autocorrélation des erreurs avec des sér) . Dans un premier temps pour des raisons plutôt pratiques (informatique), le test a une version moindres carrés ordinaires (OLS). Jusqu’en 2020, tous les logiciels ne proposaient que le test OLS. T.Therneau avec la V3 de package survival a substitué - assez brutalement - le test GLS au test OLS. Si les résultats sont proches dans le cadre d’une durée continue et que le test GLS peut être considéré comme un test exact, cela devient problématique dans une situation de durée discrète/groupée 4. Le test OLS reste, à mon sens, la méthode à privilégier dans le cas discret.\nIl est également important de souligner que pour P.Grambsch et T.Therneau 5 n’est qu’un moyen parmi d’autres d’analyser une violation de l’hypothèse de proportionnalité. Ce n’est pas the solution (comme tout autre test au passage). Le croisement des courbes de séjours peut-être suffisant pour alerter sur cette violation.\nPrincipe du test: consiste à regarder la corrélation entre les résidus de Schoenfeld obtenus directement avec la fonction de score de la vraisemblanc partielle de Cox et une fonction de la durée.\nPrincipe de calcul des résidus\n\nLes résidus bruts sont directement calculés à partir des équations de scores [voir section estimation].\nIls ne sont calculés que pour les observations qui ont connues l’évènement, au moment où un évènement s’est produit.\nLa somme des résidus pour chaque covariable est égale à 0. Il s’agit de la propriété de l’équation de score à l’équilibre.\nOn utilise généralement les résidus standardisés (remis à l’échelle / scaled) - par leur variance -. C’est la mesure de cette variance qui distingue le test OLS du test GLS.\n\nPour une observation dont l’évènement s’est produit en \\(t_i\\), le résidu brut de Schoenfeld pour la covariable \\(X_k\\), après estimation du modèle, est égal à:\n\\[rs_{ik}=X_{ik}- \\sum_{j\\in R_i}X_{jk}\\frac{e^{X_{j}^{'}b}}{\\sum_{j\\in R_i}e^{X_{j}^{'}b}}= X_{ik} - E(X_{j\\in R_i})\\]\n\nCe résidu est formellement la contribution d’une observation ou d’un moment d’évènement au score. Il se lit comme la différence entre la valeur observée d’une covariable et sa valeur espérée au moment où l’évènement s’est produit.\nSi la constance des rapports de risque varie peu les résidus ne doivent pas suivre une tendance précise localement ou globalement, à la hausse ou à la baisse.\n\nPourquoi?\nPar l’exemple, sans censure à droite et en ne considérant que les résidus bruts: Avec un rapport de risque strictement égal à 1 en début d’exposition, une population soumise au risque \\(R_i=100\\) avec 50 hommes et 50 femmes. Si l’hypothèse PH (strictement) respectée, lorsqu’il reste 90 personnes soumises au risque, on devrait avoir 45 hommes et 45 femmes. Avec \\(R_i=50\\), 25 hommes et 25 femmes,…….avec \\(R_i=10\\), 5 hommes et 5 femmes.\nAu final l’espérance d’avoir la caractéristique \\(X\\) est toujours égal à 0.5 et les résidus bruts prendront toujours la valeur -.5 si \\(X=0\\) et .5 si \\(X=1\\). En faisant une simple régression linéaire entre les résidus, qui alternent ces deux valeurs, et \\(t\\), le coefficient estimé sera en toute logique très proche de 0.\nDe manière encore plus simple, cette proportionnalité avec un risque ratio égal à 1 suggère qu’au cours de la durée d’observation, on observe une succession d’un même nombre d’hommes et de femmes qui connaissent l’évènement. Si tous les hommes ou presque avaient observés l’évènement plutôt en début d’éxposition et si toutes les femmes ou presque avaient observé l’évènement plutôt en fin d’exposition, l’hypothèse de proportionnalité pourraient fortement remise en cause.\nOn trouvera des éléments de calcul du test OLS ici\n\n\n\n\n\n\nAvertissement\n\n\n\n\nTest omnibus: Ne pas l’utiliser bien qu’il figure généralement en bas des output. Il n’a pas d’interprétation directe, et les p-value peuvent présenter des valeurs très faibles alors que ce n’est pas le cas pour les covariables prises une à une. Rester comme c’est souvent le cas à un test à un degré de liberté.\nTransformations de la durée: n’importe quelle fonction de la durée peut être utilisée pour réaliser le test. On retient généralement les fonctions suivantes: \\(g(t)=t\\) (« identity »), \\(g(t)=log(t)\\), \\(g(t)=KM(t)\\) ou \\(g(t)=1- S(t\\)) où \\(S(t\\)) est l’estimateur de Kaplan-Meier. Enfin une transformation appelée « rank » est utilisée seulement pour les durées strictement continue ou suffisamment dispersées . Par exemple \\(t=(0.1,0.5,1,2.6,3)\\) donne une transformation \\(t=(1,2,3,4)\\). A savoir : $g(t)=t rend le test relativement sensible aux évènements tardifs lorsque la population restant soumise est peu nombreuse (outliers).\nPar défaut Stata, Sas, Python: \\(g(t)=t\\)\nPar défaut R: \\(g(t)= 1 - S(t)\\)\n\n\n\nPour des raisons de reproductibilité dans l’espace des logiciels et dans le temps pour les différentes versions du package survival de R, on ne présente ici que la version OLS.\nTest OLS avec \\(g(t)=t\\)\n\nTest OLS Grambsch-Therneau avec \\(g(t)=t\\)\n\n\nVariables\nchi2\ndf\nP&gt;Chi2\n\n\n\n\nyear\n0.80\n1\n0.3720\n\n\nage\n1.61\n1\n0.2043\n\n\nsurgery\n5.54\n1\n0.0186\n\n\n\nIci l’hypothèse de proportionnalité des risques est questionnable pour la variable surgery. Le risque ratio pourrait ne pas constant dans le temps. Ce n’est pas du tout étonnant, le premier décès pour les personnes opérées d’un pontage n’est observé qu’au bout de 165 jours. Au final, un test était-il bien nécessaire pour arriver à ce constat ???????\nTest OLS avec \\(g(t)=1- S(t)\\)\n\nTest Grambsch-Therneau avec \\(g(t)=1- S(t)\\)\n\n\nVariables\nchi2\ndf\nP&gt;Chi2\n\n\n\n\nyear\n1.96\n1\n0.162\n\n\nage\n1.15\n1\n0.284\n\n\nsurgery\n3.96\n1\n0.046\n\n\n\nR-Stata-Sas-Python\n\nR\n\n\nAttention seulement version GLS du test depuis le V3 de survival.\n\nAprès avoir créer un objet à l’estimation du modèle de Cox, on utilise la fonction cox.zph. Cette fonction utilise par défaut \\(g(t)=1-S(t)\\) où \\(S(t)\\) sont les estimateurs de la courbe de Kaplan-Meier. On peut modifier cette fonction. Il est préférable de conserver cette fonction par défaut.\nTest OLS: j’ai récupéré le programme du test antérieur, renommé cox.zphold. On peut le charger simplement, et il est facilement exécutable. Pour le charger: source(\"https://raw.githubusercontent.com/mthevenin/analyse_duree/main/cox.zphold/cox.zphold.R\")\n\n\n8.2.1.1 Stata\nLe test (OLS) est obtenu avec la commande estat phtest, d. Par défaut Stata utilise \\(g(t)=t\\). On peut modifier cette fonction.\n\n\n8.2.1.2 SAS\nLe test (OLS) est disponible depuis quelques années avec l’argument zph sur la ligne proc lifetest. Par défaut SAS utilise \\(g(t)=t\\). On peut modifier cette fonction.\n\n\n8.2.1.3 Python\nLe test (OLS) est donné avec la fonction proportional_hazard_test de la librairie lifelines. La fonction utilise par défaut \\(g(t)=t\\), mais on peut afficher les résultats pour toutes les transformations de \\(t\\) disponibles avec l’option time_transform=’all’.\n\n\n\n\n\n\n8.2.2 Intéraction avec la durée\nPetit retour sur l’estimation du modèle\nPour estimer le modèle de Cox, les données sont dans un premier temps splitées aux moment où au moins un évènement a été observé.\nSur l’application, avec 2 individus avec la covariable age (rappel: il s’agit de l’âge en \\(t_0\\):\n\nBase spittées sur les intervals d’évènement\n\n\nid\nage\ndied\n\\(t_0\\)\n\\(t\\)\n\n\n\n\n2\n51\n0\n0\n1\n\n\n2\n51\n0\n1\n2\n\n\n2\n51\n0\n2\n3\n\n\n2\n51\n0\n3\n5\n\n\n2\n51\n1\n5\n6\n\n\n3\n54\n0\n0\n1\n\n\n3\n54\n0\n1\n2\n\n\n3\n54\n0\n2\n3\n\n\n3\n54\n0\n3\n5\n\n\n3\n54\n0\n5\n6\n\n\n3\n54\n0\n6\n8\n\n\n3\n54\n0\n8\n9\n\n\n3\n54\n0\n9\n12\n\n\n3\n54\n1\n12\n16\n\n\n\nLes bornes des intervalles \\([t_0;t]\\) présentent des valeurs seulement lorsqu’un évènement s’est produit (principe de la vraisemblance partielle). Il n’y a donc pas de valeurs pour \\(t\\) et \\(t_0\\) en \\(t=4\\) pour \\(id=(2,3)\\)et \\(t=7,10,11,13,14,15\\) pour \\(id=3\\).\nLes deux individus observent l’évènement en \\(t=6\\) pour \\(id=2\\), et en \\(t=16\\) pour \\(id=3\\). Avant ce moment la valeur de la variable évènement/censure (ici \\(d\\)) prend toujours la valeur 0, et prend la valeur 1 le jour du décès.\nSur cette base splitée aux moments d’évènement (n=3573), on pourra vérifier facilement que les résultats obtenus par le modèle de Cox sont identiques à ceux obtenus précédemment.\nIntroduction d’une intéraction avec une fonction de la durée\nOn a une variable de durée (on prendra \\(g(t)=t\\)) qui sera croisée avec la variable surgery.\nLe modèle s’écrit:\n\\[h(t | X,t) = h_0(t)e^{b_1age + b_2year + b_3 surgery + b_4 (surgery\\times t)}\\]\nLe modèle avec cette intéraction donne les résultats suivants:\n\nModèle de Cox avec une intéraction entre une fonction de la durée et la variable *surgery\n\n\n\n\n\n\n\n\n\n\nVariable\n\\(e^b\\)\nStd.err\nz\nP&gt;|z|\n95% IC\n\n\n\n\nyear\n0.884\n0.059\n-1.84\n0.066\n0.776 ; 1.008\n\n\nage\n1.029\n0.014\n+2.15\n0.032\n1.003 ; 1.057\n\n\n\\(surgery(t_{0+})\\)\n0.173\n0.117\n-2.60\n0.009\n0.046 ; 0.649\n\n\n\\(surgery\\times t\\)\n1.002\n0.001\n+2.02\n0.043\n1.000 ; 1.004\n\n\n\nOn retrouve donc un résultat proche de celui obtenu à partir du test OLS sur les résidus de Schoenfeld pour la variable surgery. Et c’est normal. Avec \\(g(t)=t\\), il a le mérite de pouvoir être interprété directement. Ce qui ne veut pas dire qu’il s’agit de la meilleure solution.\nDonc, malgré une hypothèse plutôt forte sur la forme fonctionnelle de l’intéraction, et dans les faits surement pas pertinente, on peut dire que chaque jour le rapport des risques entre personnes opérées et personnes non opérées augmente de +0.2%. Pour plus précis, étant à l’origine &lt;1, l’écart se modère. L’effet de l’opération sur la survie des individus s’estompe donc avec le temps.\nA noter\n\nLe modèle n’est plus un modèle à risque proportionnel. La variable surgery n’est plus une variable fixe mais une variable tronquée dynamique qui prend la valeur de \\(t\\) pour les personnes qui ont été opérées d’un pontage avant leur entrée dans le registre de greffe.\n\nSi \\(surgery = 0\\)\n\n\n\nid\nsurgery\ndied\n\\(t_0\\)\n\\(t\\)\nsurgery*t\n\n\n\n\n2\n0\n0\n0\n1\n0\n\n\n2\n0\n0\n1\n2\n0\n\n\n2\n0\n0\n2\n3\n0\n\n\n2\n0\n0\n3\n5\n0\n\n\n2\n0\n1\n5\n6\n0\n\n\n\nSi \\(surgery = 1\\) (jusqu’à \\(t=6\\) car aucun décès précoce pour ce groupe)\n\n\n\nid\nsurgery\ndied\n\\(t_0\\)\n\\(t\\)\nsurgery*t\n\n\n\n\n40\n1\n0\n0\n1\n1\n\n\n40\n1\n0\n1\n2\n2\n\n\n40\n1\n0\n2\n3\n3\n\n\n40\n1\n0\n3\n5\n5\n\n\n40\n1\n1\n5\n6\n6\n\n\n\nExemple pour une variable quantitative (age)\n\n\n\nid\nage\ndied\n\\(t_0\\)\n\\(t\\)\nage*t\n\n\n\n\n2\n51\n0\n0\n1\n51\n\n\n2\n51\n0\n1\n2\n102\n\n\n2\n51\n0\n2\n3\n153\n\n\n2\n51\n0\n3\n5\n255\n\n\n2\n51\n1\n5\n6\n306\n\n\n\n\nL’altération des rapports de risque dépend de la forme fonctionnelle de l’intéraction choisie. Ici la variation dans la durée du rapport des risque est constante, ce qui est une hypothèse assez forte. On a, en quelques sorte, réintroduit une hypothèse de proportionnalité, ici sur le degré d’altération des écarts de risques dans le temps, qui devient lui même strictement constant.\n\n\n\n8.2.3 Que faire ?\nNe rien faire\nOn interprète le risque ratio comme un ratio moyen pendant la durée d’observation (P.Allison). Difficilement soutenable pour l’analyse des effets cliniques, elle peut être envisagée dans d’autres domaines. Attention au nombre de variables qui ne respectent pas l’hypothèse, l’estimation de la baseline du risque pourrait être sensiblement affectée si l’analyse a des visée prédictives. Il convient tout de même lors de l’interprétation, de préciser les variables qui seront analysées sous cette forme très « moyenne » sur la période d’observation.\nOn peut également adapter cette stratégie du « ne rien faire » selon sens de l’altération des rapports de risque. Si aux cours du temps des écarts de risque, s’accentuent à la hausse comme à la baisse, on peut conserver cet estimateur moyen. Mais si cette non proportionnalité conduit à un changement du sens des rapport de risque je suis moins convaincu de la pertinence de cette stratégie. Encore une fois, et il faut le rappeler, l’estimation des courbes de survie doit permette d’anticiper ce dernier cas de figure.\nIl faut également tenir compte de l’intérêt portée par les variables qui présentent un problème par rapport à l’hypothèse. Il n’est peut-être pas nécessaire de complexifier le modèle pour des variables introduites comme simples contrôles.\nMais plus problématique [important]… On sait qu’une des causes du non respect de l’hypothèse peut provenir d’effets de sélection liées à des variables omises ou non observables. En analyse de durée ce problème prend le nom de frailty (fragilité) lorsque cette non homogénéité n’est pas observable. Des estimations, plus complexes, sont possibles dans ce cas, et sont en mesure malgré leur interprétation plutôt difficile de régler le problème. Il convient donc de bien spécifier le modèle au niveau des variables de contrôle observables et disponibles.\nModèle de Cox stratifié\nUtiliser la méthode dite de « Cox stratifiée » (non traitée). Utile si l’objectif est de présenter des fonctions de survie prédites ajustées, et si une seule covariable (binaire) présente un problème. Les HR ne seront pas estimés pour la variable qui ne respecte pas l’hypothèse.\nIntéraction\nIntroduire une interaction avec la durée. Cela peut permettre en plus d’enrichir le modèle au niveau de l’interprétation. Valable si peu de covariables présentent des problèmes de stabilité des rapports de risque, dans l’idéal une seule variable. Attention tout de même à la forme de la fonction, dans l’exemple on a contraint l’effet d’interaction à être strictement linéaire, ce qui est une hypothèse plutôt forte…. on introduit de nouveau une contrainte de proportionnalité dans le modèle.\nModèles alternatifs\nUtiliser un modèle alternatif: modèles paramétriques à risques proportionnels si la distribution du risque s’ajuste bien, le modèle paramétrique « flexible » de Parmar-Royston ou un modèle à temps discret. Pour la dernière solution, on peut également corriger la non proportionnalité avec l’introduction d’une intéraction. Si on ne le fait pas, les risques prédits, par définition des probabilités conditionnelles, resteront toujours dans les bornes contrairement au modèle de Cox.\nUtiliser un modèle non paramétrique additif dit d’Aalen ou une de ses variantes (non traité). Mais ces modèles, dont les résultats sont présentés par des graphiques, se commentent assez difficilement.\nForêt aléatoire\nAutre méthode : les forêts aléatoires. L.Breiman a dès le départ proposé une estimation des modèles de survie par cette méthode. Par définition, pas sensible à l’hypothèse PH. Mais cela reste des méthodes à finalité prédictive, moins riche en interprétation."
  },
  {
    "objectID": "07-cox.html#footnotes",
    "href": "07-cox.html#footnotes",
    "title": "8  Le modèle de Cox",
    "section": "",
    "text": "Se reporter à la définition des grandeurs dans la section Théorie↩︎\n\\(n! = (n)\\times(n-1)\\times(n-2)\\times....\\times3\\times2\\times1\\)↩︎\nIl s’agit bien de la personne qui maintient le package survival dans R↩︎\nPour les personnes utilisant R, je donne un moyen pour récupérer et exécuter le test OLS sous R↩︎\nSe reporter à leur ouvrage Modeling Survival Data: Extending the Cox Model (2001)↩︎"
  },
  {
    "objectID": "08-discret.html#organisation-des-données",
    "href": "08-discret.html#organisation-des-données",
    "title": "9  Modèle à durée discrète",
    "section": "9.1 Organisation des données",
    "text": "9.1 Organisation des données\nFormat long\nLes données doivent être en format long: pour chaque individu on a une ligne par durée observée ou par intevalle de durées jusqu’à l’évènement ou la censure. On retrouve le split des données du modèle de Cox, mais généralisé à des intervalles où aucun évènement n’est observé. Avec des données de type discrètes ou groupées, phénomène classique en sciences sociales, il y a souvent peu de différence entre un allongement aux temps d’évènement et aux temps d’observation (voir encadré plus loin sur le modèle d Cox à temps discret).\nDurée\nLa durée est dans un premier temps construite sous forme d’un simple compteur, par exemple \\(t=1,2,3,4,5...\\) (des valeurs non entières sont possibles). Le choix de la forme fonctionnelle de la durée sera présentée plus tard.\nVariable évènement/censure\nSi l’individu a connu l’évènement, elle prend la valeur 0 avant celui-ci. Au moment de l’évènement sa valeur est égale à 1. Pour les observations censurées, la variable prend toujours la valeur 0.\nApplication\nOn reprend les données de la base transplantation, mais les durées ont été regroupées par période de 30 jours. Il n’y a pas de durée mesurée comme nulle, on a considéré que les 30 premiers jours représentaient, le premier mois d’exposition. Cette variable de durée se nomme mois.\nFormat d’origine\n\nDurée discrète: données en format d’origine\n\n\nid\nyear\nage\nsurgery\nmois\ndied\n\n\n\n\n1\n67\n30\n0\n2\n1\n\n\n\nLa personne décède lors du deuxième intervalle de 30 jours\nFormat long et variables pour l’analyse\n\nDurée discrète: données en format long\n\n\nid\nyear\nage\nsurgery\nmois\ndied\nt\n\n\n\n\n1\n67\n30\n0\n2\n0\n1\n\n\n1\n67\n30\n0\n2\n1\n2"
  },
  {
    "objectID": "08-discret.html#ajustement-de-la-durée",
    "href": "08-discret.html#ajustement-de-la-durée",
    "title": "9  Modèle à durée discrète",
    "section": "9.2 Ajustement de la durée",
    "text": "9.2 Ajustement de la durée\nUn des principaux enjeux réside dans la paramétrisation de la durée:\n\nElle peut-être modélisée sous forme de fonction d’une variable de type quantitative/continue.\nElle peut-être modélisée comme variable discrète, de type indicatrice \\({0;1}\\), sur tous les points d’observation ou sous forme de regroupements. Il doit y avoir au moins un évènement observé dans chaque intervalle.\n\n\n\n\n\n\n\nLe modèle de Cox à durée discrète/groupée\n\n\n\nCox est également à l’origine du modèle à durée discrète (je crois également en 1972). Par rapport aux pratiques courantes, la différence repose sur les bornes intervalles, identiques à celles définies pour l’estimation de la courbe de survie KM ou du modèle semi-paramétrique, à savoir une définition sur les moments d’évènement. Avec un ajustement de la durée reposant sur des indicatrices (ajustement sur des variables discrètes), ce modèle est quasiment identique au modèle semi paramétrique.\nOn peut remarquer que dans les sciences sociales, avec des durées assez fortement groupées, les intervalles directement observés et les intervalles définis aux moments d’évènements sont souvent identiques et s’ils ne le sont pas c’est souvent en tout début ou en toute fin de la période d’observation/exposition. En cas d’ajustement de la durée par des indicatrices, la définition des bornes des intervalles aux moments des évènements permet de s’assurer au moins une occurence de l’évènement.\n\n\n\n9.2.1 Ajustement avec une fonction quantitative de la durée\nLe modèle étant paramétrique, on doit trouver une fonction qui ajuste le mieux les données. Toutes transformations de la variable est possible: \\(f(t)=a\\times t\\), \\(f(t)=a\\times log(t)\\)……formes quadratiques. Les ajustements sous forme de splines cubiques tendent à se développer ces dernières années.\nPour sélectionner cette fonction, on peut tester différents modèles sans covariable additionnelle, et sélectionner la forme dont le critère d’information de type vraisemblance pénalisée (AIC, BIC) est le plus faible, avec au moins des différences de -6 ou -8.\nExemple:\nOn va tester les paramétrisations suivantes: une forme linéraire stricte \\(f(t)=a\\times t\\) et des effets quadratiques d’ordres 2 et 3: \\(f(t)=a_1\\times t + a_2\\times t^{2}\\) et \\(f(t)=a_1\\times t + a_2\\times t^{2} + a_3\\times t^{3}\\).\n\n\n\nProbabilité de décéder avec 3 ajustements de la durée\n\n\nCritères AIC\n\n\n\n\\(f(t)\\)\nAIC\n\n\n\n\n\\(a\\times t\\)\n504\n\n\n\\(a_1\\times t + a_2\\times t^{2}\\)\n492\n\n\n\\(a_1\\times t + a_2\\times t^{2} + a_3\\times t^{3}\\)\n486\n\n\n\nOn peut utiliser la troisième forme à savoir \\(a_1\\times t + a_2\\times t^{2} + a_3\\times t^{3}\\) 3.\nEstimation du modèle avec toutes les covariables\n\nModèle logistique à durée discrète (\\(f(t)\\) continue)\n\n\n\n\n\n\n\n\n\n\nVariables\nOR - RR\nStd. err\nz\nP&gt;|z|\n95% IC\n\n\n\n\n\\(t\\)\n0.678\n0.057\n-4.52\n0.000\n0.587 ; 0.810\n\n\n\\(t^2\\)\n1.014\n0.005\n+2.83\n0.005\n1.004 ; 1.024\n\n\n\\(t^3\\)\n1.000\n0.000\n-2.11\n0.035\n1.000 ; 1.000\n\n\n\\(year\\)\n0.876\n0.015\n-1.80\n0.072\n0.758 ; 1.012\n\n\n\\(age\\)\n1.034\n0.163\n+2.27\n0.023\n1.005 ; 1.064\n\n\n\\(surgery\\)\n0.364\n0.110\n-2.25\n0.024\n0.151 ; 0.877\n\n\n\n\n\n\n\n\n\n\nConstante\n0.440\n0.110\n-3.29\n0.001\n0.270 ; 0.718\n\n\n\nRemarque: les variables year et age ont été centrée sur leur moyenne pour rendre la constante interprétable. La constante reporte donc l’Odds de décéder lors des 30 premiers jours d’une personne dont l’âge et l’année à l’entrée dans le registre est égal à l’âge et à l’année moyenne et qui n’a pas été opéré préalablement.\nSi maintenant on estime un modèle de Cox sur ces données journalières groupées, on remarque que les résultats obtenus, et ce n’est pas une surprise, sont très proches.\n\nModèle de Cox\n\n\n\n\n\n\n\n\n\n\nVariables\nOR - RR\nStd. err\nz\nP&gt;|z|\n95% IC\n\n\n\n\n\\(year\\)\n0.878\n0.059\n-1.93\n0.053\n0.769 ; 1.002\n\n\n\\(age\\)\n1.029\n0.014\n+2.13\n0.033\n1.002 ; 1.057\n\n\n\\(surgery\\)\n0.379\n0.165\n-2.22\n0.026\n0.111 ; 0.892\n\n\n\n\n\n9.2.2 Ajustement discret\n\nIl s’agit d’introduire la variable de durée dans le modèle comme une variable catégorielle (indicatrices).\nDémarche pas conseillé si on a beaucoup de points d’observation, ce qui est le cas ici.\nA l’inverse, si peu de points d’observation la paramétrisation avec une durée continue n’est pas conseillé.\nLa correction de la non proportionnalité peut être plus compliquée à mettre en oeuvre.\n\nOn va supposer que l’on ne dispose que de 4 intervalles d’observation. Pour l’exemple, on va créer ces points à partir des quartiles de la durée, et conserver pour chaque personne une seule observation par intervalle.\n\n\\(t=1\\): Entre le début de l’exposition et 4 mois.\n\\(t=2\\): Entre 5 mois et 11 mois .\n\\(t=3\\): Entre 12 mois et 23 mois.\n\\(t=4\\): 24 mois et plus.\n\nOn va estimer le risque globalement sur l’intervalle. La base sera plus courte que la précédente (197 observations pour 103 individus). Il ne sera plus possible ici d’interpréter les résultats en termes de rapport de probabilité, l’évènement devenant trop fréquent à l’intérieur de chaque intervalle.\n\nModèle logistique à durée discrète (\\(f(t)\\) indicatrices)\n\n\n\n\n\n\n\n\n\n\nVariables\nOR - RR\nStd. err\nz\nP&gt;|z|\n95% IC\n\n\n\n\n\\(0-4 mois\\)\n2.811\n1.177\n+2.47\n0.014\n1.237 ; 6.387\n\n\n\\(5-11 mois\\)\nref\n-\n-\n-\n-\n\n\n\\(12-23 mois\\)\n0.559\n0.346\n-0.94\n0.347\n0.166 ; 1.881\n\n\n\\(24-46 mois\\)\n1.741\n1.159\n+0.83\n0.405\n0.472 ; 6.417\n\n\n\\(year\\)\n0.816\n0.076\n-2.18\n0.029\n0.680 ; 0.980\n\n\n\\(age\\)\n1.048\n0.019\n+2.53\n0.011\n1.011 ; 1.087\n\n\n\\(surgery\\)\n0.330\n0.166\n-2.21\n0.027\n0.123 ; 0.882\n\n\n\n\n\n\n\n\n\n\nConstante\n0.407\n0.151\n2.43\n0.015\n0.198 ; 0.840\n\n\n\nOn trouve des résultats proches de ceux éstimés avec un ajustement continu de la durée. C’est normal, la dirée fait office de variable d’ajustement peu ou pas corrélée avec les autres variables introduites.\n\n\n\nVariables\nAjustement discret\nAjustement continu\n\n\n\n\n\\(year\\)\n0.816\n0.876\n\n\n\\(age\\)\n1.048\n1.034\n\n\n\\(surgery\\)\n0.330\n0.364\n\n\n\n\n\n\n\n\n\nLien avec des modèles usuels à durée continue\n\n\n\nSi la durée discrète/groupée sous tend une durée continue (ce qui est clairement le cas ici):\n\nOn l’a déjà souligné, l’ajustement avec des durées sous forme d’indicatrices correspond au modèle à durée discrète défini par Cox, et le plus proche du modèle semi-paramétrique. Il est également assimilable à un modèle de type exponential piecewice constant (donc un modèle de poisson).\nSi l’ajustement se fait en utilisation une transformation de la durée par une fonction:\n\n\\(f(t)= log(t)\\) correspond à un modèle de Weibull à risque proportionnel 4.\n\nSi l’ajustement se fait avec des splines cubiques, le modèle à durée discrète correspond à un modèle de type Parmar-Royston. Avec une forme quadratique classique, on peut également obtenir cette correspondance."
  },
  {
    "objectID": "08-discret.html#proportionnalité-des-risques",
    "href": "08-discret.html#proportionnalité-des-risques",
    "title": "9  Modèle à durée discrète",
    "section": "9.3 Proportionnalité des risques",
    "text": "9.3 Proportionnalité des risques\n\nFormellement un modèle logistique à temps discret repose sur une hypothèse d’Odds proportionnel [Odds ratios constants pendant la durée d’observation]. Contrairement au modèle de Cox, l’estimation des probabilités (risque) n’est pas biaisée si l’hypothèse PH n’est pas respectée, les paramètres estimés sont considérés au pire comme des approximation.\nComme pour le modèle de Cox, la correction de la non proportionnalité peut se faire en intégrant une interaction avec la durée dans le modèle.\n\nAvec un ajustement continue, on remarque de nouveau que le résultat du modèle est de nouveau très proche de celui estimé avec un modèle de Cox.\n\nModèle logistique à durée discrète avec correction de la non proportionnalité\n\n\n\n\n\n\n\n\n\n\nVariables\nOR - RR\nStd. err\nz\nP&gt;|z|\n95% CI\n\n\n\n\n\\(t\\)\n0.702\n0.059\n-4.2\n0.000\n0.595 ; 0.828\n\n\n\\(surgery(t=0)\\)\n0.155\n0.108\n-2.67\n0.008\n0.039 ; 0.609\n\n\n\\(surgery\\times t\\)\n1.072\n0.036\n2.08\n0.037\n1.004 ; 1.145\n\n\n\\(t^2\\)\n1.013\n0.005\n2.37\n0.018\n1.002 ; 1.023\n\n\n\\(t^3\\)\n1.00\n0.000\n-1.71\n0.086\n1.000 ; 1.000\n\n\n\\(year\\)\n0.872\n0.064\n-1.86\n0.062\n0.755 ; 1.007\n\n\n\\(age\\)\n1.033\n0.015\n2.23\n0.026\n1.004 ; 1.063\n\n\n\\(constante\\)\n0.445\n0.112\n-3.22\n0.001\n0.272 ; 0.728\n\n\n\nSi on avait omis les variables year et age du modèle:\n\n\n\nProbabilité de décéder après correction de la non proportionnalité pour la variable surgery"
  },
  {
    "objectID": "08-discret.html#footnotes",
    "href": "08-discret.html#footnotes",
    "title": "9  Modèle à durée discrète",
    "section": "",
    "text": "la distribution des probabilités sous cette loi n’est pas, contrairement aux lois normale ou logistique, symétrique. Dans ce qui suit, il ne serait pas conseillé de l’utiliser dans l’application avec l’ajustement sous forme d’indicatrice avec seulement 4 intervalles↩︎\nje remercie Emilie Counil et Nargès Gouhoubi pour m’avoir informé de leur existence↩︎\nCe n’est pas le cas ici, mais si on sélectionne une forme cubique, je conseille vivement de regarder les probabilités conditionnelle obtenues, en particulier en fin de périodes d’observation/exposition si peut d’individu reste soumis au risque. On peut rencontrer des problèmes d’overfitting avec des probabilités conditionnelles estimées trop proche de 1. Pour les personnes qui suivent la formation, c’est le cas avec les données des TP↩︎\nvoir la courte section sur les modèle paramétriques usuels↩︎"
  },
  {
    "objectID": "09-tvc.html#facteur-dynamique-traitée-de-manière-fixe",
    "href": "09-tvc.html#facteur-dynamique-traitée-de-manière-fixe",
    "title": "10  Variables dynamiques",
    "section": "10.1 Facteur dynamique traitée de manière fixe",
    "text": "10.1 Facteur dynamique traitée de manière fixe\nOn reprend l’exemple sur malformation cardiaque, en ajoutant la variable relative à la greffe. La question est donc de savoir si une transplantation du coeur réduitle risque journalier de décéder (ou augmente la durée de survie).\nOn a dans la base 2 variables: une variable binaire pour savoir si l’individu à été greffé ou non, transplant, et la variable wait de type continue tronquée donnant la durée en jour jusqu’à l’opération depuis l’inscription dans le registre (0 si \\(transplant=0\\)).\nOn va dans un premier temps estimer le modèle de Cox avec la variable fixe transplant.\n\nModèle de cox avec une variable dynamique (binaire) traitée de manière fixe (estimation biaisée\n\n\nVariables\nHR\nStd. err\nz\nP&gt;|z|\n95% CI\n\n\n\n\nyear\n0.910\n0.060\n-1.42\n0.155\n0.799 ; 1.036\n\n\nage\n1.054\n0.015\n3.71\n0.000\n1.025 ; 1.084\n\n\nsurgery\n0.541\n0.243\n-1.37\n0.171\n0.224 ; 1.304\n\n\ntransplant\n0.278\n0.088\n-4.06\n0.000\n0.150 ; 0.515\n\n\nwait\n0.992\n0.005\n-1.50\n0.134\n0.982 ; 1.002\n\n\n\nInterprétation: traitée de manière fixe, la greffe réduit donc sensiblement le risque journalier de décéder (RR=0.278). De même on peut admettre une certaine cohérence pour la durée jusqu’à la transplantation: plus elle est précoce et plus les personnes survivent (HR=0.992).\nSauf que…..\nAu niveau des données le modèle à été estimé, pour une personne greffée (ici id=70), à partir de ce mapping:\n\nMapping de la base avec une variable dynamique binaire traitée de manière fixe\n\n\nid\nyear\nage\nsurgery\ntransplant\nwait\ndied\n\\(t_0\\)\n\\(t\\)\n\n\n\n\n70\n72\n52\n0\n1\n5\n0\n0\n1\n\n\n70\n72\n52\n0\n1\n5\n0\n1\n2\n\n\n70\n72\n52\n0\n1\n5\n0\n2\n3\n\n\n70\n72\n52\n0\n1\n5\n0\n3\n5\n\n\n70\n72\n52\n0\n1\n5\n0\n5\n6\n\n\n70\n72\n52\n0\n1\n5\n0\n6\n8\n\n\n70\n72\n52\n0\n1\n5\n0\n8\n9\n\n\n70\n72\n52\n0\n1\n5\n0\n9\n12\n\n\n70\n72\n52\n0\n1\n5\n0\n12\n16\n\n\n70\n72\n52\n0\n1\n5\n0\n16\n17\n\n\n70\n72\n52\n0\n1\n5\n0\n17\n18\n\n\n70\n72\n52\n0\n1\n5\n0\n18\n21\n\n\n70\n72\n52\n0\n1\n5\n0\n21\n28\n\n\n70\n72\n52\n0\n1\n5\n1\n28\n30\n\n\n\nUne personne est codée greffée avant le jour de la transplantation. L’effet causal est donc mal mesuré si sa dimension temporelle a été ignorée, ici le jour exact de l’opération. C’est le même principe pour l’évènement, la personne est codée décédée (1) le jour du décès, et vivante avant (0)."
  },
  {
    "objectID": "09-tvc.html#estimation-avec-une-variable-dynamique",
    "href": "09-tvc.html#estimation-avec-une-variable-dynamique",
    "title": "10  Variables dynamiques",
    "section": "10.2 Estimation avec une variable dynamique",
    "text": "10.2 Estimation avec une variable dynamique\nIl convient donc de modifier l’information avec le délai d’attente jusqu’à la greffe. Le principe de construction de la variable dynamique, quelle que soit le logiciel utilisé, doit suivre la logique suivante:\n\\(tvc = transplant\\) , si \\(transplant=1\\) et \\(t&lt;wait\\) alors \\(tvc=0\\)\n\n10.2.1 Modèle de Cox\n\nMapping correct de la base avec une variable dynamique binaire\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nyear\nage\nsurgery\ntransplant\nwait\ndied\n\\(t_0\\)\n\\(t\\)\nTVC\n\n\n\n\n70\n72\n52\n0\n1\n5\n0\n0\n1\n0\n\n\n70\n72\n52\n0\n1\n5\n0\n1\n2\n0\n\n\n70\n72\n52\n0\n1\n5\n0\n2\n3\n0\n\n\n70\n72\n52\n0\n1\n5\n0\n3\n5\n0\n\n\n70\n72\n52\n0\n1\n5\n0\n5\n6\n1\n\n\n70\n72\n52\n0\n1\n5\n0\n6\n8\n1\n\n\n70\n72\n52\n0\n1\n5\n0\n8\n9\n1\n\n\n70\n72\n52\n0\n1\n5\n0\n9\n12\n1\n\n\n70\n72\n52\n0\n1\n5\n0\n12\n16\n1\n\n\n70\n72\n52\n0\n1\n5\n0\n16\n17\n1\n\n\n70\n72\n52\n0\n1\n5\n0\n17\n18\n1\n\n\n70\n72\n52\n0\n1\n5\n0\n18\n21\n1\n\n\n70\n72\n52\n0\n1\n5\n0\n21\n28\n1\n\n\n70\n72\n52\n0\n1\n5\n1\n28\n30\n1\n\n\n\nSi on estime maintenant le modèle avec cette variable dynamique qui indique clairement le moment de la transition (jour de la greffe):\n\nModèle de Cox avec une variable dynamique binaire\n\n\n\n\n\n\n\n\n\n\nVariables\nHR\nStd. err\nz\nP&gt;|z|\n95% CI\n\n\n\n\n\\(year\\)\n0.887\n0.060\n-1.79\n0.074\n0.777 ; 1.012\n\n\n\\(age\\)\n1.031\n0.014\n2.19\n0.029\n1.003 ; 1.059\n\n\n\\(surgery\\)\n0.374\n0.163\n-2.25\n0.024\n0.159 ; 0.880\n\n\n\\(TVC transplantation\\)\n0.921\n0.281\n-0.27\n0.787\n0.507 ; 1.674\n\n\n\nL’impact de la greffe apparaît maintenant bien plus modéré sur la survie des individus. Cela ne signifie pas non plus que des personnes ont pu être sauvée grâce à cette opération (ou plutôt leur durée de vie augmentée), mais des complications lors de l’opération ou post-opératoire, surtout à une époque où ces techniques étaient à leurs balbutiements, ont pu également accélérer la mortalité. Il faut également garder en tête que l’état de santé des personnes est particulièrement dégradé, cette opération étant celle de la dernière chance.\nR - Stata - Sas - Python\n\nSasR - Stata, Python\n\n\nLa base n’est pas modifiée et la création de la TVC est faite en aveugle dans la procédure phreg, après l’instruction model. Ce n’est franchement pas super.\n\n\nLa base doit être transformée en format long aux temps d’évènement (survsplit avec R, stsplit avec Stata) avant la création de la variable dynamique.\n\n\n\n\n\n10.2.2 Modèle à temps discret\nMême principe pour la construction de la variable dynamique. Pour rappel l’échelle temporelle est le mois, on a créé en amont une variable qui regroupe les valeurs de la variable wait en périodes de 30 jours.\n\nModèle logistique à durée discrète avec variable dynamique binaire\n\n\n\n\n\n\n\n\n\n\nVariables\nOR - RR\nStd. err.\nz\nP&gt;|z|\n95% IC\n\n\n\n\n\\(t\\)\n0.686\n0.070\n-3.71\n0.000\n0.562 ; 0.837\n\n\n\\(t^2\\)\n1.015\n0.006\n2.53\n0.011\n1.003 ; 1.026\n\n\n\\(t^3\\)\n1.000\n0.000\n-1.97\n0.049\n1.000 ; 1.000\n\n\n\\(year\\)\n0.876\n0.065\n-1.79\n0.073\n0.758 ; 1.012\n\n\n\\(age\\)\n1.034\n0.015\n2.22\n0.027\n1.004 ; 1.064\n\n\n\\(surgery\\)\n0.363\n0.163\n-2.25\n0.024\n0.151 ; 0.876\n\n\n\\(TVC \\; greffe\\)\n1.029\n0.355\n0.08\n0.934\n0.524 ; 2.022\n\n\n\n\n\n\n\n\n\n\n\\(Constante\\)\n0.440\n0.110\n-3.29\n0.001\n0.270 ; 0.718"
  },
  {
    "objectID": "09-tvc.html#précautions",
    "href": "09-tvc.html#précautions",
    "title": "10  Variables dynamiques",
    "section": "10.3 Précautions",
    "text": "10.3 Précautions\n\nRappel: la cause doit précèder l’effet.\nLorsque l’évènement étudié n’est pas intrinsèquement de type absorbant comme le décès, la cause peut se manifester ou plutôt être observée après la survenue de l’évènement étudié. Les modèles de durée standards ne peuvent pas gérer ces situations car l’observation sort du risque après la survenue de l’évènement. Il y a d’autres techniques, par exemple de type économétrique, qui sont plus à même de traiter ce genre de situations.\nMême si la cause est bien mesurée avant l’évènement d’intérêt, un choc n’est peut-être qu’un point final d’un processus causal antérieur: une séparation est rarement un évènement ponctuel, une phase plus ou moins longue de mésentente dans le couple lui a vraisemblablement préexister. La datation du début d’un processus causal n’est donc pas toujours facile à mesurer.\n\nLogique d’adaptation: la cause identifiée est mesurée avant l’évènement étudié.\nLogique d’anticipation: la cause identifiée est mesurée après l’occurrence de l’évènement étudié. L’origine causale est bien antérieure à l’évènement, mais elle n’est pas directement observable.\n\nLorsque les variables dynamiques sont de type quantitatives/continues, le problème on doit aussi considérer avec des phénomènes d’anticipation sur les valeurs attendues de ces variables, observées postérieurement à l’évènement étudié. On peut introduire des « lags » dans le modèle pour saisir ce phénomène : par exemple \\(x_t= x_{t+1}\\). Ce décalage des durées d’occurrence peut être aussi introduite pour les variables discrètes (naissance d’un enfant par exemple)."
  },
  {
    "objectID": "10-manipulation.html#calcul-des-variables-danalyses",
    "href": "10-manipulation.html#calcul-des-variables-danalyses",
    "title": "11  Eléments de mise en forme des données",
    "section": "11.1 Calcul des variables d’analyses",
    "text": "11.1 Calcul des variables d’analyses\nOn partira de la base individus-séquences suivante:\n\n\nCode\ndf = data.frame(id  =  c(1, 1, 1, 2),\n                deb =  c(2020, 2023, 2024, 2022),\n                fin =  c(2021, 2024, 2025, NA), \n                  x =  c(1,2,1,2))\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\nx\n\n\n\n\n1\n2020\n2021\n1\n\n\n1\n2023\n2024\n2\n\n\n1\n2024\n2025\n1\n\n\n2\n2022\nNA\n2\n\n\n\n\n\nOn supposera que l’année de collecte, pour toutes les observations, est 2025 2.\nSi cela n’est pas donné dans le module biographique, il peut être intéressant de construire les numéros de séquences des trajectoires.\n\n\nCode\ndf$nseq = 1 \ndf = df %&gt;% group_by(id) %&gt;% mutate(nseq = cumsum(nseq))  \n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\n\n\n\n\n1\n2020\n2021\n1\n1\n\n\n1\n2023\n2024\n2\n2\n\n\n1\n2024\n2025\n1\n3\n\n\n2\n2022\nNA\n2\n1\n\n\n\n\n\nExemple 1 : durée de séjour de la première séquence observée\nSupposons que x traduit un type de relation/union, par exemple x=1 est une relation non cohabitante et x=2 est une relation cohabitante. On s’intéresse à la durée de la première relation, sans distinction entre 1 et 2. Il suffit de séléctionner la première séquence.\n\n\nCode\ndf = filter(df, nseq==1)\n\n\nLa variable de fin va permettre de repérer les informations censurées, et de générer la variable d’évènement. A ce niveau il est donc important de ne pas encore remplacer la date de censure par sa valeur.\n\nSi fin est une valeur manquante: observation censurée.\nSi fin est une valeur renseignée: occurence de l’évènement.\n\n\n\nCode\ndf$e = ifelse(is.na(df$fin), 0,1)\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\ne\n\n\n\n\n1\n2020\n2021\n1\n1\n1\n\n\n2\n2022\nNA\n2\n1\n0\n\n\n\n\n\nPour la variable de durée 3, une repéré les observations censurées, elle est calculée directement avec les variables fin et deb.\n\n\nCode\ndf$dur = ifelse(df$e==1, df$fin - df$deb + 1, 2025 - df$deb + 1)\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\ne\ndur\n\n\n\n\n1\n2020\n2021\n1\n1\n1\n2\n\n\n2\n2022\nNA\n2\n1\n0\n4\n\n\n\n\n\nExemple 2 : changement de métrique temporelle\nToujours avec le même exemple, mais en ajoutant une observation, supposons que l’on dispose également de l’information sur les mois. Sur les mois où l’évènement à eu lieu, mais également sur les mois où l’enquête a été réalisée.\n\n\nCode\ndf2 = data.frame(id  = c(1, 1, 1, 2,3),\n                deb  = c(2020, 2023, 2024, 2022, 2021),\n                debm = c(2,5,3,10,9),\n                fin  = c(2021, 2024, 2025, NA,2021), \n                finm = c(4,2,12,NA,11), \n                x    = c(1,2,1,2,1),\n                enq  = c(2025,2025,2025,2025,2025),\n                enqm = c(4,4,4,5,4))\n\ndf2$nseq = 1 \ndf2 = df2 %&gt;% group_by(id) %&gt;% mutate(nseq = cumsum(nseq))  \n\nkable(df2)\n\n\n\n\n\nid\ndeb\ndebm\nfin\nfinm\nx\nenq\nenqm\nnseq\n\n\n\n\n1\n2020\n2\n2021\n4\n1\n2025\n4\n1\n\n\n1\n2023\n5\n2024\n2\n2\n2025\n4\n2\n\n\n1\n2024\n3\n2025\n12\n1\n2025\n4\n3\n\n\n2\n2022\n10\nNA\nNA\n2\n2025\n5\n1\n\n\n3\n2021\n9\n2021\n11\n1\n2025\n4\n1\n\n\n\n\n\nOn remarque que la nouvelle observation (id=3) a connu l’évènement, ici la fin de la relation, la même année qu’au début d’exposition (le début de la relation)…. mais au bout de 2,6,11 mois???? Commeon dispose de l’information sur les mois de début et de fin cela peut être intéressant de l’exploite. De la même manière si l’enquête a été réalisée la même année, les entretiens n’ont pas eu lieu le même mois. On aura besoin de cette information pour les observations censurées.\nDe nouveau on sélectionne la première séquence, et pour la lisibilité de la base on retire les informations qui ne seront pas ou plus exploitées (nseq, x).\n\n\nCode\ndf2 = filter(df2,nseq==1)\ndf2 = select(df2, -c(x,nseq))\n\nkable(df2)\n\n\n\n\n\nid\ndeb\ndebm\nfin\nfinm\nenq\nenqm\n\n\n\n\n1\n2020\n2\n2021\n4\n2025\n4\n\n\n2\n2022\n10\nNA\nNA\n2025\n5\n\n\n3\n2021\n9\n2021\n11\n2025\n4\n\n\n\n\n\nOn génère la variable censure/évènement (toujours à faire avant la variable de durée) de la même manière que pour l’exemple 1.\n\n\nCode\ndf2$e = ifelse(is.na(df2$fin), 0, 1)\n\nkable(df2)\n\n\n\n\n\nid\ndeb\ndebm\nfin\nfinm\nenq\nenqm\ne\n\n\n\n\n1\n2020\n2\n2021\n4\n2025\n4\n1\n\n\n2\n2022\n10\nNA\nNA\n2025\n5\n0\n\n\n3\n2021\n9\n2021\n11\n2025\n4\n1\n\n\n\n\n\nPour la variable de durée, le principe est de multiplié par 12 la différence entre l’année de fin et l’année de début et d’ajouter la différence entre le mois de fin et le mois de début.\nPour les observations censurées, ici l’année de fin est identique mais les mois varient. En terme de programmation, surtout si avec R on utilise ifelse, il est préférable d’y aller doucement en créant une durée pour les observations qui ont connu l’évènement et une durée pour les observations censurées. Puis de regrouper les deux cas. C’est ce qui est fait dans le code qui suit.\nDurée selon les valeurs de e:\n\n\nCode\ndf2$dur1 = ifelse(df2$e==1, 12*(df2$fin - df2$deb) + (df2$finm - df2$debm),  0) \ndf2$dur0 = ifelse(df2$e==0, 12*(2025 - df2$deb)    + (df2$enqm  - df2$debm), 0) \n\nkable(df2)\n\n\n\n\n\nid\ndeb\ndebm\nfin\nfinm\nenq\nenqm\ne\ndur1\ndur0\n\n\n\n\n1\n2020\n2\n2021\n4\n2025\n4\n1\n14\n0\n\n\n2\n2022\n10\nNA\nNA\n2025\n5\n0\n0\n31\n\n\n3\n2021\n9\n2021\n11\n2025\n4\n1\n2\n0\n\n\n\n\n\nOn regroupe par simple sommation (le else étant 0).\n\n\nCode\ndf2$dur  = df2$dur1 + df2$dur0\n\ndf2 = select(df2, -c(dur1,dur0))\n\nkable(df2)\n\n\n\n\n\nid\ndeb\ndebm\nfin\nfinm\nenq\nenqm\ne\ndur\n\n\n\n\n1\n2020\n2\n2021\n4\n2025\n4\n1\n14\n\n\n2\n2022\n10\nNA\nNA\n2025\n5\n0\n31\n\n\n3\n2021\n9\n2021\n11\n2025\n4\n1\n2\n\n\n\n\n\nOn dispose ainsi des éléments nécessaire pour faire une analyse de durée avec une métrique mensuelle 4.\nExemple 3 : importation d’un début d’expositon externe\nOn repart de la première base\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\n\n\n\n\n1\n2020\n2021\n1\n1\n\n\n1\n2023\n2024\n2\n2\n\n\n1\n2024\n2025\n1\n3\n\n\n2\n2022\nNA\n2\n1\n\n\n\n\n\nOn suppose maintenant que x traduit des situations sur le marché du travail. Par exemple x=1 est un emploi en CDD et x=2 un emploi en CDI. On s’intéresse à la durée entre la fin des études et le premier emploi, quel que soit sont type.\n\nOn ne dispose pas ici de toutes l’information pour calculer la durée, soit la fin des études. Elle peut être donnée dans une base classique regroupant l’ensemble des caractéristiques individuelles de type fixe (année de naissance, sexe…).\nComme on s’intéresse à la durée de recherche du premier emploi, dans le module biographique la date de début va devenir la date de fin.\nPour les observations présente dans la base biographique, il n’y a pas de censure à droite. Mais si on regarde le fichier des caractéristiques générales, fixe:\n\n\n\nCode\netude = data.frame(id = c(1,2,3), fin_etude = c(2020,2021,2023))\nkable(etude)\n\n\n\n\n\nid\nfin_etude\n\n\n\n\n1\n2020\n\n\n2\n2021\n\n\n3\n2023\n\n\n\n\n\nUne nouvelle observation (id=3) apparaît. Au moment de l’enquête, elle n’a pas (encore) trouvé un emploi depuis la fin de ces études. On a donc une observation qui sera censurée.\n\n\n\n\n\n\nNote\n\n\n\nCertaines bases biographiques peuvent être structurées avec des trajectoires strictement continue, l’année (l’âge) de fin étant l’année (l’âge) de début de la trajectoire suivante. Dans ce cas, l’information serait immédiatement disponible, avec la présence d’un nombre de séquences plus important dans la base.\n\n\nOn va devoir:\n\nSélectionner la première sequence d’emploi dans la base df (variable nseq).\nLa fusionner avec la base étude.\n\nAvant la fusion, on peut conserver seulement les informations nécessaires (id, deb). La variable deb va changer également de statut en devenant l’année de fin de la période de recherche d’emploi.\n\n\nCode\ndf = filter(df, nseq==1)\ndf = select(df, -c(fin,x,nseq))\n\ndf = rename(df, fin = deb)\nkable(df)\n\n\n\n\n\nid\nfin\n\n\n\n\n1\n2020\n\n\n2\n2022\n\n\n\n\n\nAprès la fusion:\n\n\nCode\ndf = full_join(etude, df,  by = c('id'))\n\ndf = rename(df, deb = fin_etude)\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\n\n\n\n\n1\n2020\n2020\n\n\n2\n2021\n2022\n\n\n3\n2023\nNA\n\n\n\n\n\nOn a toutes les informations pour générer la variable censure/évènement et la variable de durée:\n\n\nCode\ndf$e = ifelse(is.na(df$fin),0,1)\n\ndf$dur = ifelse(df$e, df$fin - df$deb + 1, 2025 - df$deb + 1)\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ne\ndur\n\n\n\n\n1\n2020\n2020\n1\n1\n\n\n2\n2021\n2022\n1\n2\n\n\n3\n2023\nNA\n0\n3"
  },
  {
    "objectID": "10-manipulation.html#appariement-de-modules-biographiques",
    "href": "10-manipulation.html#appariement-de-modules-biographiques",
    "title": "11  Eléments de mise en forme des données",
    "section": "11.2 Appariement de modules biographiques",
    "text": "11.2 Appariement de modules biographiques\nOn repart de la première base, avec les numéros de séquence.\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\n\n\n\n\n1\n2020\n2021\n1\n1\n\n\n1\n2023\n2024\n2\n2\n\n\n1\n2024\n2025\n1\n3\n\n\n2\n2022\nNA\n2\n1\n\n\n\n\n\n\n11.2.1 Mise en forme d’une base\nPour apparier des informations de plusieurs modules biographiques, on doit transformer les bases en format individus-séquences en format individus-périodes (ici individus années).\n\nEtape 1: allongement sur chaque séquence après avoir générées leur durée\nEtape 2: générer une variable de période (année) sur chaque ligne. Elle servira pour l’appariement.\n\nPourquoi ne pas utiliser la simple différence entre la fin et le début ?\nDurée (fin - début) et allongement de la base:\nOn ne génère pas des variables d’analyse, on aurait besoin de l’information sur l’année de l’enquête pour les informations censurées.\n\n\nCode\ndf$fin[is.na(df$fin)] = 2025\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\n\n\n\n\n1\n2020\n2021\n1\n1\n\n\n1\n2023\n2024\n2\n2\n\n\n1\n2024\n2025\n1\n3\n\n\n2\n2022\n2025\n2\n1\n\n\n\n\n\nAllongement de la base:\n\n\nCode\ndf1 = df\ndf1$dur1 = df1$fin - df1$deb\n\ndf1$dur1b = df1$dur1 # uncount supprime la variable d'origine \ndf1 = uncount(df1,dur1b)\n\nkable(df1)\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\ndur1\n\n\n\n\n1\n2020\n2021\n1\n1\n1\n\n\n1\n2023\n2024\n2\n2\n1\n\n\n1\n2024\n2025\n1\n3\n1\n\n\n2\n2022\n2025\n2\n1\n3\n\n\n2\n2022\n2025\n2\n1\n3\n\n\n2\n2022\n2025\n2\n1\n3\n\n\n\n\n\nPour générer la variable période (année), on a besoin d’un compteur qui sera associé à la variable deb. On doit bien contrôler l’opération par identifiant et numéro de séquence.\n\n\nCode\ndf1$c = 1\ndf1 = df1 %&gt;% group_by(id,nseq) %&gt;% mutate(year = deb  + cumsum(c)) \n\nkable(df1)\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\ndur1\nc\nyear\n\n\n\n\n1\n2020\n2021\n1\n1\n1\n1\n2021\n\n\n1\n2023\n2024\n2\n2\n1\n1\n2024\n\n\n1\n2024\n2025\n1\n3\n1\n1\n2025\n\n\n2\n2022\n2025\n2\n1\n3\n1\n2023\n\n\n2\n2022\n2025\n2\n1\n3\n1\n2024\n\n\n2\n2022\n2025\n2\n1\n3\n1\n2025\n\n\n\n\n\nProblème: les années de début ne sont pas correncte: 2021 au lieu de 2020 pour la première séquence de id=1 par exemple.\n\n\n\n\n\n\nImportant\n\n\n\nOn doit donc impérativement augmenter la différence entre la fin et le début par +1 pour que l’ensemble des périodes (années) soit couvertes.\n\n\nOn reprend donc les opérations précédentes mais avec durée = fin - debut + 1\n\nAllongement de la base avec durée augmentée\n\n\n\nCode\ndf2 = df\ndf2$dur2 = df2$fin - df2$deb + 1\n\ndf2$dur2b = df2$dur2 # uncount supprime la variable d'origine \ndf2 = uncount(df2,dur2b)\n\nkable(df2)\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\ndur2\n\n\n\n\n1\n2020\n2021\n1\n1\n2\n\n\n1\n2020\n2021\n1\n1\n2\n\n\n1\n2023\n2024\n2\n2\n2\n\n\n1\n2023\n2024\n2\n2\n2\n\n\n1\n2024\n2025\n1\n3\n2\n\n\n1\n2024\n2025\n1\n3\n2\n\n\n2\n2022\n2025\n2\n1\n4\n\n\n2\n2022\n2025\n2\n1\n4\n\n\n2\n2022\n2025\n2\n1\n4\n\n\n2\n2022\n2025\n2\n1\n4\n\n\n\n\n\n\nCréation de la variable year: sur chaque individus-séquences, la somme entre le compteur et l’année de début doit être réduite de 11.\n\n\n\nCode\ndf2$c = 1\ndf2 = df2 %&gt;% group_by(id,nseq) %&gt;% mutate(year = deb  + cumsum(c) - 1)\n\ndf2 = select(df2, -c(deb,fin,dur2))\n\n\nkable(df2)\n\n\n\n\n\nid\nx\nnseq\nc\nyear\n\n\n\n\n1\n1\n1\n1\n2020\n\n\n1\n1\n1\n1\n2021\n\n\n1\n2\n2\n1\n2023\n\n\n1\n2\n2\n1\n2024\n\n\n1\n1\n3\n1\n2024\n\n\n1\n1\n3\n1\n2025\n\n\n2\n2\n1\n1\n2022\n\n\n2\n2\n1\n1\n2023\n\n\n2\n2\n1\n1\n2024\n\n\n2\n2\n1\n1\n2025\n\n\n\n\n\nLes années sont toutes couvertes….mais un peu trop. En effet, lorsque les trajectoires sont continues soit lorsque l’année de fin d’une séquence est identique à l’année de début de la suivante, les années vont être doublonnées. On doit dont supprimer ce doublon.\n\nSuppression des doublons des trajectoires continues.\n\nDe nouveaux on doit faire un choix, soit on priviligie l’année de fin, soit on privilégie l’année de début. Les applications ont des fonctions qui permettent de supprimer les doublons5. On peut le faire manuellement en regardant pour chaque personnes-années le nombre de doublon. Cela se fait facilement à l’aide d’un compteur, ici la variable nyear.\n\n\nCode\ndf2 = df2 %&gt;% group_by(id,year) %&gt;% mutate(nyear = cumsum(c))\n\nkable(df2)\n\n\n\n\n\nid\nx\nnseq\nc\nyear\nnyear\n\n\n\n\n1\n1\n1\n1\n2020\n1\n\n\n1\n1\n1\n1\n2021\n1\n\n\n1\n2\n2\n1\n2023\n1\n\n\n1\n2\n2\n1\n2024\n1\n\n\n1\n1\n3\n1\n2024\n2\n\n\n1\n1\n3\n1\n2025\n1\n\n\n2\n2\n1\n1\n2022\n1\n\n\n2\n2\n1\n1\n2023\n1\n\n\n2\n2\n1\n1\n2024\n1\n\n\n2\n2\n1\n1\n2025\n1\n\n\n\n\n\nSi on souhaite garder l’année de fin on filtre les observations en conservant celles dont nyear=1. Si on souhaite privilégier les années de début on foltre les observations en conservant celles dont nyear=2. Si on souhaite conserver les années de fin de séquence:\n\n\nCode\ndf2 = filter(df2, nyear==1)\n\ndf2 = select(df2, -c(nseq,c,nyear))\n\nkable(df2)\n\n\n\n\n\nid\nx\nyear\n\n\n\n\n1\n1\n2020\n\n\n1\n1\n2021\n\n\n1\n2\n2023\n\n\n1\n2\n2024\n\n\n1\n1\n2025\n\n\n2\n2\n2022\n\n\n2\n2\n2023\n\n\n2\n2\n2024\n\n\n2\n2\n2025\n\n\n\n\n\n\n\n\n\n\n\nEn résumé\n\n\n\n\nA la date (année/âge) de censure remplacer la valeur manquante par sa valeur. Si ultérieurement on a besoin de garder l’information sur la censure - valeur manquante - , on peut générer une variable mirroir de fin.\nSur chaque séquence calculer la durée avec une augmentation de +1.\nCréer une variable période (année) sur chaque ligne. Elle servira à définir la clé d’appariement.\nSupprimer les doublons sur les transition continue \\(fin_t = debut_{t+1}\\).\n\n\n\n\n\n11.2.2 Fusion des informations biographiques\n\n11.2.2.1 Fusion avec l’ensemble des périodes observables\nPour commencer par un exemple plutôt simple, on note que pour id=1 l’année 2022 n’est pas renseignée (trajectoire non continue). Si on reprend un exemple précédent (relations de couple), cette année pourrait être identifiée comme une période sans relation. Une façon simple de boucher ce type “trous”, est d’utiliser les années de naissances des individus, et de créer une base individus-périodes qui couvre toutes les années de vie de l’individu jusqu’à l’enquête. On remontera jusque là, mais on va par exemple considérer que pour id=1 et id=2 ce début de tout est en 2018.\n\n\nCode\ndftout = data.frame(id  =  c(1, 2),\n                    t0  =  c(2018, 2018))\n\nkable(dftout)    \n\n\n\n\n\nid\nt0\n\n\n\n\n1\n2018\n\n\n2\n2018\n\n\n\n\n\n\nOn ajoute l’information sur l’année de l’enquête (2025).\nOn génère la durée\nOn allonge la base\nOn génère la variable année sur chaque ligne (on contrôle seulement sur id)\n\n\n\nCode\ndftout$tmax = 2025\n\ndftout$dur  = dftout$tmax - dftout$t0 + 1\n\ndftout = uncount(dftout,dur)\n\n\ndftout$c = 1\ndftout = dftout %&gt;% group_by(id) %&gt;% mutate(year = t0  + cumsum(c) - 1)\n\ndftout = select(dftout, -c(t0,tmax,c))\n\nkable(dftout)    \n\n\n\n\n\nid\nyear\n\n\n\n\n1\n2018\n\n\n1\n2019\n\n\n1\n2020\n\n\n1\n2021\n\n\n1\n2022\n\n\n1\n2023\n\n\n1\n2024\n\n\n1\n2025\n\n\n2\n2018\n\n\n2\n2019\n\n\n2\n2020\n\n\n2\n2021\n\n\n2\n2022\n\n\n2\n2023\n\n\n2\n2024\n\n\n2\n2025\n\n\n\n\n\nOn peut maintenant apparier cette couverture de toutes les années de vie jusqu’à l’enquête à la base biographique:\n\n\nCode\ndf2 = full_join(df2, dftout, by = c(\"id\",\"year\"))\n\ndf2 = arrange(df2, id, year)\nkable(df2)    \n\n\n\n\n\nid\nx\nyear\n\n\n\n\n1\nNA\n2018\n\n\n1\nNA\n2019\n\n\n1\n1\n2020\n\n\n1\n1\n2021\n\n\n1\nNA\n2022\n\n\n1\n2\n2023\n\n\n1\n2\n2024\n\n\n1\n1\n2025\n\n\n2\nNA\n2018\n\n\n2\nNA\n2019\n\n\n2\nNA\n2020\n\n\n2\nNA\n2021\n\n\n2\n2\n2022\n\n\n2\n2\n2023\n\n\n2\n2\n2024\n\n\n2\n2\n2025\n\n\n\n\n\nPour supprimer les informations qui précèdent la première séquence de la biographie, on peut générer un compteur sur la variable x après avoir remplacer ses valeurs manquantes par des 0. On gardera les lignes pour lesquels ce compteur est supérieur à 1.\n\n\nCode\ndf2$x[is.na(df2$x)] = 0\n\ndf2 = df2 %&gt;% group_by(id) %&gt;% mutate(nx = cumsum(x))\n\nkable(df2)    \n\n\n\n\n\nid\nx\nyear\nnx\n\n\n\n\n1\n0\n2018\n0\n\n\n1\n0\n2019\n0\n\n\n1\n1\n2020\n1\n\n\n1\n1\n2021\n2\n\n\n1\n0\n2022\n2\n\n\n1\n2\n2023\n4\n\n\n1\n2\n2024\n6\n\n\n1\n1\n2025\n7\n\n\n2\n0\n2018\n0\n\n\n2\n0\n2019\n0\n\n\n2\n0\n2020\n0\n\n\n2\n0\n2021\n0\n\n\n2\n2\n2022\n2\n\n\n2\n2\n2023\n4\n\n\n2\n2\n2024\n6\n\n\n2\n2\n2025\n8\n\n\n\n\n\nOn supprime les lignes lorsque nx=0.\n\n\nCode\ndf2 = filter(df2, nx&gt;0)\n\ndf2 = select(df2, -c(nx))\n\nkable(df2)    \n\n\n\n\n\nid\nx\nyear\n\n\n\n\n1\n1\n2020\n\n\n1\n1\n2021\n\n\n1\n0\n2022\n\n\n1\n2\n2023\n\n\n1\n2\n2024\n\n\n1\n1\n2025\n\n\n2\n2\n2022\n\n\n2\n2\n2023\n\n\n2\n2\n2024\n\n\n2\n2\n2025\n\n\n\n\n\n\n\n11.2.2.2 Fusion avec une autre base biographique\nOn peut être amené à fusionner plusieurs modules biographique. Jusqu’à présent, une même année, tous les individus ne pouvaient être que dans une situation, par exemple une seul emploi, un seul lieu de résidence etc… Pour certains phénomènes, une même années ou pendant une période plus longue on peut observer simultanément plusieurs états différent, ou plus classiquement observer une somme d’un même état. On parle ici d’overlapping. Ce type de situation est typiquement celle qu’on observe avec le nombre d’enfants.\nSupposons que le base ci-dessous traduit la naissance et potentiellement le décès des enfants.\n\n\nCode\ndfy = data.frame(id  =  c(1, 2, 2),\n                deb =  c(2022, 2019, 2023),\n                fin =  c(NA, 2024,NA), \n                nseq =  c(1,1,2))\n\nkable(dfy)\n\n\n\n\n\nid\ndeb\nfin\nnseq\n\n\n\n\n1\n2022\nNA\n1\n\n\n2\n2019\n2024\n1\n\n\n2\n2023\nNA\n2\n\n\n\n\n\n\nid=1 a un premier enfant en 2022 qui est toujours en vie au moment de l’enquête (2025)\nid=2:\n\nA un premier enfant en 2019 qui décède en 2024\nA un second enfant en 2023, toujours en vie au moment de l’enquête\nDe la naissance du second enfant au décès du premier, on va donc avoir des doublons (overlapping) sur les années\n\n\nSi on reprend les manipulations précédentes jusqu’à la création de la variable year:\n\n\nCode\ndfy$fin[is.na(dfy$fin)] = 2025\ndfy$dur = dfy$fin - dfy$deb + 1\n\ndfy$durb = dfy$dur  # Uncount supprime la variable d'origine \ndfy = uncount(dfy,durb)\n\ndfy$c = 1\ndfy = dfy %&gt;% group_by(id,nseq) %&gt;% mutate(year = deb  + cumsum(c) - 1)\n\n\nLa variable year est bien renseignée 2 fois pour les années 2023 et 2024.\nOn peut s’intéresser au fait d’avoir ou non un enfant, ou de manière plus générale au nombre d’enfant. En créant cette information, on se donne également le moyen de corriger cet overlapping:\n\nOn peut de nouveau générer un compteur contrôlé par individu année\nEn génerant un total de ligne doublonnée, on récupèrera par exemple ici le nombre d’enfant en vie chaque année.\nEn ne gardant que la ligne ou le compteur est égal à 1, on supprime les doublons tout en gardant l’information sur le nombre d’enfant en vie une année donnée.\n\n\n\nCode\ndfy = dfy %&gt;% group_by(id,year) %&gt;% mutate(ny = cumsum(c))\ndfy = dfy %&gt;% group_by(id,year) %&gt;% mutate(tot_y =  sum(c))\n\nkable(dfy)\n\n\n\n\n\nid\ndeb\nfin\nnseq\ndur\nc\nyear\nny\ntot_y\n\n\n\n\n1\n2022\n2025\n1\n4\n1\n2022\n1\n1\n\n\n1\n2022\n2025\n1\n4\n1\n2023\n1\n1\n\n\n1\n2022\n2025\n1\n4\n1\n2024\n1\n1\n\n\n1\n2022\n2025\n1\n4\n1\n2025\n1\n1\n\n\n2\n2019\n2024\n1\n6\n1\n2019\n1\n1\n\n\n2\n2019\n2024\n1\n6\n1\n2020\n1\n1\n\n\n2\n2019\n2024\n1\n6\n1\n2021\n1\n1\n\n\n2\n2019\n2024\n1\n6\n1\n2022\n1\n1\n\n\n2\n2019\n2024\n1\n6\n1\n2023\n1\n2\n\n\n2\n2019\n2024\n1\n6\n1\n2024\n1\n2\n\n\n2\n2023\n2025\n2\n3\n1\n2023\n2\n2\n\n\n2\n2023\n2025\n2\n3\n1\n2024\n2\n2\n\n\n2\n2023\n2025\n2\n3\n1\n2025\n1\n1\n\n\n\n\n\nIl ne reste plus qu’à supprimer les lignes où ny&gt;1\n\n\nCode\ndfy = filter(dfy, ny==1)\ndfy = select(dfy, -c(ny,deb, fin, dur, nseq, c))\n\nkable(dfy)\n\n\n\n\n\nid\nyear\ntot_y\n\n\n\n\n1\n2022\n1\n\n\n1\n2023\n1\n\n\n1\n2024\n1\n\n\n1\n2025\n1\n\n\n2\n2019\n1\n\n\n2\n2020\n1\n\n\n2\n2021\n1\n\n\n2\n2022\n1\n\n\n2\n2023\n2\n\n\n2\n2024\n2\n\n\n2\n2025\n1\n\n\n\n\n\nAvec une ligne par année, on peut la fusionner avec une autre base biographique en format individus-années (même principe qu’avec la fusion avec la base sur toutes les années de vie).\n\n\nCode\ndf2y = full_join(dfy, df2, by = c(\"id\",\"year\"))\n\ndf2y = arrange(df2y, id,year)\n\ndf2y = select(df2y, c(id,year,x,tot_y))\n\ndf2y$tot_y[is.na(df2y$tot_y)] = 0\ndf2y$x[is.na(df2y$x)]   = 0\n\nkable(df2y)\n\n\n\n\n\nid\nyear\nx\ntot_y\n\n\n\n\n1\n2020\n1\n0\n\n\n1\n2021\n1\n0\n\n\n1\n2022\n0\n1\n\n\n1\n2023\n2\n1\n\n\n1\n2024\n2\n1\n\n\n1\n2025\n1\n1\n\n\n2\n2019\n0\n1\n\n\n2\n2020\n0\n1\n\n\n2\n2021\n0\n1\n\n\n2\n2022\n2\n1\n\n\n2\n2023\n2\n2\n\n\n2\n2024\n2\n2\n\n\n2\n2025\n2\n1"
  },
  {
    "objectID": "10-manipulation.html#sélection-dun-type-de-séquence-et-mise-en-forme-pour-lanalyse",
    "href": "10-manipulation.html#sélection-dun-type-de-séquence-et-mise-en-forme-pour-lanalyse",
    "title": "11  Eléments de mise en forme des données",
    "section": "11.3 Sélection d’un type de séquence et mise en forme pour l’analyse",
    "text": "11.3 Sélection d’un type de séquence et mise en forme pour l’analyse"
  },
  {
    "objectID": "10-manipulation.html#durée-jusquà-la-première-séquence",
    "href": "10-manipulation.html#durée-jusquà-la-première-séquence",
    "title": "11  Eléments de mise en forme des données",
    "section": "11.4 Durée jusqu’à la première séquence",
    "text": "11.4 Durée jusqu’à la première séquence\n\n\nCode\ndf =  data.frame(id  =  c( 1, 1, 1, 2, 3, 3, 4),\n                 deb =  c(2018, 2022, 2024, 2019, 2023, 2024, 2023),\n                 fin =  c(2021, 2024, 2025, NA, 2024, NA, NA), \n                 y  =   c(1, 2, 1, 2, 3, 2, 1),\n                 nseq = c(1, 2, 3, 1, 1, 2, 1)\n                 )\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\n\n\n\n\n1\n2018\n2021\n1\n1\n\n\n1\n2022\n2024\n2\n2\n\n\n1\n2024\n2025\n1\n3\n\n\n2\n2019\nNA\n2\n1\n\n\n3\n2023\n2024\n3\n1\n\n\n3\n2024\nNA\n2\n2\n\n\n4\n2023\nNA\n1\n1\n\n\n\n\n\nOn va s’intéresser à la durée jusqu’à l’occurence de la séquence de type 2 ou 3 (variable y). On considéra que le début de l’exposition est donné par la variable deb sur la première séquence.\n\nid=1: début de l’exposition/observation en 2018, observe l’évènement en 2022.\nid=2: début de l’exposition/observation en 2019, observe l’évènement la même année.\nid=3: début de l’exposition/observation en 2019, observe l’évènement la même année.\nid=4: début de l’exposition/observation en 2023, n’a pas connu l’évènement au moment de l’enquête.\n\nRecupération de l’année de l’évènement\nOn peut repérer la présence d’une des deux séquences d’intérêt avec une indicatrice.\n\n\nCode\ndf$e = ifelse(df$y==2 | df$y==3,1,0)\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne\n\n\n\n\n1\n2018\n2021\n1\n1\n0\n\n\n1\n2022\n2024\n2\n2\n1\n\n\n1\n2024\n2025\n1\n3\n0\n\n\n2\n2019\nNA\n2\n1\n1\n\n\n3\n2023\n2024\n3\n1\n1\n\n\n3\n2024\nNA\n2\n2\n1\n\n\n4\n2023\nNA\n1\n1\n0\n\n\n\n\n\nDe nouveau l’utilisation d’un compteur sur cette variable indicatrice, peut s’avérer utile pour repérer le moment de l’occurence.\n\n\nCode\ndf = df %&gt;% group_by(id) %&gt;% mutate(n  = cumsum(e)) \n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne\nn\n\n\n\n\n1\n2018\n2021\n1\n1\n0\n0\n\n\n1\n2022\n2024\n2\n2\n1\n1\n\n\n1\n2024\n2025\n1\n3\n0\n1\n\n\n2\n2019\nNA\n2\n1\n1\n1\n\n\n3\n2023\n2024\n3\n1\n1\n1\n\n\n3\n2024\nNA\n2\n2\n1\n2\n\n\n4\n2023\nNA\n1\n1\n0\n0\n\n\n\n\n\nPour id=(2,3,4), ce compteur permet d’obtenir l’information souhaitée, à savoir n=0 en situation d’attente/séjour/survie et n=1 l’année de l’évènement. Pour id=1 cependant, l’alternance en y=1 et y=(2,3) ne permet pas de récupérer l’année d’occurence (première fois en 2 ou 3). Cela peut être fait, en faisant un compteur sur le compteur précédent:\n\n\nCode\ndf = df %&gt;% group_by(id) %&gt;% mutate(nn  = cumsum(n)) \n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne\nn\nnn\n\n\n\n\n1\n2018\n2021\n1\n1\n0\n0\n0\n\n\n1\n2022\n2024\n2\n2\n1\n1\n1\n\n\n1\n2024\n2025\n1\n3\n0\n1\n2\n\n\n2\n2019\nNA\n2\n1\n1\n1\n1\n\n\n3\n2023\n2024\n3\n1\n1\n1\n1\n\n\n3\n2024\nNA\n2\n2\n1\n2\n3\n\n\n4\n2023\nNA\n1\n1\n0\n0\n0\n\n\n\n\n\nRécupération des information censurée\nPour récupérer l’information sur les observations qui seront censurée, on peut faire un total sur la variable n ou e: si n=0, l’individu n’aura pas connu l’évènement.\n\n\nCode\ndf = df %&gt;% group_by(id) %&gt;% mutate(N  = sum(n)) \n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne\nn\nnn\nN\n\n\n\n\n1\n2018\n2021\n1\n1\n0\n0\n0\n2\n\n\n1\n2022\n2024\n2\n2\n1\n1\n1\n2\n\n\n1\n2024\n2025\n1\n3\n0\n1\n2\n2\n\n\n2\n2019\nNA\n2\n1\n1\n1\n1\n1\n\n\n3\n2023\n2024\n3\n1\n1\n1\n1\n3\n\n\n3\n2024\nNA\n2\n2\n1\n2\n3\n3\n\n\n4\n2023\nNA\n1\n1\n0\n0\n0\n0\n\n\n\n\n\nPour id=4, N est bien égal à 0.\nRécupération du début de l’exposition\nLe début de l’exposition étant ici l’année de début de la première séquence. On peut facilement récupérer cette sur toute les lignes en la repérant (ici en générant une nouvelle variable avec la fonction ifelse), et en sommant sa valeur sur les autres lignes (=0).\n\n\nCode\n1df$ debexp = ifelse(df$nseq==1, df$deb, 0)\n                    \n2df = df %&gt;% group_by(id) %&gt;% mutate(debexp  = sum(debexp))\n\nkable(df)\n\n\n\n1\n\nLa variable debex est égale à deb si nseq=1, 0 sinon.\n\n2\n\nOn somme cette valeur sur chaque individu pour l’ajouter aux séquences suivantes.\n\n\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne\nn\nnn\nN\ndebexp\n\n\n\n\n1\n2018\n2021\n1\n1\n0\n0\n0\n2\n2018\n\n\n1\n2022\n2024\n2\n2\n1\n1\n1\n2\n2018\n\n\n1\n2024\n2025\n1\n3\n0\n1\n2\n2\n2018\n\n\n2\n2019\nNA\n2\n1\n1\n1\n1\n1\n2019\n\n\n3\n2023\n2024\n3\n1\n1\n1\n1\n3\n2023\n\n\n3\n2024\nNA\n2\n2\n1\n2\n3\n3\n2023\n\n\n4\n2023\nNA\n1\n1\n0\n0\n0\n0\n2023\n\n\n\n\n\nMise en forme finale de la base\nOn peut maintenant conserver les lignes qui nous intéresse à savoir celle où nn=1 (évènement) ou N=0 (censure).\n\n\nCode\ndf = filter(df, nn==1 | N==0)\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne\nn\nnn\nN\ndebexp\n\n\n\n\n1\n2022\n2024\n2\n2\n1\n1\n1\n2\n2018\n\n\n2\n2019\nNA\n2\n1\n1\n1\n1\n1\n2019\n\n\n3\n2023\n2024\n3\n1\n1\n1\n1\n3\n2023\n\n\n4\n2023\nNA\n1\n1\n0\n0\n0\n0\n2023\n\n\n\n\n\nOn dispose déjà de la variable d’évènement/censure (e ou n = (0,1), on finit donc par la variable de durée.\n\n\nCode\ndf$fin[is.na(df$fin)] = 2025\n\ndf$dur = ifelse(df$e==1, df$deb - df$debexp + 1, df$fin - df$debexp + 1)\n\ndf = select(df, c(id,e,dur))\n\nkable(df)\n\n\n\n\n\nid\ne\ndur\n\n\n\n\n1\n1\n5\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n4\n0\n3\n\n\n\n\n\nCes informations sont suffisantes pour estimer une fonction de séjour et on peut ajouter, si elles ne sont pas présentes, des covariables fixes issues du fichier des caractéristiques générales. Pour l’ajout de covariables dynamiques, leur ajout n’est pas forcément difficile pour une analyse en durée discrète 6. Pour les analyses type Cox, selon la nature de la variable dynamique, l’opération (quel que soit le logiciel utilisé) risque d’être plus ou moins compliquée."
  },
  {
    "objectID": "10-manipulation.html#durée-de-séjour-dans-la-séquence-dintérêt-et-variables-danalyse",
    "href": "10-manipulation.html#durée-de-séjour-dans-la-séquence-dintérêt-et-variables-danalyse",
    "title": "11  Eléments de mise en forme des données",
    "section": "11.5 Durée de séjour dans la séquence d’intérêt et variables d’analyse",
    "text": "11.5 Durée de séjour dans la séquence d’intérêt et variables d’analyse\nEn première ou deuxième analyse, on peut également voir s’intéresser à la durée de séjour dans l’état précédent. Par exemple, si l’analyse précédent consistait à regarder la durée de séjour dans le premier emploi, on pourrait regarder ensuite la durée jusqu’à sa reprise.\nCela va un peu (voir plus) se compliquer. On va repartir de la base de départ précédente en ajoutant une observation.\n\n\nCode\ndf =  data.frame(id  =  c( 1, 1, 1, 2, 3, 3, 4, 5, 5, 5 , 5),\n                 deb =  c(2018, 2022, 2024, 2019, 2023, 2024, 2023, 2019, 2021, 2023, 2024),\n                 fin =  c(2021, 2024, 2025, NA, 2024, NA, NA, 2021, 2023, 2024, NA), \n                 y  =   c(1, 2, 1, 2, 3, 2, 1, 1, 2, 1,3),\n                 nseq = c(1, 2, 3, 1, 1, 2, 1, 1, 2, 3, 4)\n)\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\n\n\n\n\n1\n2018\n2021\n1\n1\n\n\n1\n2022\n2024\n2\n2\n\n\n1\n2024\n2025\n1\n3\n\n\n2\n2019\nNA\n2\n1\n\n\n3\n2023\n2024\n3\n1\n\n\n3\n2024\nNA\n2\n2\n\n\n4\n2023\nNA\n1\n1\n\n\n5\n2019\n2021\n1\n1\n\n\n5\n2021\n2023\n2\n2\n\n\n5\n2023\n2024\n1\n3\n\n\n5\n2024\nNA\n3\n4\n\n\n\n\n\nFiltrage des observations hors champs\nOn peut déjà supprimer les observations hors champs, à savoir ici id=4 qui n’a pas connu l’évènement dont on analyse la durée.\n\n\nCode\n1df$e23 = ifelse(df$y==2 | df$y==3,1,0)\n\ndf = df %&gt;%  group_by(id) %&gt;% mutate(n23  = cumsum(e23)) \n2df = filter(df, n23!=0)\n\nkable(df)\n\n\n\n1\n\nNom de la variable e23 pour repérer la présence de l’évènement dont on analyse la durée.\n\n2\n\nCe compteur est suffisant car l’observation n’a qu’une ligne.\n\n\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne23\nn23\n\n\n\n\n1\n2022\n2024\n2\n2\n1\n1\n\n\n1\n2024\n2025\n1\n3\n0\n1\n\n\n2\n2019\nNA\n2\n1\n1\n1\n\n\n3\n2023\n2024\n3\n1\n1\n1\n\n\n3\n2024\nNA\n2\n2\n1\n2\n\n\n5\n2021\n2023\n2\n2\n1\n1\n\n\n5\n2023\n2024\n1\n3\n0\n1\n\n\n5\n2024\nNA\n3\n4\n1\n2\n\n\n\n\n\nRécupération de l’évènement analysé\nIci l’évènement sera un retour dans l’état y=1. Il y a de nouveau une possibilité de censure à droite si une observation reste dans l’état 2 ou 3 jusqu’au moment de l’enquête.\nIl peut être utile d’utiliser des variables décalées pour repérer les changements d’état d’une séquence à une autre. Ces décalages sont appelées lead ou lag:\n\nlead: \\(x_t = x_{t+1}\\)\nlag: \\(x_t = x_{t-1}\\)\n\nOn va utilise ici des lead et donc pouvoir repérer les changements d’état d’une séquence à une autre. Comme on s’intéresse au retour à l’état 1:\n\n\nCode\n1df$e = ifelse(df$y==1,1,0)\n\n2df = df %&gt;%  group_by(id) %&gt;% mutate(diff_e  = e - lead(e))\n\nkable(df)\n\n\n\n1\n\ne est une indicatrice qui repère l’état 1\n\n2\n\nOn fait redescendre la valeur de e sur la séquence précédente, et on calcule la difference.\n\n\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne23\nn23\ne\ndiff_e\n\n\n\n\n1\n2022\n2024\n2\n2\n1\n1\n0\n-1\n\n\n1\n2024\n2025\n1\n3\n0\n1\n1\nNA\n\n\n2\n2019\nNA\n2\n1\n1\n1\n0\nNA\n\n\n3\n2023\n2024\n3\n1\n1\n1\n0\n0\n\n\n3\n2024\nNA\n2\n2\n1\n2\n0\nNA\n\n\n5\n2021\n2023\n2\n2\n1\n1\n0\n-1\n\n\n5\n2023\n2024\n1\n3\n0\n1\n1\n1\n\n\n5\n2024\nNA\n3\n4\n1\n2\n0\nNA\n\n\n\n\n\nPour chaque dernière séquence la valeur du lag est une valeur manquante. On repère l’évènement avec une valeur de -1 (transition de 0 à 1). On ne peut pas encore filtrer les informations car il va falloir récupérer la fin de la séquence, mais on peut déjà construire l’information.\n\n\nCode\ndf$e = ifelse(df$diff_e==-1,1,0)\ndf$e[is.na(df$e)] = 0\ndf = df %&gt;%  group_by(id) %&gt;% mutate(e  = sum(e)) \n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne23\nn23\ne\ndiff_e\n\n\n\n\n1\n2022\n2024\n2\n2\n1\n1\n1\n-1\n\n\n1\n2024\n2025\n1\n3\n0\n1\n1\nNA\n\n\n2\n2019\nNA\n2\n1\n1\n1\n0\nNA\n\n\n3\n2023\n2024\n3\n1\n1\n1\n0\n0\n\n\n3\n2024\nNA\n2\n2\n1\n2\n0\nNA\n\n\n5\n2021\n2023\n2\n2\n1\n1\n1\n-1\n\n\n5\n2023\n2024\n1\n3\n0\n1\n1\n1\n\n\n5\n2024\nNA\n3\n4\n1\n2\n1\nNA\n\n\n\n\n\nRécupération de l’année final avec succesion d’états de même type\nLa difficulté ici est apportée seulement par id=3. Jusqu’à 2025, on a successivement l’état 2 puis 3. Il va donc falloir récupérer cette dernière année de succession de 2 et 3, jusqu’à la censure ou jusqu’à un retour dans l’état 1. S’il n’y avait pas ce genre de situation, l’utilisation de la variable diff_e aurait été suffisante pour récupérer l’année de fin lorsqu’on a plusieurs séquences (situations pour id=1,5).\nOn va de nouveau utiliser un lead, mais sur la variable e23.\n\n\nCode\n1df = select(df, -c(nseq, diff_e))\n\n2df = df %&gt;%  group_by(id) %&gt;% mutate(lead_e23 = lead(e23, n = 1, default = NA))\n\n3df$idem = ifelse(df$e23 == df$lead_e23, 1, 0)\ndf$idem[is.na(df$idem)]=0\n4df = df %&gt;%  group_by(id) %&gt;% mutate(idem  = sum(idem))\n\n\nkable(df)\n\n\n\n1\n\nOn supprime les colonnes non utilisées pour gagner ici de la lisibilité\n\n2\n\nlead sur la variable e23.\n\n3\n\nLa variable idem permet de repérer une suite d’état 2 et 3. On ne passe pas ici par une variable de différence (le faire par prudence si on le souhaite).\n\n4\n\nIci le total est égal à 1. Si on avait eu une séquence supplémentaire de 3, il serait égal à 2. L’important ici est de repérer la situation, soit 0 ou supérieur à 0.\n\n\n\n\n\n\n\nid\ndeb\nfin\ny\ne23\nn23\ne\nlead_e23\nidem\n\n\n\n\n1\n2022\n2024\n2\n1\n1\n1\n0\n0\n\n\n1\n2024\n2025\n1\n0\n1\n1\nNA\n0\n\n\n2\n2019\nNA\n2\n1\n1\n0\nNA\n0\n\n\n3\n2023\n2024\n3\n1\n1\n0\n1\n1\n\n\n3\n2024\nNA\n2\n1\n2\n0\nNA\n1\n\n\n5\n2021\n2023\n2\n1\n1\n1\n0\n0\n\n\n5\n2023\n2024\n1\n0\n1\n1\n1\n0\n\n\n5\n2024\nNA\n3\n1\n2\n1\nNA\n0\n\n\n\n\n\nOn doit maintenant récupérer la dernière année de fin des situations où idem&gt;0, et la placer sur la première.\n\n\nCode\n1df$fin[is.na(df$fin)] = 2025\n2df$lead_e23[is.na(df$lead_e23)]   = -10\n\n3df$truefin = ifelse((df$lead_e23 != df$e23) & df$idem&gt;0, df$fin,0)\n\n4df = df %&gt;% group_by(id) %&gt;% mutate(truefin = sum(truefin))\ndf$fin = ifelse(df$idem&gt;0, df$truefin, df$fin)\n\ndf = select(df, -c(y,e23,lead_e23,idem))\n\nkable(df)\n\n\n\n1\n\nOn remplace l’année de la censure par sa valeur (important pour id=3).\n\n2\n\nPour régler un problème de gestion des NA avec ifelse. A tester avec if_else ou case_when.\n\n3\n\nOn recupère la valeur de l’année de fin lorsqu’il y a une succession d’états de même nature pour l’analyse.\n\n4\n\non remplace la valeur dans la variable fin en cas de succession seulement.\n\n\n\n\n\n\n\nid\ndeb\nfin\nn23\ne\ntruefin\n\n\n\n\n1\n2022\n2024\n1\n1\n0\n\n\n1\n2024\n2025\n1\n1\n0\n\n\n2\n2019\n2025\n1\n0\n0\n\n\n3\n2023\n2025\n1\n0\n2025\n\n\n3\n2024\n2025\n2\n0\n2025\n\n\n5\n2021\n2023\n1\n1\n0\n\n\n5\n2023\n2024\n1\n1\n0\n\n\n5\n2024\n2025\n2\n1\n0\n\n\n\n\n\nOn peut [enfin] sélectionner et conserver une seule ligne par individu et générer la variable de durée\n\n\nCode\ndf= select(df,-truefin)\n\ndf = df %&gt;%  group_by(id) %&gt;% mutate(nn23     = cumsum(n23)) \ndf = filter(df, n23==nn23)\n\ndf$dur= df$fin - df$deb + 1 \n\ndf = select(df, -c(n23,nn23))\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ne\ndur\n\n\n\n\n1\n2022\n2024\n1\n3\n\n\n2\n2019\n2025\n0\n7\n\n\n3\n2023\n2025\n0\n3\n\n\n5\n2021\n2023\n1\n3"
  },
  {
    "objectID": "10-manipulation.html#footnotes",
    "href": "10-manipulation.html#footnotes",
    "title": "11  Eléments de mise en forme des données",
    "section": "",
    "text": "Des éléments de manipulation/programmation pour un exemple volontairement très compliqué sont donnés dans méthodes =&gt; notes méthodologiques. Ayant été fait en 2015, le code pour R est largement out of date↩︎\nIci on a une enquête réalisée une même année pour toute les observations, ce n’est pas toujours le cas. De même au lieu de l’année, si les datations avaient été données par l’âge, au moment de l’enquête l’âge varierait d’une personne à une autre. Ces datations différentes (année ou âge) peuvent être présentes dans chaque module biographique d’une enquête, ou dans le fichier des caractéristiques fixes. Dans ce cas l’information devra être récupérée↩︎\nLa mesure est ici discrète/groupée, il me semble toujours préférable d’allonger les durées à +1. On démarre donc toujours un premier janvier pour terminer un 31 décembre sur l’information est donnée par des année. Ici t=1 représente la première année après la sortie des études. Une personne qui aura eu un emploi durant cette année, l’aura eu durant cette première année, que ce soit 2 semaines après ou 11 mois après. Si on disposait des mois, cela pourrait être intéressant de modifier cette métrique temporelle. Voir exemple 3↩︎\nContrairement au durée annuelle je n’ai pas ajouté 1 à chaque durée, ce qui est de nouveau envisageable par exemple si on veut explicitement indiquer les évènements qui ont lieu le premier mois. Pour id=3 la relation a t-elle durée du 1er septembre au 30 novembre, ou du 30 septembre au 1er novembre?? On a toujours un problème de précision, mais ici d’une trentaine de jours↩︎\navec R par exemple la fonction unique de dplyr↩︎\nEn conservant l’information sur les années, on transformera la base en format individu-période et on procédera à une fusion des informations↩︎"
  },
  {
    "objectID": "11-concurrent.html#problématique",
    "href": "11-concurrent.html#problématique",
    "title": "12  Risques concurrents",
    "section": "12.1 Problématique",
    "text": "12.1 Problématique\nOn étudie un processus dont l’occurence a plusieurs modalités, types ou causes:\n\nLa mortalité par cause de décès, les types de sortie du chômage: formation, emploi, radiation.\nLes types de sortie de l’emploi: chômage, longue maladie, sortie du marché du travail hors retraite.\nLes lieux de migration ou les espaces de mobilité résidentielle\nLes types de rupture d’union: séparation-divorce, veuvage).\n\nRappel: Déjà abordé dans la partie théorie, avec un recueil de données de type prospectif les “perdu.e.s de vue” peuvent difficilement être assimilés à des sorties d’observation non informatives (censures).\nL’analyse des risques concurrents est un cas particulier des modèles multi-états avec différents risques considérés comme absorbants.\nEn présence de risques concurrents, l’estimation de Kaplan-Meier ne peut se faire que sous l’hypothèse d’indépendance entre chacun des risques. Sinon l’estimateur de Kaplan-Meier n’est plus une probabilité. Une estimation de type KM d’un évènement en concurrence avec d’autres impose que ces derniers soient traités comme des censures à droites non informatives. Mais il n’est pas possible de tester cette hypothèse."
  },
  {
    "objectID": "11-concurrent.html#risques-cause-specific-et-biais-sur-les-estimateurs-km",
    "href": "11-concurrent.html#risques-cause-specific-et-biais-sur-les-estimateurs-km",
    "title": "12  Risques concurrents",
    "section": "12.2 Risques cause-specific et biais sur les estimateurs KM",
    "text": "12.2 Risques cause-specific et biais sur les estimateurs KM\nSi les risques ne sont pas indépendants les uns par rapport aux autres, la somme des estimateurs de (1-KM) pour chaque risque n’est pas égale - elle est supérieure - à l’estimateur de (1-KM) où les risques concurrents sont regroupés en un évènement unique. Par exemple les décès si on analyse ses causes.\nLe risque calculé en considérant les risques concurrents comme des censures à droite est appelé “cause-specific risk.\nCause specific risk\nPour le risque de type \\(k\\), le risque cause-spécific en \\(t_i\\) est égal à:\n\\[h_k(t_i)=\\frac{d_{i,k}}{R_i}\\] Où \\(d_{i,k}\\) est le nombre d’évènement de type \\(k\\) survenu en \\(t_i\\) et \\(R_i\\) la population soumise en \\(t_i\\).\n Conséquence: si les risques ne sont pas indépendants, la fonction de survie estimée avec la méthode Kaplan Meier n’exprime plus une probabilité.\nExemple sur les décès causés par une malformation cardiaque\nDans la base d’origine, il n’y a pas directement cette dimension de risque concurrent, même si on trouve dans la littérature médicale des études prenant le décès rapide post greffe comme un risque de ce type. Les données étant assez anciennes, avec beaucoup de décès post-opératoire, je ne me suis pas « risquer » à générer directement un risque concurrent sur cette information. Une sortie concurrente a donc été simulée sans plus de précision (variable compet), que l’on considèrera non strictement indépendante à la cause d’intérêt. Ce risque entre donc en concurrence avec la cause du décès directement liée à la malformation cardiaque, que la personne ait été transplantée ou non.\n\n\n\n\n\n\n           |    Survival Status\n           |       (1=dead) \n    compet |         0          1 |     Total\n-----------+----------------------+----------\n         0 |        28          0 |        28 \n         1 |         0         56 |        56 \n         2 |         0         19 |        19 \n-----------+----------------------+----------\n     Total |        28         75 |       103 \n\n\nVariable compet:\ncause 1 =&gt; décès directement provoquer par la malformation: compet=1 cause 2 =&gt; autre cause compet=0 =&gt; censure à droite\nLorsqu’on a analysé le décès par la méthode KM, la proportion de survivant.e.s était de 15%.\nSi on applique la méthode de Kaplan Meier à la cause 1 en traitant la cause 2 comme une censure à droite (\\(n=18+29=48\\)), puis en sommant les deux estimateurs, la fonction de répartition excède 100% au bout de 1000 jours environs. La proportion de survivant.e.s est donc négative.\n\n\n\nFonction de répartition avec une cause concurrente traitée comme une censure à droite"
  },
  {
    "objectID": "11-concurrent.html#estimations-en-présence-de-risques-concurrents-cif",
    "href": "11-concurrent.html#estimations-en-présence-de-risques-concurrents-cif",
    "title": "12  Risques concurrents",
    "section": "12.3 Estimations en présence de risques concurrents (CIF)",
    "text": "12.3 Estimations en présence de risques concurrents (CIF)\n\n12.3.1 Estimation non paramétrique\n\nUtiliser l’estimateur de Nelson Aalen: il s’agit du risque instantané cumulé. Comme il ne s’agit pas d’une probabilité, il a été longtemps utilisé comme mesure de l’incidence en présence de risques concurrents dans une logique dite cause specific.\n\n\\[H_k (t_i)=\\sum_{t_i\\leq t}\\left(\\frac{e_{i,k}}{n_i}\\right) \\]\n\nActuellement, l’estimateur le plus utilisé est la fonction dite d’incidence cumulée - CIF- de Kalbfleisch-Prentice et Marubini-Valscchi:\n\nIl repose sur une probabilité tout en supportant la non indépendance des risques.\nSon interprétation est identique à la fonction de répartition \\(F(t)=1-S(t)\\). Cette fonction est donc croissante.\nIl est possible de tester les différences entres CIF: test de Gray (R, SAS) ou test de Pepe-Mori (Stata).\n\n\nCIF (Cumulative Incidence Function)\n\nSi \\(h_k(t_i)\\) est le risque cause-spécific en \\(t_i\\) et \\(S(t_i-1)\\) l’estimateur de Kaplan-Meier en \\(t_i-1\\) lorsque tous les risques sont regroupés en un évènement unique, l’incidence cumulée pour le risque \\(k\\) en \\(t_i\\) est égale à:\n\n\\[IC_k(t_i)= \\sum_{t_i\\leq t}S(t_i-1)h_k(t_i)\\]\n\nLes valeurs prises par cette fonction pour la cause \\(k\\) ne dépendent donc pas seulement des individus ayant observé l’évènement à partir de cette seule cause, mais aussi du nombre de personnes qui n’ont pas encore observés l’évènement à partir des autres causes identifiées. Cette dernière information est donnée par \\(S(t_i-1)\\).\nL’incidence cumulée peut ainsi s’interpréter, simplement, comme la proportion d’individus qui sont sortis du risque jusqu’en \\(t_i\\) en raison de la cause \\(k\\).\n\n\n\n\nRisques concurrent: estimation de la CIF\n\n\n\n\n\n            failure:  compet == 1\n competing failures:  compet == 2\n\n    Time       CIF         SE     [95% Conf. Int.]\n--------------------------------------------------\n       1    0.0097     0.0097     0.0009    0.0477\n       2    0.0388     0.0190     0.0127    0.0892\n       3    0.0583     0.0231     0.0239    0.1149\n\n\n       5    0.0777     0.0264     0.0363    0.1395\n       6    0.0874     0.0278     0.0429    0.1515\n       8    0.0971     0.0292     0.0497    0.1634\n       9    0.1068     0.0304     0.0566    0.1751\n      12    0.1166     0.0316     0.0638    0.1868\n      16    0.1362     0.0338     0.0785    0.2099\n      18    0.1461     0.0349     0.0860    0.2212\n      21    0.1657     0.0367     0.1014    0.2437\n      32    0.1756     0.0376     0.1093    0.2550\n      37    0.1856     0.0384     0.1173    0.2662\n      40    0.1957     0.0393     0.1254    0.2775\n      43    0.2058     0.0400     0.1337    0.2888\n      45    0.2158     0.0408     0.1420    0.2999\n      50    0.2259     0.0415     0.1503    0.3110\n      51    0.2360     0.0422     0.1588    0.3221\n      53    0.2461     0.0428     0.1673    0.3330\n      58    0.2562     0.0434     0.1759    0.3439\n      61    0.2662     0.0440     0.1845    0.3548\n      66    0.2763     0.0445     0.1932    0.3656\n      69    0.2864     0.0450     0.2020    0.3763\n      72    0.3066     0.0459     0.2197    0.3976\n      77    0.3167     0.0464     0.2286    0.4082\n      78    0.3267     0.0467     0.2376    0.4187\n      81    0.3368     0.0471     0.2466    0.4292\n      85    0.3469     0.0475     0.2556    0.4396\n      90    0.3570     0.0478     0.2648    0.4500\n      96    0.3671     0.0481     0.2739    0.4604\n     102    0.3771     0.0484     0.2831    0.4707\n     110    0.3874     0.0487     0.2925    0.4812\n     149    0.3980     0.0489     0.3021    0.4920\n     165    0.4085     0.0492     0.3118    0.5027\n     186    0.4193     0.0495     0.3217    0.5137\n     188    0.4301     0.0497     0.3316    0.5246\n     207    0.4408     0.0499     0.3417    0.5354\n     219    0.4516     0.0501     0.3517    0.5462\n     263    0.4624     0.0502     0.3618    0.5570\n     285    0.4846     0.0505     0.3826    0.5791\n     308    0.4957     0.0506     0.3931    0.5900\n     340    0.5068     0.0507     0.4037    0.6009\n     583    0.5221     0.0514     0.4171    0.6168\n     675    0.5401     0.0524     0.4322    0.6361\n     733    0.5580     0.0532     0.4477    0.6548\n     995    0.5808     0.0548     0.4659    0.6795\n    1032    0.6036     0.0559     0.4851    0.7031\n    1386    0.6340     0.0583     0.5083    0.7357\n\n\n            failure:  compet == 2\n competing failures:  compet == 1\n\n    Time       CIF         SE     [95% Conf. Int.]\n--------------------------------------------------\n       3    0.0097     0.0097     0.0009    0.0477\n       6    0.0194     0.0136     0.0038    0.0619\n      16    0.0292     0.0166     0.0079    0.0761\n      17    0.0391     0.0191     0.0128    0.0897\n      28    0.0489     0.0213     0.0182    0.1029\n      30    0.0587     0.0232     0.0240    0.1157\n      35    0.0686     0.0250     0.0302    0.1286\n      36    0.0786     0.0267     0.0367    0.1411\n      39    0.0885     0.0282     0.0435    0.1534\n      40    0.0986     0.0296     0.0504    0.1658\n      68    0.1188     0.0322     0.0650    0.1901\n      80    0.1288     0.0334     0.0724    0.2020\n     100    0.1389     0.0345     0.0800    0.2138\n     153    0.1495     0.0356     0.0880    0.2261\n     334    0.1605     0.0368     0.0964    0.2392\n     342    0.1720     0.0381     0.1052    0.2526\n     852    0.1913     0.0417     0.1175    0.2787\n     979    0.2141     0.0460     0.1320    0.3094\n\n\nEn présence du risque concurrent, et traité comme tel, la moitié des personnes sont décédées suite à la malformation cardiaque au bout de 308 jours (200 jours avec une estimation de type « cause specific »).\nOn peut vérifier que la somme des estimateurs permet d’obtenir la survie toutes causes confondues. Il n’y a pas de surprise à cela, dans l’estimateur Marubini-Valscchi la survie d’ensemble intervient comme un facteur de pondération du quotient d’intensité dite « cause-specific ».\n\n\n\n\n\n\nR-Stata-Sas-Python\n\n\n\nL’estimation avec des risques de type « cause-specific » demande juste de recoder la variable évènement/censure, en glissant les risques concurrents en censure à droite.\nPour l’estimation des CIF (risque de sous répartition):\n\nR: la librairie cmprsk permet d’estimer simplement les incidences cumulées avec la fonction cuminc.\nSas: maintenant directement estimable avec proc lifetest. Il suffit d’indiquer le ou les risques d’intérêt dans l’instruction indiquant la variable de durée et de censure avec l’option failcode=valeur.\nStata: Estimation avec la commande externe stcompet. La commande génère des variables qui demande des manipulations supplémentaires pour afficher les résultats sous forme de tableau par exemple. On peut utiliser et préférer la commande externe stcomlist.\nPython: le wrapper de R (cmprsk) ne fonctionne plus à ce jour à défaut de mise à jour [2022].\n\n\n\n\n\n12.3.2 Compararaison des CIF\n\nTest d’homogénéité de Gray: est basé sur une autre mesure du risque en évènement concurrent. Sur le principe, identique à la philosopjie des test du logrank. Il s’agit du « subdistribution risks (« risque de sous-répartition », A.Latouche). Son interprétation n’est pas aisée car les personnes ayant observé un risque concurrent sont remises dans le Risk Set. Mais il est directement lié à l’estimation des CIF. Disponible avec SAS et R. Il est également sensible l’hypothèse de proportionnalité et à la distribution des censures à droites entre les groupes comparés. A ma connaissance il n’y a pas de variantes pondérées.\nTest de Pepe & Mori: teste directement deux courbes d’incidences et seulement 2. Je n’ai pas le recul nécessaire sur cette alternative, qui n’est implémenté que dans Stata.\n\n\nTest de Gray pour la variable surgery\n\n\nRisques\nChi2\nP&gt;Chi2\n\n\n\n\nCause1\n5.783\n0.0161\n\n\nCause2\n0.129\n0.7191\n\n\n\n\nTest de Pepe-Mori pour la variable surgery\n\n\nRisques\nChi2\nP&gt;Chi2\n\n\n\n\nCause1\n6.203\n0.0127\n\n\nCause2\n1.880\n0.7038\n\n\n\n\n\n\n\n\n\nR-Stata-Sas-Python\n\n\n\n\nSas: le test de Gray est estimé si on ajoute l’option strata=nom_variable à la proc lifetest sous risque concurrent (voir encadré précédent). Le test de Pepe-Mori est disponible via une macro externe (%compcif: non testée) :\nStata: Le test de Gray n’est pas disponible, il faut passer par une exécution de la fonction cuminc de la librairie R cmprsk directement dans stata (voir la commande rsource). Pour faire plus simple, on peut estimer le modèle de Fine-Gray avec une seule variable (discrète). Le résultat est comparable à celui du test (voir plus bas). Le test de Pepe-Mori est disponible via la commande externe stpepemori.\nR: On ajoute une variable à la fonction cuminc de la librairie cmprsk. Pas de test de Pepe-Mori sur les fonctions d’incidence à ma connaissance.\nPython: ne pas essayer d’utiliser la librairie cmprsk qui n’est pas mis à jour et ne fonctionne plus."
  },
  {
    "objectID": "11-concurrent.html#modèles",
    "href": "11-concurrent.html#modèles",
    "title": "12  Risques concurrents",
    "section": "12.4 Modèles",
    "text": "12.4 Modèles\n\n12.4.1 Modèles Semi paramétriques\nCette présentation sera plutôt brève. Dans le domaine des sciences sociales, je préconise plutôt l’utilisation d’un modèle multinomial à temps discret de type logistique. Le modèle de Cox en présence de risques concurrent n’est valable que dans une logique de risques « cause-specific », le modèle de Fine et Gray bien que directement relié à l’estimation des incidences cumulées, repose sur une définition du risque (de sous répartition) dont l’interprétation n’est pas naturelle. Il est également soumis à l’hypothèse de proportionnalité des risques.\nModélisation des risques « cause-specific » : Cox\nModèle de Cox «standard» pour chaque évènement, les évènements concurrents sont traités comme des censures à droite. Aucune interprétation sur les fonctions d’incidence ne peut-être faite.\nModèle de Fine-Gray: subdistribution hazard regression\nModèle de type semi-paramétrique avec une redéfinition du risque lié à l’estimation des fonctions d’incidence (voir test de Gray). La différence avec le Cox classique réside dans le calcul du risk-set : les évènements concurrents ne sont pas considérés comme des censures, on laisse les individus leur « survivre » jusqu’à la durée maximale observée dans l’échantillon. L’interprétation n’est donc pas très intuitive (Fine et Gray le soulignent). Ce modèle est relativement contreversé. Il ne sera donc pas exécuté pour l’application\n\nPour les questions liées à l’interprétation de ces deux types de modèles, se reporter à: https://onlinelibrary.wiley.com/doi/epdf/10.1002/sim.7501\n\n\n\n\n\n\nR-Stata-Sas-Python\n\n\n\n\nR: on utilise la fonction crr du package cmprsk.\nSas: même principe que pour l’estimation non paramétrique, on ajoute l’option eventcode=valeur à l’instruction model de la proc phreg.\nStata: on utilise la commande interne stcrreg.\nPython : ne pas essayer d’utiliser la librairie cmprsk qui n’est pas mis à jour et ne fonctionne donc plus.\n\n\n\n\n\n12.4.2 Modèle à temps discret\n\nIl s’agit d’une extension du modèle à temps discret à évènement unique (toutes causes regroupées) avec ici le modèle logistique multinomial.\nS’il ne permet pas une interprétation sur les fonctions d’incidences, les risques concurrents ne sont pas traitées comme des censures à droite.\nLe modèle multinomial repose sur une hypothèse dite « d’indépendance des alternatives non pertinentes » (IIA). Cela peut donc paraitre contradictoire d’utiliser ce modèle pour des évènements qui sont supposés non indépendants. Néanmoins la dépendance entre risques concurrents n’est pas non plus stricte et cette hypothèse d’IIA, seulement testable par le bon sens, est souvent illustrée par l’exemple des couleurs des bus dans le choix du mode de transport, ou les couleurs de chaussure dans les études marketing. Soit est une situation relativement limite.\nEn terme de lecture, les estiupateurs du modèle logistique multinomial peuvent directement s’interpréter comme des rapports de risque (ou relative risk ratio).\nEn sciences sociales, il me semble que ce type de modèle soit à privilégier.\nOn peut également envisager un modèle de type probit multinomial, mais on peut rencontrer des problèmes d’estimations (repose sur la loi normale multivariée). Prévoir un regroupement des causes concurrentes, et dans tous les cas de figure ne pas dépasser trois causes.\nNiveau lecture, on peut utiliser une méthode de standardisation, de type AME (Average Marginal Effect).\n\nPour l’application, nous avons pris le mois (30 jours) comme métrique temporelle. On rappelle que les valeurs des estimateurs sont fictives en raison de la simulation des évènement pour le risque concurrent (cause2)\n\n\nTable 12.1: Modèle logistique multinomial avec risques concurrent\n\n\n\n\n(a) Cause 1\n\n\nCause 1\nRRR\np&gt;|z|\n95% IC\n\n\n\n\n\\(t\\)\n0.816\n0.000\n0.752 - 0.885\n\n\n\\(t^2\\)\n1.003\n0.000\n1.001 - 1.005\n\n\n\\(year\\)\n0.879\n0.116\n0.749 - 1.032\n\n\n\\(age\\)\n1.045\n0.012\n1.010 - 1.081\n\n\n\\(surgery\\)\n0.318\n0.033\n0.110 - 0.913\n\n\n\\(constante\\)\n0.231\n0.000\n0.148 - 0.360\n\n\n\n\n\n\n(b) Cause 2\n\n\nCause 2\nRRR\np&gt;|z|\n95% IC\n\n\n\n\n\\(t\\)\n0.817\n0.003\n0.713 - 0.935\n\n\n\\(t^2\\)\n1.003\n0.052\n1.000 - 1.006\n\n\n\\(year\\)\n0.816\n0.141\n0.622 - 1.070\n\n\n\\(age\\)\n1.011\n0.654\n0.964 - 1.061\n\n\n\\(surgery\\)\n0.541\n0.431\n0.117 - 2.496\n\n\n\\(constante\\)\n0.076\n0.000\n0.037 - 0.157\n\n\n\n\n\n\nNotes:\n\nOn a utilisé le terme RRR - Relative Risk Ratio - pour la colonne raportant les estimations. Dans un cadre de risque concurrent il est un peu difficile d’utiliser formellement la notion de hazard rate tel qu’il a été difini plus haut, enfin les modèles multinomiaux ne reportent pas formellement des Odds Ratios dont l’utilisation devrait être réservé exclusivement à une alternative binaire.\nles variables year et age ont été centrées sur leur valeur moyenne pour donner aux constantes des valeurs acceptables.\nPour faciliter la lecture on peut utiliser une méthode de standardisation de type AME (Average Marginal Effect)."
  },
  {
    "objectID": "12-parametrique.html#principes",
    "href": "12-parametrique.html#principes",
    "title": "13  Modèles paramétriques",
    "section": "13.1 Principes",
    "text": "13.1 Principes\n\nDans les modèles paramétriques usuels, la durée de survie est distribuée selon une loi dont la densité \\(f(t)\\) pleinement paramétrée.\n\nPour utiliser l’approche paramétrique, il faut avoir de bonnes raisons de penser que durée de survie sont distribués selon une certaine loi connue plutôt qu’une autre.\nLa majorité des distributions reposent sur une hypothèse dite AFT (Acceleretad Failure Time). Une autre, Gompertz, très utilisée en démographie (mortalité), repose seulement sur la proportionnalité des risques. Certaines peuvent reposer sur les deux comme le modèle de Weibull. Enfin, les modèles log-logistique ou log-normal n’ont qu’une paramétrisation de type AFT."
  },
  {
    "objectID": "12-parametrique.html#hypothèse-aft-accelerated-failure-time",
    "href": "12-parametrique.html#hypothèse-aft-accelerated-failure-time",
    "title": "13  Modèles paramétriques",
    "section": "13.2 Hypothèse AFT: Accelerated Failure Time",
    "text": "13.2 Hypothèse AFT: Accelerated Failure Time\nL’hypothèse AFT signifie que l’effet des covariables est multiplicatif par rapport à la durée de survie/séjour. Par opposition, les modèles PH décrivent un effet multiplicatif par rapport au risque.\nSelon les caractérisques des individus, le temps ne s’écoulent pas à la même vitesse, ils ne partagent donc plus la même métrique temporelle. Cela renvoie a des interprétations de type dilation/contraction du temps, par analogie à la théorie de la relativité.\nExemple simple: la durée de vie d’un être humain et d’un chien.\nOn dit qu’une année de vie d’un être humain est équivalent à 7 années de vie d’un chien. C’est typiquement une hypothèse d’AFT.\n\\(S_h(t) = S_c(7\\times t)\\).\nC’est ce facteur multiplicatif qu’estime un modèle paramétrique de type AFT.\n\\[S(t_i | X_1)=  S(\\phi t_i | X_0)\\]\nRemarque: si un modèle s’estime AFT s’estime également sous hypothèse PH comme celui de Weibull: \\(h(t_i | X_1)= -\\rho \\phi h(t_i | X_0)\\)\n\nAvantage: l’interprétation des modèles est directement liée aux fonctions de survie. Cela s’avère donc pratique après une analyse non paramétrique de type Kaplan-Meier par exemple.\nInconvénient: ne permet pas l’introduction de variables dynamiques.\n\nHumain versus chien: la probabilité qu’un être humain survive 80 ans est égale à la probabilité qu’un chien survive 11 ans (80/7). Le temps s’écoulerait donc plus vite pour le chien que pour l’être humain du point de vue d’un référentiel extérieur. Ce raisonnement peut s’appliquer aux quantile du temps de survie: le temps de survie médian d’un être humain est 7 fois plus élevé que celui d’un chien. En terme d’interprétation des paramètres estimés, si la durée de survie est plus courte, alors le risque est plus élevé."
  },
  {
    "objectID": "12-parametrique.html#principe-de-construction-des-modèles-aft",
    "href": "12-parametrique.html#principe-de-construction-des-modèles-aft",
    "title": "13  Modèles paramétriques",
    "section": "13.3 Principe de construction des modèles AFT",
    "text": "13.3 Principe de construction des modèles AFT\nLe raisonnement mathématique est ici plus complexe que pour les modèles de Cox ou à durée discrète. On donnera juste quelques pistes en début de raisonnement. On part d’une expression proche du modèle linéaire à une transformation logarithmique près de la variable dépendante. En imposant la contrainte \\(t_i&gt;0\\), en ne posant qu’une seule covariable \\(X\\) de type binaire, et en se situant de nouveau dans une logique de temps continu (pas d’évènement simultané):\n\\[log(t_i)= \\alpha_0 +  \\alpha_1X_i + bu_i\\]\n\\(b\\) est un paramètre d’échelle identique pour toutes les observations et \\(u_i\\) un terme terme d’erreur qui suit une loi de distribution de densité \\(f(u)\\). Cette combinaison linéaire définira le paramètre de position. C’est la forme de \\(f(u)\\) qui définie le type de modèle paramétrique.\nOn peut écrire: \\(f(u_i) = f(\\frac{log(t_i)- \\alpha_0 - \\alpha_1X_i}{b})\\).\nRemarque: pour une distibution normale/gaussienne, le paramètre de position est l’espérance et le paramètre d’échelle l’écart-type."
  },
  {
    "objectID": "12-parametrique.html#quelques-modèles-paramétriques-usuels",
    "href": "12-parametrique.html#quelques-modèles-paramétriques-usuels",
    "title": "13  Modèles paramétriques",
    "section": "13.4 Quelques modèles paramétriques usuels",
    "text": "13.4 Quelques modèles paramétriques usuels\nModèle exponentiel et de Weibull\nWeibull\n\nPeut estimer un modèle PH ou AFT, d’où sa popularité.\nDistribution monotone des durées d’évènement, toujours croissante ou décroissante.\n\\(f(t)=\\lambda\\alpha t^{\\alpha - 1}e^{-\\alpha t^\\lambda}\\) et \\(h(t)=\\lambda\\alpha(\\lambda t)^{\\alpha - 1}\\), \\(\\alpha&gt;0\\) et \\(\\lambda&gt;0\\). Si \\(\\lambda&gt;1\\) le risque est croissant, décroissant si \\(\\lambda&lt;1\\), et est constant (loi exponentielle) si \\(\\lambda=1\\).\n\nExponentiel\n\nProcessus sans mémoire, utilisé pour étudier par exemple la durée de vie composants électriques ou électroniques.\nLa fonction de risque est une constante.\nCas limite de la loi de Weibull. Un modèle de type exponentiel peut-être de type AFT ou PH.\nPour contourner la constance du risque dans le temps, on peut estimer un modèle en scindant la durée en plusieurs intervalles. Le risque sera constant à l’intérieur de ces intervalles, il s’agit d’un modèle “exponential piecewise” (exponentiel par morceau).\n\nLog-logistique\n\nEstime un modèle de type AFT seulement. Proche du modèle log-normal (plus difficile à estimer).\nPermet une interprétation en terme d’Odds de survie.\nLa fontion du risque peut-être “U-shaped” (unimodale croissante puis décroissante).\n\nAutres lois: Gompertz (PH seulement), Gamma et Gamma généralisé…..\nSélection de la loi On peut sélectionner la loi en comprarant les AIC où les BIC des modèles. Pour le modèle de Weibull, on peut regarder s’il ajuste bien les données si la transformation \\(log(-log(S(t_i)))\\) est linéaire par rapport à \\(log(t_i)\\).\nApplication\nComparaison des AIC (sans covariable)\n\nWeibull: 400.1\n\nExponentiel: 461.0\n\nGompertz: 409.6\n\nLog-logistique: 391.8"
  },
  {
    "objectID": "12-parametrique.html#exemple-avec-le-modèle-de-weibull",
    "href": "12-parametrique.html#exemple-avec-le-modèle-de-weibull",
    "title": "13  Modèles paramétriques",
    "section": "13.5 Exemple avec le modèle de Weibull",
    "text": "13.5 Exemple avec le modèle de Weibull\n\n\nTable 13.1: Modèle de Weibull\n\n\n\n\n(a) Accelerated Failure Time (AFT)\n\n\nVariables\nTime Ratio\np&gt;|z|\n95% IC\n\n\n\n\n\\(year\\)\n1.176\n0.184\n0.926 - 1.493\n\n\n\\(age\\)\n0.940\n0.013\n0.896 - 0.987\n\n\n\\(surgery\\)\n7.173\n0.011\n1.557 - 33.048\n\n\n\\(\\rho\\)\n0.556\n-\n0.464 - 0.667\n\n\n\n\n\n\n(b) Proportional hazard (PH)\n\n\nVariables\nHR\np&gt;|z|\n95% IC\n\n\n\n\n\\(year\\)\n0.914\n0.175\n0.802 - 1.041\n\n\n\\(age\\)\n1.035\n0.014\n1.007 - 1.063\n\n\n\\(surgery\\)\n0.334\n0.012\n0.143 - 0.783\n\n\n\\(\\rho\\)\n0.556\n-\n0.464 - 0.667\n\n\n\n\n\n\nNote: la constante n’est pas reporté. \\(\\rho\\) indique la valeur estimé d’un paramètre de forme. Son signe indique sur le risque est décroissant ou croissant (1 si risque constant), et permet de passer de la paramétrisation AFT à la paramétrisation PH (et inversement).\n\nAFT: Un jour de survie d’une personne qui n’a pas été opérée d’un pontage correspond environ à 7 jours de survie d’une personne opérée. Cette remise à l’échelle de la métrique temporelle entre les deux groupes exprime bien le gain en durée de survie pour les personnes opérées, soit des risques journaliers de décès plus faibles (et plus faibles à valeurs constantes, proportionnalité oblige).\nPH: Lecture en rapport de risque ou hazard rate (idem Cox). Si on avait reporté les coefficients (échelle log) \\(b_{ph} = -\\rho \\times b_{aft}\\). Ici \\(-0.556 \\times (1.97) = -1.096\\). Et \\(e^{-1.096}=0.334\\)\n\nAttention: on ne peut pas comparer la qualité d’un modèle paramétrique à celle d’un modèle de Cox par des critères type AIC ou BIC. Les deux méthodes d’estimation diffèrent."
  },
  {
    "objectID": "12-parametrique.html#le-modèle-de-parmar-royston",
    "href": "12-parametrique.html#le-modèle-de-parmar-royston",
    "title": "13  Modèles paramétriques",
    "section": "13.6 Le modèle de Parmar-Royston",
    "text": "13.6 Le modèle de Parmar-Royston\n\nLe bon ajustement par une loi de distribution predéfinie peut s’avérer contraignante. Le modèle de Cox avait justement pour objectif de se défaire de cette contrainte, la plupart des distributions utilisée étant monotone ou unimodale (log-logistique ou log-normal).\nLe principe des splines peut-être rapproché de celui qui a été utilisé plus haut dans le modèle logistique à durée discrète avec l’introduction des polynomes [ \\(f(t)= (a_1\\times t) + (a_2\\times t^2) + (a_3\\times t^3) + ...+ (a_k\\times t^k)\\).\n\nCette méthode brute d’ajustement consiste finalement à introduire une intéraction ou plusieurs intéractions entre la variable de durée avec elle-même.\nElle est sujette à une forte sensibilité aux outliers (overfitting) au delà de \\(k&gt;2\\) [lors la formation il suffit de la tester pour k=3 et calculer la probabilité conditionnelle pour s’en convaincre].\n\nLes splines cubiques restreintes propose une méthode d’ajustement et de lissage de meilleure qualité et permet de contrôler les effets overfitting.\n\nles splines cubiques sont donc basées sur des polynomes d’ordre 3 (d’où cubique) avec une estimation par morceau (intervalles). les morceaux sont définis manuellement ou par un nombre de degrés de liberté obtenu par quantile du logarithme de la fonction de survie après avoir exclu les observation censurées.\n\nDeux degrés de liberté (1 noeud) avec un intervalle allant jusqu’au log de la moitié des survivants et un second à partir de cette seconde moitié.\nSur le même principe trois degrés de liberté (2 noeuds) coupe la durée en 3 intervalles sur ses terciles.\nEn pratique, il est préférable de donner à l’application de nombre de degré de liberté plutôt que d’indiquer manuellement la position des noeuds.\nIl convient également de ne pas être trop gourmand sur le nombre de noeuds, un ou deux étant souvant suffisant (donc 2 ou 3 degrés de liberté).\nOn peut choisir le nombre de degrés de liberté en estimant des modèles sans covariables et comparer les AIC (vraisemblance pénalisée).\n\n\nContrairement aux autres modèles, et sans rentrer dans les détails, le modèle de Parmar-Royston part de la fonction de risque cumulée et non des taux de risque/hasard. Les risk ratios sont obtenus en utilisant les relations entre les différentes grandeurs (voir section *théorie).\n\nExemple\nAvec 2 degrés de liberté (un noeud):\n\nModèle de Parmar-Roytston\n\n\nVariables\n$e^(b)\np&gt;|z|\n95% IC\n\n\n\n\n\\(year\\)\n0.885\n0.067\n0.777 - 1.008\n\n\n\\(age\\)\n1.030\n0.026\n1.004 - 1.058\n\n\n\\(surgery\\)\n0.373\n0.025\n0.159 - 0.876\n\n\n\\(spline 1\\)\n3.157\n0.000\n2.503 - 3.981\n\n\n\\(spline 2\\)\n1.289\n0.002\n1.099 - 1.511\n\n\n\\(constante\\)\n0.510\n0.000\n0.386 - 0.674\n\n\n\nA savoir:\n\nAvec un degré de liberté, le modèle de Parmar-Royston estime un modèle de Weibull sous paramétrisation PH.\nLes paramètres pour les splines ne sont pas interprétables directement. Ils servent calculer la baseline du risque via l’équation du polynome (non reporté car expression bien corsée)."
  },
  {
    "objectID": "13-annexe.html#tests-grambsch-therneau-ols-sur-les-résidus-de-schoenfeld",
    "href": "13-annexe.html#tests-grambsch-therneau-ols-sur-les-résidus-de-schoenfeld",
    "title": "14  Annexes",
    "section": "14.1 Tests Grambsch-Therneau OLS sur les résidus de Schoenfeld",
    "text": "14.1 Tests Grambsch-Therneau OLS sur les résidus de Schoenfeld\n\n\n\n\n\n\nImportant\n\n\n\nAttention il ne s’agit pas du test actuellement implémenté dans la nouvelle version de survival (v3) qui, malheureusement, lui a substitué la version dite exacte (moindres carrés généralisés). Le programme de la fonction du test OLS est néanmoins facilement récupérable et exécutable. lien.\nJe continue de préconiser l’utilisation de cette version OLS du test, reproductible avec les autres applications statistiques (Stata,Sas,Python).\n\n\n\nLe test dit “simplifié”, qui n’apparait pas dans le texte original de P.Gramsch et T.Thernau lien, répond à un soucis d’instabilité des variances des résidus de Schoenfeld en fin de durée d’observation lorsque peu d’observation restent soumises au risque. Cet argument est soulevé dans leur ouvrage de 2022 lien avant d’en présenter sa version.\nIl est simplifié car on applique à tous les résidus bruts la variance du paramètre (\\(b\\)) estimés par le modèle de Cox.\nLe test devient alors un simple test de corrélation entre les résidus et une fonction de la durée (centrée). Dans l’esprit, il peut être également approché par une regression linéaire par les moindre carrés ordinaires entre les résidus et une fonction de la durée (voir page 134 de l’ouvrage de Grambsch et Therneau).\n\nSoit les données suivantes, avec t la variable de durées, Y la variable de censure et X la seule et unique covariable.\n\nPas d’évènement simultané (donc pas de correction de la vraisemblance)\nCovariable de type indicatrice\n\n\n\n\n\\(t_i\\)\n\\(Y_i\\)\n\\(X_i\\)\n\n\n\n\n1\n1\n1\n\n\n2\n0\n0\n\n\n3\n0\n0\n\n\n4\n1\n1\n\n\n5\n1\n1\n\n\n6\n1\n0\n\n\n7\n0\n1\n\n\n\n\ntest = data.frame(time=  c(1,2,3,4,5,6,7),\n                    Y=c(1,0,0,1,1,1,0),\n                    X=     c(1,0,0,1,1,0,1))\n\nEstimation du modèle de Cox:\n\nlibrary(survival)\nfit = coxph(formula = Surv(time, Y) ~ X, data=test)\nfit\n\nCall:\ncoxph(formula = Surv(time, Y) ~ X, data = test)\n\n    coef exp(coef) se(coef)    z     p\nX 0.6217    1.8622   1.1723 0.53 0.596\n\nLikelihood ratio test=0.31  on 1 df, p=0.5797\nn= 7, number of events= 4 \n\n\nCalcul des résidus brut (si et seulement si \\(Y=1\\)) dans le cas d’une seule covariable avec \\(b\\) égal à 0.62:\n\\[rs_{i}=X_{i}- \\sum_{j\\in R_i}X_{i}\\frac{e^{0.62\\times X}}{\\sum_{j\\in R_i}e^{0.62\\times X}}= X_{i} - E(X_{j\\in R_i})\\] Il y a ici 4 résidus à calculer, pour \\(t=(1,4,5,6)\\)\nRésidus pour \\(t=1\\)\n\n\\(a_1= \\sum_{j\\in R_i}e^{0.62\\times X} = e^{0.62} + 1 + 1 + e^{0.62} + 1 + e^{0.62}= 10.43\\)\n\\(b_1= \\sum_{j\\in R_i}X_{i}\\frac{e^{0.62\\times X}}{\\sum_{j\\in R_i}e^{0.62\\times X}} = 4\\times\\frac{e^{0.62}}{10.43} = 0.71\\)\n\\(r_1 = 1 - 0.71 = 0.29\\)\n\nRésidus pour \\(t=4\\)\n\n\\(a_4 = e^{0.62} + e^{0.62} + 1 + e^{0.62} = 6.58\\)\n\\(b_4 = 4\\times\\frac{e^{0.62}}{6.58} = 0.84\\)\n\\(r_4 = 1 - 0.84 = 0.15\\)\n\nRésidus pour \\(t=5\\)\n\n\\(a_5 = e^{0.62} + e^{0.62} + 1 = 4.71\\)\n\\(b_5 = 2\\times\\frac{e^{0.62}}{4.71} = 0.78\\)\n\\(r_5 = 1 - 0.78 = 0.21\\)\n\nRésidus pour \\(t=6\\)\n\n\\(a_6 = e^{0.62} + 1 = 2.86\\)\n\\(b_6 = \\frac{e^{0.62}}{2.86} = 0.65\\)\n\\(r_6 = 0 - 0.65 = -0.65\\)\n\nLes résidus “standardisés”, ou plutôt scaled residuals (je cale sur une traduction correcte en français) sont égaux à:\n\\[sr_i = b + nd \\times Var(b) \\times r_i\\] Avec \\(nd= \\sum Y_i\\)\n\n\\(\\sum Y_i = 4\\)\n\\(Var(b) = (1.1723)^2=1.37\\)\n\\(sr_1 = 0.62 + 4\\times 1.37 \\times 0.29 = 2.20\\)\n\\(sr_4 = 0.62 + 4\\times 1.37 \\times 0.15 = 1.47\\)\n\\(sr_5 = 0.62 + 4\\times 1.37 \\times 0.21 = 1.78\\)\n\\(sr_6 = 0.62 + 4\\times 1.37 \\times (-0.65) = -2.95\\)\n\nAvec \\(g(t_i)\\) une fonction de la durée (\\(g(t_i)=t_i\\), \\(g(t_i)=1-KM(t_i)\\)…) et \\(\\overline{g(t)}\\) sa valeur moyenne, la statistique du test score simplifié pour une covariable est égale à :\n\\[\\frac{[\\sum_i(g(t_i) - \\overline{g(t_i)}\\times sr_i)]^2}{nd \\times Var(b) \\times (\\sum_i(g(t_i) - \\overline{g(t_i)})^2}\\] Et suis un \\(\\chi^2\\) à 1 degré de liberté.\nAvec \\(\\overline{g(t_i)}=t_i\\), le calcul de la statistique de test est:\n\n\\(\\overline{g(t_i)}= \\frac{28}{7}=4\\)\n\\(\\frac{[(1-4)\\times 2.20] + [(4-4)\\times 1.47 + (5-4)\\times 1.78 + (6-4)\\times (-2.95)]^2 }{4\\times 1.37 \\times [(1-4)^2 + (4-4)^2 + (5-4)^2 + (6-4)^2] } = \\frac{114.9}{76.72} = 1.49\\)\n\n\n#source(\"D:/D/Marc/SMS/FORMATIONS/analyse_duree/cox.zphold/cox.zphold.R\")\n\nsource(\"https://raw.githubusercontent.com/mthevenin/analyse_duree/master/cox.zphold/cox.zphold.R\")\n\n\ncox.zphold(fit, transform=\"identity\")\n\n     rho chisq     p\nX -0.688  1.49 0.222"
  },
  {
    "objectID": "13-annexe.html#fragilité-et-immunité",
    "href": "13-annexe.html#fragilité-et-immunité",
    "title": "14  Annexes",
    "section": "14.2 Fragilité et immunité",
    "text": "14.2 Fragilité et immunité\nSeulement quelques remarques, le traitement de ces problématiques dépassant largement le contenu de la formation.\n\n14.2.1 Fragilité (Frailty)\nPour la fragilité, je conseille fortement de lire la dernière section du document de travail de Simon Quantin (cf bibliographie), il n’y a pas meilleure présentation du problème que la sienne ^[petite maj par rapport à la version précédente: il ne traite que la fragilité individuelle stricto sensu et non la fragilité plus connu sous le terme de shared frailty* (proche modèle multiniveau). Problèmatique importante, car une des origines de la non proportionnalité des risques réside dans l’omission de variables. Ici on va être confronté une omission sur des traits non observables ou latents, qui accélèrent dès le début de la période d’exposition la survenue de l’évènement. L’introduction d’un facteur de fragilité se fait par l’introduction d’un effet aléatoire dans le modèle, de nature plus complexe, et rendant l’interprétation des modèles plus compliquée.\nOn peut distinguer deux types de modèles:\n\nles modèles à fragilité partagée, c’est la situation la plus simple car la logique se rapproche des modèles multiniveaux, des groupes d’individus, identifiables, partagent une même fragilité, par exemple géographique.\nles modèles à fragilité non partagée, avec des caractéristiques latentes non observable comme les préférences, ou en médecine certains traits génétiques non identifiés.\n\n\n\n14.2.2 Immunité (Cure fraction)\nLe phénomène d’immunité est un cas particulier du précédent, et a été étudié dès le début des années 1950, en questionnant l’exposition au risque d’une partie des observations. On s’interrogeait par exemple sur les risques de rechute et de décès après le traitement d’un premier cancer. Visuellement on peut commencer à se proser des questions sur la présence d’une fraction immunisée ou non susceptible de connaître l’évènement lorsque la fonction de séjour ne tend pas vers 0 mais présente une longue asymptote (plateau) sur une valeur supérieure à 0: \\(\\lim_{t \\to \\infty}S(t)=a\\).\nLes modèles avec une fraction immunisée peuvent être de type mixte en associant une probabilité d’être immunisé aux observations censurées à droite à un modèle de durée 1. Plus dans le vent je crois, on a également des modèles de type non mixte, avec il me semble une connotation bayesienne qui semble s’accroître. Il n’y a donc pas de méthode unifiée à ce jour [Si vous voulez vous en convaincre].\nOn peut également noter, c’est important, que cette problématique affecte les analyses avec des évènements dits récurrents. Ici, la stratégie classique qui consiste à introduire dans un modèle un simple effet aléatoire de type fragilité partagée (shared frailty) pour contrôler risque d’être insuffisante. Ici le groupe est constitué de chaque séquence de remise dans le risque set. Exemple pour la fécondité: une personne ayant eu un enfant est exposée au risque d’en avoir un autre, l’horloge temporelle étant alors simplement réinitialisée. Et donc, quid des préférences individuelles en terme de fécondité 2.\nEnfin, les modèles à fragilité ou à fraction immunisée repose tous sur une hypothèse très forte. La fragilité ou le degré d’immunité est toujours défini (estimé) en début d’exposition, et il ne varie pas. Cela peut ne pas toujours faire sens, en particulier pour les préférences, pas forcément stables ou fixes dans le temps."
  },
  {
    "objectID": "13-annexe.html#exemple-danalyse-de-durées-canada-dry",
    "href": "13-annexe.html#exemple-danalyse-de-durées-canada-dry",
    "title": "14  Annexes",
    "section": "14.3 Exemple d’analyse de durées Canada Dry",
    "text": "14.3 Exemple d’analyse de durées Canada Dry\nEn remettant en cause l’utilisation abusive de concepts empruntés à l’analyse de survie dans une étude publiée récemment, ce qui suit peut paraître un peu polémique. En revanche il est important de préciser que les résultats obtenus avec une modélisation assez classique, ne sont pas, à quelques détails près, remis en cause. Il s’agit seulement de contester l’utilisation d’une terminologie très typée et conditionnée au respect de certaines hypothèses de base.\nCette étude [Lien] mobilise des données de suivi, à savoir l’EDP (Echantillon Démographique Permanent). L’étude cherche à mettre à profit l’ajout par l’Insee de données fiscales à cet échantillon pour analyser la fécondité des femmes nullipares au sein des couples. L’inclusion de cette données fiscale est initiée pour l’année 2011.\nSélection initiale de l’échantillon sur la première année d’inclusion En sélectionnant l’échantillon de départ sur 2011, année d’ajout d’une nouvelle information, aucune durée d’exposition ne peut être clairement définie pour cette première année. Ces le béaba des analyses de durée/survie, une origine commune doit être clairement définie ce qui ne peut pas être ici. Cette origine commune permet lors du suivi de construire une population initiale soumise au risque sur la même ligne de départ3. Il n’y avait pas d’autre choix compte tenu de la problématique de prendre l’année de mise en couple (cohabitant) pour définir le point d’origine, et donc demarrer l’observation à l’année 2012. Cela aurait permis en comparant les données fiscales d’une année sur l’autre de repérer les femmes qui se sont mis en couple. Et c’est ce qui est fait pour les inclusions sur les années suivantes.\nOn peut noter que ce biais d’inclusion joue de manière différenciée selon l’âge de la femme en 2011: surement nul ou très faible pour les femmes les plus jeunes, et selon le rang de l’union plus ou moins élevé pour les plus âgés. Sur ce dernier point le rang de l’union qui n’est peut-être pas mesurable avec les données utilisées, constitue surement une dimension de contrôle importante.\nSélection les années suivantes (jusqu’en 2017)\nComme cela a été noter juste au-dessus, les inclusions suivantes se font sur la base d’une origine commune à savoir l’année de mise en couple.\nAu final, on se retrouve donc avec un échantillon d’inclusion fait sur des critères différents: inclusions des nouvelles données pour 2011, et sur les mises en couples pour les années suivantes.\nUne fausse variable de durée En conservant la mise en couple comme origine, et en analysant la fécondité des femmes nullipares sur les 5 premières années, une analyse de survie reposant sur un suivi sur les 5 premières années de vie en couple cohabitant était facilement réalisable. L’autrice, je pense en allant piocher maladroitement et partiellement dans un ouvrage de méthodologie très connu (voir encadré ci-dessous), a été conduit à créer une pseudo variable de durée dont la longueur est artificielle et qui ne permet pas de mesurer proprement les durées d’exposition. Elle a en effet retrancher au âges observés durant les années de suivi l’âge de 18 ans.\nPour l’année 2011 et pour des âges d’inclusion allant de 18 à 26 ans, la construction de la durée est donc:\n\n\n\n\n\nAu final, et toujours sous l’angle revendiqué d’une analyse des durée ou de survie, aucune fonction de séjour sous jacente n’est estimable, et donc par définition aucune interprétation en termes de risques instantané n’est admissible. Sur le modèle, la construction de la vraisemblance est incompatible avec ce type d’analyse. Mais malheureusement on peut lire:\n\n“des modèles de risque instantané à temps discret” [page 419].\nEn note de bas de page: Le risque instantané est la probabilité conditionnelle que l’événement survienne entre le moment t et t + Δt, sachant qu’il n’a pas eu lieu avant t [page 419].\n\nCette définition est très légère. Le risque instantané est lié à la fonction de survie comme cela classiquement. Mais aucune fonction de séjour n’est ici estimable dans cet article, sachant que la survie est une probabilité. C’est la relation fondamentale qui lie risque (hazard rate), survie et densité qui permet de construire la vraisemblance d’un modèle de durée/survie. Cette relation est incontournable et elle n’est pas respectée ici.\n\n“Le deuxième modèle peut se passer de l’hypothèse des risques instantanés proportionnels” [page 420].\n\nComment peut on soulever l’hypothèse de proportionnalité lorsque les individus ne sont pas observé à des durées identique????\nDans l’article cette correction est faite également sur les variables dynamique. Il s’agit d’un emprunt à l’ouvrage de Singer et Willet. L’autrice n’en ait pas responsable, mais l’existence même de cette hypothèse très contestable avec des covariables non fixes4.\n\n“Le quatrième modèle enfin inclut l’interaction des revenus relatifs avec le temps d’exposition au risque” [page 421].\n\nIl n’y a malheureusement pas de temps d’exposition au risque clairement défini dans l’étude. On peut néanmoins remarquer que pour les couples dont la femme est très jeune (18-20 ans), l’inclusion en 2011 pourrait correspondre à l’année de mis en couple.\nVoir le point précédent pour l’introduction de cette intéraction .\n\nDans le même ordre d’idée: “*La spécification quadratique de la durée d’exposition indique une augmentation initiale de la fonction de risque…” [page 423]. Il n’y a malheureusement pas de fonction de risque estimable au sens de l’analyse de la survie.\n“L’équation suivante représente le calcul du risque continu cumulé” [pages 421], alors qu’aucune fonction de survie, et donc de risque cumulé ne peut être obtenu (je sais je me répète), mais cela permet à l’autrice de justifier l’utilisation de la fonction de lien complémentaire log-log. On pourra se reporter au cours de German Rodriguez pour une démonstration rigoureuse (lien en début de document).\n\nAu final:\nAprès de brefs échanges avec la rédaction en chef de l’Ined5, il a été convenu qu’il ne s’agissait pas en effet d’une analyse de durée ou de survie….Ouf!…. Mais que l’autrice, via une réponse peu inspirée d’un spécialiste de l’Ined sur les questions de fécondité, n’avait pas souhaité faire une analyse de ce type 6… Ha bon?\n\n\n\n\n\n\nL’ouvrage de Singer et Willet\n\n\n\nClairement revendiqué dans l’article, la stratégie méthodologique repose sur une section du fameux ouvrage de J.Singer et J.Willet Applied longitudinal data analysis: Modeling change and event occurrence. Considéré comme une véritable bible des modèles à durée discrètes, il a reposé plusieurs années dans mon bureau et donc été utilisé régulièrement. Il n’en reste pas moins exempt de défauts, avec un chapitrage peu conventionnel, un côté supermarché à modèles excessif, mais également des partis pris méthodologiques non discutés comme le test et la correction contestable de la non proportionnalité des risques pour des variables dynamiques.\nIl n’en reste pas moins, qu’à l’exception du dernier chapitre dédié aux temps continu, toutes la méthodologie et les applications sont réalisés dans le cadre stricte de la censure à droite non informative, et les auteurs prennent clairement le soins d’alerter très clairement les lecteurs des problèmes posés par présence de censure ou de troncature à gauche avec les données prospectives ou de suvi de stock, tel que l’EDP:\n\npages 320 et 321: In what follows, we typically assume that all censoring occurs on the right. In section 15.6, however, we describe what you can do when your data set includes what are known as late entrants into the risk set. You are most likely to encounter late entrants if you study stock samples, age-heterogeneous groups of people who already occupy the initial state when data collection begins—for example, a random sample of adults who have yet to experience a depressive episode. Your plan is to follow everyone for a fixed period of time—say ten years— and record whether and when sample participants experience their first episode. Because each sample member was a different age when data collection began, however, the ten years you cover do not cover the same ten years of peoples’ lives: you follow the 20-year-olds until they are 30, the 21-year-olds until they are 31, the 22-year-olds until they are 32, and so on. For an outcome like depression onset, it makes little sense to clock “time” using chronological years (2009,2010, etc). Instead, you would like to clock “time” in terms of age, but you do not observe everyone during the identical set of ages.]\n\nOn trouvera également les programmes associés aux exemples de l’ouvrage [ici]. On trouvera dans l’ouvrage, mot pour mot, le modèle utilisé par l’autrice. Mais l’exemple utilisait des données retrospéctives avec une stricte censure à droite non informative7."
  },
  {
    "objectID": "13-annexe.html#footnotes",
    "href": "13-annexe.html#footnotes",
    "title": "14  Annexes",
    "section": "",
    "text": "Le plus classique utilise un algorithme Expectation Maximisation utilisé en imputation: on estime une probabilité d’être susceptible de connaitre l’évènement aux observations censurées à droite, qui intervient comme facteur de pondération dans le modèle de durée. Cette probabilité et le modèle de durée qui lui est associé est réévalué à chaque boucle de l’algorithme jusqu’à convergence. Le principale problème de cette méthode résite dans l’estimation de la variance, souvent effectué par bootstrap. Cette méthode à l’avantage d’être implémentable en durée discrète, bien qu’à ma connaissance aucun logiciel ne la propose (j’ai une commande Stata encore perfectible sous le coude). On trouve en revanche ce type d’estimation sous R, pour les modèles de Cox ou les modèles paramétriques dans le package smcure↩︎\nen situation de récurrence, toujours penser à remettre à jour les conditions initiales, par exemple pour la fécondite l’âge de la mère à la naissance de l’enfant pour les rang supérieur à 1↩︎\nCeci ayant été écrit pendant des mondiaux d’athlétisme, dont les épreuves de courses sont pas définition des épreuves de durée, on pourra dire ici que les faux départs sont généralisés et autorisés↩︎\nAvec une variable fixe une des méthodes de correction, via l’introduction d’une intéraction, consiste justement à la transformaer en variable dynamique↩︎\nNous étions 3 à avoir exprimé de l’étonnement sur le contenu de cet article↩︎\nArgument lunaire: l’autrice n’utilise par la durée comme variable dépendante mais comme une simple covariable. Alors 1) en présence de censure à droite, c’est quand même compliquer d’introduire la durée directement comme variable dépendante et 2) l’introduction de la durée ou d’une fonction de celle-ci comme covariable ou variable d’ajustement est une caractéristique commune à toute type de moèle de durée de tpye parmétrique. Sur les modèles usuels, seul le modèle de Cox figure comme exception↩︎\nOn pourrait également questionner cet article paru dans Population, sur la nature non informative des ruptures d’union par rapport à la fécondité↩︎"
  },
  {
    "objectID": "14-R.html#packages-et-fonctions",
    "href": "14-R.html#packages-et-fonctions",
    "title": "15  R",
    "section": "15.1 Packages et fonctions",
    "text": "15.1 Packages et fonctions\n\n\n\n\n\n\n\nAnalyse\nPackages - Fonctions\n\n\n\n\nNon paramétrique\n\ndiscsurv\n\nlifetable\ncontToDisc\n\nsurvival\n\nsurvfit\nsurvdif\n\nsurvRM2\n\nrmst2\n\n\n\n\nModèles à risques proportionnel\n\nsurvival\n\ncoxph\ncox.zph (v3) cox.zphold (récupération v2)\nsurvsplit\n\nbase et tydir\n\nuncount\nglm\n\n\n\n\nModèles paramétriques (ph ou aft)\n\nsurvival\n\nsurvreg\n\nflexsurv\n\nsurvreg\n\n\n\n\nRisques concurents\n\ncmprsk\n\ncuminc\n\nnnet\n\nmultinom\n\n\n\n\nAutres (graphiques - mise en forme)\n\nsurvminer\njtools\ngtsummary\n\n\n\n\nInstallation\nLes dernières versions de certains packages peuvent être installées via Github (ex: survminer). Pour les récupérer, passer par le package devtools.\n\n#install.packages(\"survival\")\n#install.packages(\"survminer\")\n#install.packages(\"flexsurv\")\n#install.packages(\"survRM2\")\n#install.packages(\"tidyr\")\n#install.packages(\"dplyr\")\n#install.packages(\"jtools\")\n#install.packages(\"gtools\")\n#install.packages(\"cmprsk\")\n#install.package(\"gtsummary\")\n#install.packages(\"muhaz\")\n#install.packages(\"nnet\")\n\nlibrary(survival)\nlibrary(survminer)\nlibrary(flexsurv)\nlibrary(survRM2)\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(jtools)\nlibrary(gtools)\nlibrary(cmprsk)\nlibrary(discSurv)\nlibrary(gtsummary)\nlibrary(muhaz)\nlibrary(nnet)"
  },
  {
    "objectID": "14-R.html#analyse-non-paramétrique",
    "href": "14-R.html#analyse-non-paramétrique",
    "title": "15  R",
    "section": "15.2 Analyse Non paramétrique",
    "text": "15.2 Analyse Non paramétrique\nChargement de la base transplantation\n\nlibrary(readr)\ntrans &lt;- read.csv(\"https://raw.githubusercontent.com/mthevenin/analyse_duree/master/bases/transplantation.csv\")\n\n\n15.2.1 Méthode actuarielle\nLa fonction disponible du paquet discsurv, lifetable, a des fonctionalités plutôt limitées. Si on peut depuis une MAJ récente définir des intervalles de durée, il n’y a toujours pas d’estimateurs les différents quantiles de la courbe de survie.\nLa programmation est rendue un peu compliquée pour pas grand chose. Je donne les codes pour info, sans plus de commentaires.\n\ntrans = as.data.frame(trans)\n\nFonction lifeTable\nIntervalle par defaut \\(dt=1\\)\n\nlt = lifeTable(dataShort=trans, timeColumn=\"stime\", eventColumn = \"died\")\n\nplot(lt, x = 1:dim(lt$Output)[1], y = lt$Output$S, xlab = \"Intervalles t = journalier\", ylab=\"S(t)\")\n\n\n\n\nS(t) méthode actuarielle avec discSurv (1)\n\n\n\n\nIntervalle \\(dt=30\\)\n\n# On définit un vecteur définissant les intervalles (il n'y avait pas plus simple????)\ndt &lt;- 1:ceiling(max(trans$stime)/30)*30\n\n# Base dis avec une nouvelle variable de durée =&gt; timeDisc \n\ndis &lt;- contToDisc(dataShort=trans, timeColumn=\"stime\", intervalLimits = dt )\n\nlt &lt;- lifeTable(dataShort=dis, timeColumn=\"timeDisc\", eventColumn = \"died\")\n\nplot(lt, x = 1:dim(lt$Output)[1], y = lt$Output$S, xlab = \"Intervalles dt = 30 jours\", ylab=\"S(t)\")\n\n\n\n\nMéthode actuarielle avec discSurv (2)\n\n\n\n\nSur les abscisses, ce sont les valeurs des intervalles qui sont reportés: 10=300 jours. Ce n’est vraiment pas terrible. Pour ce type d’estimateurs, il est préférable d’utiliser Sas ou Stata.\n\n\n15.2.2 Méthode Kaplan-Meier\nLe package survival est le principal outil d’analyse des durée. Le package survminer permet d’améliorer la présentation des graphiques.\nEstimation des fonctions de survie\nFonction survfit\n\n\n\nsyntaxe\n\nfit &lt;- survfit(Surv(time, status) ~ x, data = base)\n\n\nOn peut renseigner directement les variables permettant de calculer la durée et non la variable de durée elle-même. Cette méthode est utilisée lorsqu’on introduit une variable dynamique dans un modèle semi-paramétrique de Cox (coxph).\n\n\n\nSyntaxe\n\nfit &lt;- survfit(Surv(variable_start, variable_end, status) ~ x, data = nom_base)\n\n\nSans comparaison de groupes:\n\nfit &lt;- survfit(Surv(stime, died) ~ 1, data = trans)\n\nfit\n\nCall: survfit(formula = Surv(stime, died) ~ 1, data = trans)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 103     75    100      72     263\n\nsummary(fit)\n\nCall: survfit(formula = Surv(stime, died) ~ 1, data = trans)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    103       1    0.990 0.00966       0.9715        1.000\n    2    102       3    0.961 0.01904       0.9246        0.999\n    3     99       3    0.932 0.02480       0.8847        0.982\n    5     96       2    0.913 0.02782       0.8597        0.969\n    6     94       2    0.893 0.03043       0.8355        0.955\n    8     92       1    0.883 0.03161       0.8237        0.948\n    9     91       1    0.874 0.03272       0.8119        0.940\n   12     89       1    0.864 0.03379       0.8002        0.933\n   16     88       3    0.835 0.03667       0.7656        0.910\n   17     85       1    0.825 0.03753       0.7543        0.902\n   18     84       1    0.815 0.03835       0.7431        0.894\n   21     83       2    0.795 0.03986       0.7208        0.877\n   28     81       1    0.785 0.04056       0.7098        0.869\n   30     80       1    0.776 0.04122       0.6989        0.861\n   32     78       1    0.766 0.04188       0.6878        0.852\n   35     77       1    0.756 0.04250       0.6769        0.844\n   36     76       1    0.746 0.04308       0.6659        0.835\n   37     75       1    0.736 0.04364       0.6551        0.827\n   39     74       1    0.726 0.04417       0.6443        0.818\n   40     72       2    0.706 0.04519       0.6225        0.800\n   43     70       1    0.696 0.04565       0.6117        0.791\n   45     69       1    0.686 0.04609       0.6009        0.782\n   50     68       1    0.675 0.04650       0.5902        0.773\n   51     67       1    0.665 0.04689       0.5796        0.764\n   53     66       1    0.655 0.04725       0.5690        0.755\n   58     65       1    0.645 0.04759       0.5584        0.746\n   61     64       1    0.635 0.04790       0.5479        0.736\n   66     63       1    0.625 0.04819       0.5374        0.727\n   68     62       2    0.605 0.04870       0.5166        0.708\n   69     60       1    0.595 0.04892       0.5063        0.699\n   72     59       2    0.575 0.04929       0.4857        0.680\n   77     57       1    0.565 0.04945       0.4755        0.670\n   78     56       1    0.554 0.04958       0.4654        0.661\n   80     55       1    0.544 0.04970       0.4552        0.651\n   81     54       1    0.534 0.04979       0.4451        0.641\n   85     53       1    0.524 0.04986       0.4351        0.632\n   90     52       1    0.514 0.04991       0.4251        0.622\n   96     51       1    0.504 0.04994       0.4151        0.612\n  100     50       1    0.494 0.04995       0.4052        0.602\n  102     49       1    0.484 0.04993       0.3953        0.592\n  110     47       1    0.474 0.04992       0.3852        0.582\n  149     45       1    0.463 0.04991       0.3749        0.572\n  153     44       1    0.453 0.04987       0.3647        0.562\n  165     43       1    0.442 0.04981       0.3545        0.551\n  186     41       1    0.431 0.04975       0.3440        0.541\n  188     40       1    0.420 0.04966       0.3336        0.530\n  207     39       1    0.410 0.04954       0.3233        0.519\n  219     38       1    0.399 0.04940       0.3130        0.509\n  263     37       1    0.388 0.04923       0.3027        0.498\n  285     35       2    0.366 0.04885       0.2817        0.475\n  308     33       1    0.355 0.04861       0.2713        0.464\n  334     32       1    0.344 0.04834       0.2610        0.453\n  340     31       1    0.333 0.04804       0.2507        0.442\n  342     29       1    0.321 0.04773       0.2401        0.430\n  583     21       1    0.306 0.04785       0.2252        0.416\n  675     17       1    0.288 0.04830       0.2073        0.400\n  733     16       1    0.270 0.04852       0.1898        0.384\n  852     14       1    0.251 0.04873       0.1712        0.367\n  979     11       1    0.228 0.04934       0.1491        0.348\n  995     10       1    0.205 0.04939       0.1279        0.329\n 1032      9       1    0.182 0.04888       0.1078        0.308\n 1386      6       1    0.152 0.04928       0.0804        0.287\n\nplot(fit)\n\n\n\n\n\n\n\n\nLe premier output fit permet d’obtenir la durée médiane, ici égale à 100 (\\(S(100)=0.494\\)). Le second avec la fonction summary permet d’obtenir une table des estimateurs. La fonction de survie peut être tracée avec la fonction plot (en pointillés les intervalles de confiance).\nOn peut obtenir des graphes de meilleur qualité avec la librairie survminer, avec la fonction ggsurvplot\n\nggsurvplot(fit, conf.int = TRUE)\n\n\n\n\n\n\n\n\nOn peut ajouter la population encore soumise au risque à plusieurs points d’observation avec l’argument risk.table = TRUE\n\nggsurvplot(fit, conf.int = TRUE, risk.table = TRUE)\n\n\n\n\n\n\n\n\n\n\n15.2.3 Comparaison des S(t) méthode KM\nOn va comparer les deuxfonctions de survie pour la variable surgery, celle pour les personnes non opérées et celle pour les personnes opérées.\n\nfit &lt;- survfit(Surv(stime, died) ~ surgery, data = trans)\nfit\n\nCall: survfit(formula = Surv(stime, died) ~ surgery, data = trans)\n\n           n events median 0.95LCL 0.95UCL\nsurgery=0 91     69     78      61     153\nsurgery=1 12      6    979     583      NA\n\nggsurvplot(fit, conf.int = TRUE, risk.table = TRUE)\n\n\n\n\n\n\n\n\nTests du logrank\nOn utilise la fonction survdiff, avec comme variante le test de Peto-Peto (rho=1).\nLa syntaxe est quasiment identique à la fonction survdiff.\n\nsurvdiff(Surv(stime, died) ~ surgery, rho=1, data = trans)\n\nCall:\nsurvdiff(formula = Surv(stime, died) ~ surgery, data = trans, \n    rho = 1)\n\n           N Observed Expected (O-E)^2/E (O-E)^2/V\nsurgery=0 91    45.28    39.12     0.968      8.65\nsurgery=1 12     2.03     8.18     4.630      8.65\n\n Chisq= 8.7  on 1 degrees of freedom, p= 0.003 \n\n\nIci la variable est binaire. Si on veux tester deux à deux les niveaux d’une variable catégorielle à plus de deux modalités, il est fortement conseillé d’utiliser la fonction pairwise_survdiff de survminer (syntaxe identique que survdiff).\nComparaison des RMST\nLa fonction rmst2 du package survRM2 permet de comparer les RMST entre 2 groupes . La strate pour les comparaisons doit être impérativement renommée arm. La fonction, issue d’une commande de Stata, n’est pas très souple.\n\ntrans$arm=trans$surgery\na=rmst2(trans$stime, trans$died, trans$arm, tau=NULL)\nprint(a)\n\n\nThe truncation time, tau, was not specified. Thus, the default tau  1407  is used. \n\nRestricted Mean Survival Time (RMST) by arm \n                Est.      se lower .95 upper .95\nRMST (arm=1) 884.576 151.979   586.702  1182.450\nRMST (arm=0) 379.148  58.606   264.283   494.012\n\n\nRestricted Mean Time Lost (RMTL) by arm \n                 Est.      se lower .95 upper .95\nRMTL (arm=1)  522.424 151.979   224.550   820.298\nRMTL (arm=0) 1027.852  58.606   912.988  1142.717\n\n\nBetween-group contrast \n                        Est. lower .95 upper .95     p\nRMST (arm=1)-(arm=0) 505.428   186.175   824.682 0.002\nRMST (arm=1)/(arm=0)   2.333     1.483     3.670 0.000\nRMTL (arm=1)/(arm=0)   0.508     0.284     0.909 0.022\n\nplot(a)"
  },
  {
    "objectID": "14-R.html#modèle-de-cox",
    "href": "14-R.html#modèle-de-cox",
    "title": "15  R",
    "section": "15.3 Modèle de Cox",
    "text": "15.3 Modèle de Cox\nIci tout est estimé de nouveau avec des fonctions du package survival:\n\nEstimation du modèle: coxph.\nTest de Grambsch-Therneau: cox.zph et cox.oldzph.\nIntroduction d’une variable dynamique: allongement de la base avec survsplit.\n\n\n15.3.1 Estimation du modèle\nPar défaut, R utilise la correction d’Efron pour les évènements simultanés. Il est préférable de ne pas la modifier.\nSyntaxe:\n\n\n\nSyntaxe\n\ncoxph(Surv(time, status) ~ x1 + x2 + ....., data=base, ties=\"nom_correction\"))\n\n\n\ncoxfit = coxph(formula = Surv(stime, died) ~ year + age + surgery, data = trans)\nsummary(coxfit)\n\nCall:\ncoxph(formula = Surv(stime, died) ~ year + age + surgery, data = trans)\n\n  n= 103, number of events= 75 \n\n            coef exp(coef) se(coef)      z Pr(&gt;|z|)\nyear    -0.11963   0.88725  0.06734 -1.776   0.0757\nage      0.02958   1.03002  0.01352  2.187   0.0287\nsurgery -0.98732   0.37257  0.43626 -2.263   0.0236\n\n        exp(coef) exp(-coef) lower .95 upper .95\nyear       0.8872     1.1271    0.7775    1.0124\nage        1.0300     0.9709    1.0031    1.0577\nsurgery    0.3726     2.6840    0.1584    0.8761\n\nConcordance= 0.653  (se = 0.032 )\nLikelihood ratio test= 17.63  on 3 df,   p=5e-04\nWald test            = 15.76  on 3 df,   p=0.001\nScore (logrank) test = 16.71  on 3 df,   p=8e-04\n\ntbl_regression(coxfit, exponentiate = TRUE,)\n\nRegistered S3 method overwritten by 'sass':\n  method    from  \n  print.css memisc\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      HR1\n      95% CI1\n      p-value\n    \n  \n  \n    year\n0.89\n0.78, 1.01\n0.076\n    age\n1.03\n1.00, 1.06\n0.029\n    surgery\n0.37\n0.16, 0.88\n0.024\n  \n  \n  \n    \n      1 HR = Hazard Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nL’output des résultats reporte le logarithme des Risques Ratios (coef) ainsi que les RR (exp(coef)). Il est intéressant de regarder la valeur de concordance (Harrel’s) qui donne des indications sur la qualité de l’ajustement (proche de l’AUC/ROC d’un modèle probabiliste standard).\nOn peut représenter sous forme graphique les résultats avec la fonction ggforest de survminer\n\nggforest(coxfit)\n\nWarning in .get_data(model, data = data): The `data` argument is not provided.\nData will be extracted from model fit.\n\n\n\n\n\n\n\n\n\n\n\n15.3.2 Hypothèse PH\n\n15.3.2.1 Test Grambsch-Therneau\nRésidus de Schoenfeld\nOn utilise la fonction cox.zph pour effectuer le test GLS (moindre carrés généralisés) qui a été substitué au test OLS (moindres carrés ordinaires) avec le passage à la v3 du package. Je donne plus loin un moyen de récupérer et d’exécuter le test OLS, que je conseille d’utiliser en présence de durées discrètes/groupées.\nLe test peut utiliser plusieurs fonctions de la durée. Par défaut la fonction utilise \\(1-KM\\), soit le complémentaire de l’estimateur de Kaplan-Meier (option transform=\"km\").\n\nTest GLS (V3 de survival)\n\nAvec transform=\"km\"\n\ncox.zph(coxfit)\n\n        chisq df     p\nyear    3.309  1 0.069\nage     0.922  1 0.337\nsurgery 5.494  1 0.019\nGLOBAL  8.581  3 0.035\n\n\nAvec transform=\"identity\" (\\(f(t)=t\\))\n\ncox.zph(coxfit, transform=\"identity\")\n\n        chisq df     p\nyear     4.54  1 0.033\nage      1.71  1 0.191\nsurgery  4.92  1 0.027\nGLOBAL   9.47  3 0.024\n\n\nRemarque: avec la v3 de survival, quelques options ont été ajoutées tel que terms qui permet pour une variable catégorielle à plus de deux modalités de choisir entre un sous test multiple sur la variable (k modalités =&gt; k-1 degré de liberté) et une série de tests à 1 degré de liberté sur chaque modalité (k-1 tests). De mon point de vue préférer la seconde solution avec terms=FALSE. le test de Grambsch-Therneau est particulièrement sensible au nombre de degré de liberté, et il convient donc d’éviter de l’utiliser dans un cadre multiple.\n\nTest OLS (V2 de survival - Stata - Sas - Python)\n\n\n\n\nRécupération du test ols\n\nsource(\"https://raw.githubusercontent.com/mthevenin/analyse_duree/main/cox.zphold/cox.zphold.R\")\n\n\n\n\n\nExécution du test ols\n\ncox.zphold(coxfit, transform=\"identity\")\n\n\n          rho chisq      p\nyear    0.102 0.797 0.3720\nage     0.129 1.612 0.2043\nsurgery 0.297 5.539 0.0186\nGLOBAL     NA 8.756 0.0327\n\n\n\n\n15.3.2.2 Introduction d’une intéraction\nLorsque la covariable n’est pas continue, elle doit être impérativement transformée en indicatrice 1. Penser à vérifier en amont que les résultats du modèle sont bien identiques avec le modèle estimé précédemment (ne pas oublier d’omettre le niveau en référence).\nLa variable d’intéraction est tt(nom_variable), la fonction de la durée (ici forme linéaire simple) est indiquée en option de la fonction: tt = function(x, t, ...) x*t.\n\ncoxfit2 = coxph(formula = Surv(stime, died) ~ year + age + surgery + tt(surgery), data = trans, tt = function(x, t, ...) x*t)\n\nsummary(coxfit2)\n\nCall:\ncoxph(formula = Surv(stime, died) ~ year + age + surgery + tt(surgery), \n    data = trans, tt = function(x, t, ...) x * t)\n\n  n= 103, number of events= 75 \n\n                 coef exp(coef)  se(coef)      z Pr(&gt;|z|)\nyear        -0.123074  0.884198  0.066835 -1.841  0.06555\nage          0.028888  1.029310  0.013449  2.148  0.03172\nsurgery     -1.754738  0.172953  0.674391 -2.602  0.00927\ntt(surgery)  0.002231  1.002234  0.001102  2.024  0.04299\n\n            exp(coef) exp(-coef) lower .95 upper .95\nyear           0.8842     1.1310   0.77564    1.0080\nage            1.0293     0.9715   1.00253    1.0568\nsurgery        0.1730     5.7819   0.04612    0.6486\ntt(surgery)    1.0022     0.9978   1.00007    1.0044\n\nConcordance= 0.656  (se = 0.032 )\nLikelihood ratio test= 21.58  on 4 df,   p=2e-04\nWald test            = 16.99  on 4 df,   p=0.002\nScore (logrank) test = 19  on 4 df,   p=8e-04\n\ntbl_regression(coxfit2, exponentiate = TRUE, estimate_fun = purrr::partial(style_ratio, digits = 3))\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      HR1\n      95% CI1\n      p-value\n    \n  \n  \n    year\n0.884\n0.776, 1.008\n0.066\n    age\n1.029\n1.003, 1.057\n0.032\n    surgery\n0.173\n0.046, 0.649\n0.009\n    tt(surgery)\n1.002\n1.000, 1.004\n0.043\n  \n  \n  \n    \n      1 HR = Hazard Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\nRappel: le paramètre estimé pour tt(surgery) ne reporte pas un rapport de risques, mais un rapport de de deux rapports de risques. C’est bien une double différence sur l’échelle d’estimation (log).\n\n\n\n15.3.3 Introduction d’une variable dynamique (binaire)\nLa dimension dynamique est ici le fait d’avoir été opéré pour une greffe du coeur.\n\nEtape 1: créer un vecteur donnant les durées aux temps d’évènement.\nEtape 2: appliquer ce vecteurs de points de coupure à la fonction survsplit.\nEtape 3: modifier la variable transplant (ou créer une nouvelle) à l’aide de la variable wait qui prend la valeur 1 à partir du jour de la greffe, 0 avant.\nEtape 1: création de l’objet cut (vecteur), qui récupère les moments où au moins un évènement est observé.\n\n\ncut= unique(trans$stime[trans$died == 1])\n\ncut\n\n [1]    1    2    3    5    6    8    9   12   16   17   18   21   28   30   32\n[16]   35   36   37   39   40   43   45   50   51   53   58   61   66   68   69\n[31]   72   77   78   80   81   85   90   96  100  102  110  149  153  165  186\n[46]  188  207  219  263  285  308  334  340  342  583  675  733  852  979  995\n[61] 1032 1386\n\n\nEtape 2: allonger la base aux durées d’évènement\n\ntvc = survSplit(data = trans, cut = cut, end = \"stime\", start = \"stime0\", event = \"died\")\n\nhead(tvc, n=20 )\n\n   id year age surgery transplant wait mois compet arm stime0 stime died\n1  15   68  53       0          0    0    1      1   0      0     1    1\n2  43   70  43       0          0    0    1      1   0      0     1    0\n3  43   70  43       0          0    0    1      1   0      1     2    1\n4  61   71  52       0          0    0    1      1   0      0     1    0\n5  61   71  52       0          0    0    1      1   0      1     2    1\n6  75   72  52       0          0    0    1      1   0      0     1    0\n7  75   72  52       0          0    0    1      1   0      1     2    1\n8   6   68  54       0          0    0    1      2   0      0     1    0\n9   6   68  54       0          0    0    1      2   0      1     2    0\n10  6   68  54       0          0    0    1      2   0      2     3    1\n11 42   70  36       0          0    0    1      1   0      0     1    0\n12 42   70  36       0          0    0    1      1   0      1     2    0\n13 42   70  36       0          0    0    1      1   0      2     3    1\n14 54   71  47       0          0    0    1      1   0      0     1    0\n15 54   71  47       0          0    0    1      1   0      1     2    0\n16 54   71  47       0          0    0    1      1   0      2     3    1\n17 38   70  41       0          1    5    1      1   0      0     1    0\n18 38   70  41       0          1    5    1      1   0      1     2    0\n19 38   70  41       0          1    5    1      1   0      2     3    0\n20 38   70  41       0          1    5    1      1   0      3     5    1\n\n\nOn vérifie qu’on obtient les même résultats avec le modèle sans tvc\n\ncoxph(formula = Surv(stime0, stime, died) ~ year + age + surgery, data = tvc)\n\nCall:\ncoxph(formula = Surv(stime0, stime, died) ~ year + age + surgery, \n    data = tvc)\n\n            coef exp(coef) se(coef)      z      p\nyear    -0.11963   0.88725  0.06734 -1.776 0.0757\nage      0.02958   1.03002  0.01352  2.187 0.0287\nsurgery -0.98732   0.37257  0.43626 -2.263 0.0236\n\nLikelihood ratio test=17.63  on 3 df, p=0.0005243\nn= 3573, number of events= 75 \n\n\n\nEtape 3: on génère la variable dynamique de sorte que les personnes n’apparaissent pas greffés avant l’opération\n\n\ntvc$tvc=ifelse(tvc$transplant==1 & tvc$wait&lt;=tvc$stime,1,0)\n\nEstimation du modèle\nEn format long, on doit préciser dans la formule l’intervalle de durée avec les variables stime0 (début) et stime(fin)\n\ntvcfit = coxph(formula = Surv(stime0, stime, died) ~ year + age + surgery + tvc, data = tvc)\n\nsummary(tvcfit)\n\nCall:\ncoxph(formula = Surv(stime0, stime, died) ~ year + age + surgery + \n    tvc, data = tvc)\n\n  n= 3573, number of events= 75 \n\n            coef exp(coef) se(coef)      z Pr(&gt;|z|)\nyear    -0.12032   0.88664  0.06734 -1.787   0.0740\nage      0.03044   1.03091  0.01390  2.190   0.0285\nsurgery -0.98289   0.37423  0.43655 -2.251   0.0244\ntvc     -0.08221   0.92108  0.30484 -0.270   0.7874\n\n        exp(coef) exp(-coef) lower .95 upper .95\nyear       0.8866      1.128    0.7770    1.0117\nage        1.0309      0.970    1.0032    1.0594\nsurgery    0.3742      2.672    0.1591    0.8805\ntvc        0.9211      1.086    0.5068    1.6741\n\nConcordance= 0.659  (se = 0.032 )\nLikelihood ratio test= 17.7  on 4 df,   p=0.001\nWald test            = 15.79  on 4 df,   p=0.003\nScore (logrank) test = 16.74  on 4 df,   p=0.002\n\ntbl_regression(tvcfit, exponentiate = TRUE, estimate_fun = purrr::partial(style_ratio, digits = 3))\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      HR1\n      95% CI1\n      p-value\n    \n  \n  \n    year\n0.887\n0.777, 1.012\n0.074\n    age\n1.031\n1.003, 1.059\n0.029\n    surgery\n0.374\n0.159, 0.880\n0.024\n    tvc\n0.921\n0.507, 1.674\n0.8\n  \n  \n  \n    \n      1 HR = Hazard Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\n\nggforest(tvcfit)\n\nWarning in .get_data(model, data = data): The `data` argument is not provided.\nData will be extracted from model fit."
  },
  {
    "objectID": "14-R.html#analyse-en-durée-discrète",
    "href": "14-R.html#analyse-en-durée-discrète",
    "title": "15  R",
    "section": "15.4 Analyse en durée discrète",
    "text": "15.4 Analyse en durée discrète\nPour la durée, on va utiliser la variable mois (regroupement sur 30 jours).\nLa fonction uncount du package tidyr permettra de splitter la base aux durées d’observation. C’est ici la principale différence avec le modèle de Cox qui est une estimation aux durées d’évènement\n\ntrans &lt;- read.csv(\"https://raw.githubusercontent.com/mthevenin/analyse_duree/master/bases/transplantation.csv\")\n\nLa variable mois, va être supprimée avec uncount. Comme on en aura besoin plus loin pour générer proprement la variable évènement, on peut créer ici une variable mirroir.\n\ntrans$T = trans$mois\n\n\ndt = uncount(trans,mois)\ndt = dt[order(dt$id),]\n\n\nhead(dt,11) \n\n    id year age died stime surgery transplant wait compet  T\n48   1   67  30    1    50       0          0    0      1  2\n49   1   67  30    1    50       0          0    0      1  2\n10   2   68  51    1     6       0          0    0      1  1\n18   3   68  54    1    16       0          1    1      1  1\n36   4   68  40    1    39       0          1   36      2  2\n37   4   68  40    1    39       0          1   36      2  2\n20   5   68  20    1    18       0          0    0      1  1\n5    6   68  54    1     3       0          0    0      2  1\n466  7   68  50    1   675       0          1   51      1 23\n467  7   68  50    1   675       0          1   51      1 23\n468  7   68  50    1   675       0          1   51      1 23\n\n\nOn va générer une variable type compteur pour mesurer la durée à chaque point d’observation.\n\ndt$x=1\ndt$t = ave(dt$x,dt$id, FUN=cumsum)\n\nhead(dt, n=8)\n\n   id year age died stime surgery transplant wait compet T x t\n48  1   67  30    1    50       0          0    0      1 2 1 1\n49  1   67  30    1    50       0          0    0      1 2 1 2\n10  2   68  51    1     6       0          0    0      1 1 1 1\n18  3   68  54    1    16       0          1    1      1 1 1 1\n36  4   68  40    1    39       0          1   36      2 2 1 1\n37  4   68  40    1    39       0          1   36      2 2 1 2\n20  5   68  20    1    18       0          0    0      1 1 1 1\n5   6   68  54    1     3       0          0    0      2 1 1 1\n\n\nSi un individu est décédé, died=1 est reporté sur toute les lignes (idem qu’avec la variable dynamique). On va modifier la variable tel que died=0 si t&lt;T$.\n\ndt = arrange(dt,id,t)\n\ndt$died[dt$t&lt;dt$T]=0\n\nhead(dt, n=8)\n\n  id year age died stime surgery transplant wait compet T x t\n1  1   67  30    0    50       0          0    0      1 2 1 1\n2  1   67  30    1    50       0          0    0      1 2 1 2\n3  2   68  51    1     6       0          0    0      1 1 1 1\n4  3   68  54    1    16       0          1    1      1 1 1 1\n5  4   68  40    0    39       0          1   36      2 2 1 1\n6  4   68  40    1    39       0          1   36      2 2 1 2\n7  5   68  20    1    18       0          0    0      1 1 1 1\n8  6   68  54    1     3       0          0    0      2 1 1 1\n\n\n\n15.4.1 \\(f(t)\\) quantitative\nAvec un effet quadratique d’ordre 3 ^[Attention ici cela marche bien. Bien vérifier qu’il n’y a pas un problème d’overfitting, comme c’est le cas dans le TP.\nOn centre également les variables year et age sur leur valeur moyenne pour donner un sens à la constante\n\ndt$t2=dt$t^2\ndt$t3=dt$t^3\n\nmy = mean(dt$year)\ndt$yearb = dt$year - my\nma = mean(dt$age)\ndt$ageb = dt$age  - ma\n\n\ndtfit = glm(died ~ t + t2 + t3 + yearb + ageb + surgery, data=dt, family=\"binomial\")\nsumm(dtfit, confint=TRUE, exp=TRUE)\n\n\n\n\n\nObservations\n1127\n\n\nDependent variable\ndied\n\n\nType\nGeneralized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n\n\n\n\n\nχ²(6)\n90.69\n\n\nPseudo-R² (Cragg-Uhler)\n0.20\n\n\nPseudo-R² (McFadden)\n0.16\n\n\nAIC\n474.67\n\n\nBIC\n509.86\n\n\n\n\n\n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nz val.\np\n\n\n\n\n(Intercept)\n0.44\n0.27\n0.72\n-3.29\n0.00\n\n\nt\n0.69\n0.59\n0.81\n-4.52\n0.00\n\n\nt2\n1.01\n1.00\n1.02\n2.83\n0.00\n\n\nt3\n1.00\n1.00\n1.00\n-2.11\n0.03\n\n\nyearb\n0.88\n0.76\n1.01\n-1.80\n0.07\n\n\nageb\n1.03\n1.00\n1.06\n2.27\n0.02\n\n\nsurgery\n0.36\n0.15\n0.88\n-2.25\n0.02\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n\ntbl_regression(dtfit, exponentiate = TRUE, estimate_fun = purrr::partial(style_ratio, digits = 3))\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    t\n0.689\n0.582, 0.805\n&lt;0.001\n    t2\n1.014\n1.005, 1.025\n0.005\n    t3\n1.000\n1.000, 1.000\n0.035\n    yearb\n0.876\n0.756, 1.011\n0.072\n    ageb\n1.034\n1.006, 1.066\n0.023\n    surgery\n0.364\n0.136, 0.815\n0.024\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\n\n\n15.4.2 \\(f(t)\\) en indicatrices \nOn va créer une variable de type dicrète regroupant la variable t sur ses quartiles (pour l’exemple seulement, tous types de regroupement est envisageable).\nOn va utiliser la fonction quantcut du package gtools.\n\ndt$ct4 &lt;- quantcut(dt$t)\ntable(dt$ct4) \n\n\n  [1,4]  (4,11] (11,23] (23,60] \n    299     275     282     271 \n\n\nOn va générer un compteur et un total d’observations sur la strate regroupant id et ct4.\n\ndt$n = ave(dt$x,dt$id, dt$ct4, FUN=cumsum)\ndt$N = ave(dt$x,dt$id, dt$ct4, FUN=sum)\n\nOn conserve la dernière observation dans la strate.\n\ndt2 = subset(dt, n==N)\n\nEstimation du modèle\n\nfit = glm(died ~ ct4 + yearb + ageb + surgery, data=dt2, family=binomial)\nsumm(fit, confint=TRUE, exp=TRUE)\n\n\n\n\n\nObservations\n197\n\n\nDependent variable\ndied\n\n\nType\nGeneralized linear model\n\n\nFamily\nbinomial\n\n\nLink\nlogit\n\n\n\n\n\n\n\n\nχ²(6)\n39.30\n\n\nPseudo-R² (Cragg-Uhler)\n0.25\n\n\nPseudo-R² (McFadden)\n0.15\n\n\nAIC\n236.48\n\n\nBIC\n259.46\n\n\n\n\n\n\n\n\n\nexp(Est.)\n2.5%\n97.5%\nz val.\np\n\n\n\n\n(Intercept)\n1.17\n0.77\n1.79\n0.73\n0.47\n\n\nct4(4,11]\n0.36\n0.16\n0.81\n-2.47\n0.01\n\n\nct4(11,23]\n0.20\n0.07\n0.58\n-2.96\n0.00\n\n\nct4(23,60]\n0.62\n0.19\n2.01\n-0.80\n0.42\n\n\nyearb\n0.82\n0.68\n0.98\n-2.18\n0.03\n\n\nageb\n1.05\n1.01\n1.09\n2.53\n0.01\n\n\nsurgery\n0.33\n0.12\n0.88\n-2.21\n0.03\n\n\n\n Standard errors: MLE\n\n\n\n\n\n\n\n\n\n\n\ntbl_regression(fit, exponentiate = TRUE, estimate_fun = purrr::partial(style_ratio, digits = 3))\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    ct4\n\n\n\n        [1,4]\n—\n—\n\n        (4,11]\n0.356\n0.152, 0.792\n0.014\n        (11,23]\n0.199\n0.061, 0.541\n0.003\n        (23,60]\n0.619\n0.183, 1.981\n0.4\n    yearb\n0.816\n0.677, 0.977\n0.029\n    ageb\n1.048\n1.012, 1.089\n0.011\n    surgery\n0.330\n0.113, 0.837\n0.027\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "14-R.html#modèles-paramétriques-usuels",
    "href": "14-R.html#modèles-paramétriques-usuels",
    "title": "15  R",
    "section": "15.5 Modèles paramétriques usuels",
    "text": "15.5 Modèles paramétriques usuels\nPour le modèle de Weibull par exemple.\n\nDe type AFT\n\nOn utilise la fonction survreg du package survival\n\nweibull = survreg(formula = Surv(stime, died) ~ year + age + surgery, data = trans, dist=\"weibull\")\nsummary(weibull)\n\n\nCall:\nsurvreg(formula = Surv(stime, died) ~ year + age + surgery, data = trans, \n    dist = \"weibull\")\n              Value Std. Error     z       p\n(Intercept) -3.0220     8.7284 -0.35   0.729\nyear         0.1620     0.1218  1.33   0.184\nage         -0.0615     0.0247 -2.49   0.013\nsurgery      1.9703     0.7794  2.53   0.011\nLog(scale)   0.5868     0.0927  6.33 2.5e-10\n\nScale= 1.8 \n\nWeibull distribution\nLoglik(model)= -488.2   Loglik(intercept only)= -497.6\n    Chisq= 18.87 on 3 degrees of freedom, p= 0.00029 \nNumber of Newton-Raphson Iterations: 5 \nn= 103 \n\ntbl_regression(weibull, exponentiate = TRUE, estimate_fun = purrr::partial(style_ratio, digits = 3))\n\nWarning: The `exponentiate` argument is not supported in the `tidy()` method\nfor `survreg` objects and will be ignored.\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      exp(Beta)\n      95% CI1\n      p-value\n    \n  \n  \n    year\n0.162\n-0.077, 0.401\n0.2\n    age\n-0.062\n-0.110, -0.013\n0.013\n    surgery\n1.970\n0.443, 3.498\n0.011\n  \n  \n  \n    \n      1 CI = Confidence Interval\n    \n  \n\n\n\n\n\nDe type PH\n\nLa paramétrisation PH n’est pas possible avec la fonction survreg. Il faut utiliser le package flexsurv, qui permet également d’estimer les modèles paramétriques disponibles avec survival. La syntaxe est quasiment identique.\nPour estimer le modèle de Weibull de type PH, on utilise en option l’agument dist=\"weibullPH.\n\nweibullph = flexsurvreg(formula = Surv(stime, died) ~ year + age + surgery, data = trans, dist=\"weibullPH\")\nweibullph\n\nCall:\nflexsurvreg(formula = Surv(stime, died) ~ year + age + surgery, \n    data = trans, dist = \"weibullPH\")\n\nEstimates: \n         data mean  est        L95%       U95%       se         exp(est) \nshape           NA   5.56e-01   4.64e-01   6.67e-01   5.16e-02         NA\nscale           NA   5.37e+00   4.27e-04   6.75e+04   2.59e+01         NA\nyear      7.06e+01  -9.01e-02  -2.20e-01   3.97e-02   6.62e-02   9.14e-01\nage       4.46e+01   3.42e-02   7.13e-03   6.13e-02   1.38e-02   1.03e+00\nsurgery   1.17e-01  -1.10e+00  -1.95e+00  -2.45e-01   4.34e-01   3.34e-01\n         L95%       U95%     \nshape           NA         NA\nscale           NA         NA\nyear      8.03e-01   1.04e+00\nage       1.01e+00   1.06e+00\nsurgery   1.43e-01   7.83e-01\n\nN = 103,  Events: 75,  Censored: 28\nTotal time at risk: 31938\nLog-likelihood = -488.1683, df = 5\nAIC = 986.3366"
  },
  {
    "objectID": "14-R.html#risques-concurrents",
    "href": "14-R.html#risques-concurrents",
    "title": "15  R",
    "section": "15.6 Risques concurrents",
    "text": "15.6 Risques concurrents\nLe package cmprsk pour l’analyse non paramétrique et le modèle de Fine-Gray (non traité).\nPackage cmprsk pour l’analyse non paramétrique et le modèle de Fine-Gray. La variable de censure/évènement, compet, correspond à la variable died avec une modalité supplémentaire simulée. On suppose l’existence d’une cause supplémentaire au décès autre qu’une malformation cardiaque et non strictement indépendante de cell-ci.\n\ncompet &lt;- read.csv(\"https://raw.githubusercontent.com/mthevenin/analyse_duree/master/bases/transplantation.csv\")\n# variable compet\ntable(compet$compet) \n\n\n 0  1  2 \n28 56 19 \n\n# variable died\ntable(compet$died) \n\n\n 0  1 \n28 75 \n\n\n\n15.6.0.1 Incidences cumulées\nOn utilise la fonction cuminc du package cmprsk.\nPas de comparaison de groupes\n\nic = cuminc(compet$stime, compet$compet)\nic \n\nEstimates and Variances:\n$est\n          500      1000      1500\n1 1 0.5067598 0.5808345 0.6340038\n1 2 0.1720161 0.2140841 0.2140841\n\n$var\n            500        1000        1500\n1 1 0.002619449 0.003131847 0.003676516\n1 2 0.001473283 0.002203770 0.002203770\n\nplot(ic)\n\n\n\n\n\n\n\n\nAvec survminer\n\nggcompetingrisks(fit = ic)\n\n\n\n\n\n\n\n\nComparaison de groupes\nLe test de Gray est automatiquement exécuté.\n\nic = cuminc(compet$stime, compet$compet, group=compet$surgery, rho=1)\nic \n\nTests:\n      stat         pv df\n1 4.604792 0.03188272  1\n2 0.272147 0.60189515  1\nEstimates and Variances:\n$est\n           500      1000      1500\n0 1 0.54917896 0.5940358 0.6604903\n1 1 0.18181818 0.4242424        NA\n0 2 0.18168014 0.2066006 0.2066006\n1 2 0.09090909 0.2121212        NA\n\n$var\n            500        1000        1500\n0 1 0.002955869 0.003335897 0.004199157\n1 1 0.014958678 0.033339569          NA\n0 2 0.001727112 0.002271242 0.002271242\n1 2 0.008449138 0.022024737          NA\n\nplot(ic)\n\n\n\n\n\n\n\n\nAvec survminer, pour obtenir un seul graphique pour toutes les courbes ajouter l’option multiple_panels = F\n\nggcompetingrisks(fit = ic)\n\n\n\n\n\n\n\nggcompetingrisks(fit = ic, multiple_panels = F)\n\n\n\n\n\n\n\n\n\n\n15.6.0.2 Modèles\nOn va utilisé seulement le modèle multinomial à durée discrète, le modèle fine-gray pendant du modèle de Cox pour les risques concurrents étant fortement critiqué. Si une analyse de type cause-specific est envisageable (issues concurrentes traitées comme des censures à droites) on utilise simplement la fonction coxph de survival.\nOn va de nouveau utiliser la variable mois (durée discrète). Le modèle sera estimé à l’aide la fonction multinom du très vieillissant package nnet, les p-values doivent-être programmées, l’output ne donnant que les erreurs-types.\nMise en formae de la base\n\ncompet &lt;- read.csv(\"https://raw.githubusercontent.com/mthevenin/analyse_duree/master/bases/transplantation.csv\")\n\ncompet$T = compet$mois\ntd = uncount(compet, mois)\ntd = arrange(td, id)\n\ntd$x=1\ntd$t = ave(td$x, td$id, FUN=cumsum)\ntd$t2 = td$t^2\n\nmy = mean(td$year)\ntd$yearb = td$year - my\nma = mean(td$age)\ntd$ageb = td$age  - ma\n\ntd$e = ifelse(td$t&lt;td$T,0, td$compet)\n\nEstimation\nPour estimer le modèle, on utilise la fonction mlogit. Les p-values seront calculées à partir d’un test bilatéral (statistique z).\n\ncompetfit = multinom(formula = e ~ t + t2 + yearb + ageb + surgery, data = td)\n\n# weights:  21 (12 variable)\ninitial  value 1238.136049 \niter  10 value 608.949443\niter  20 value 341.102661\niter  30 value 277.143136\niter  40 value 275.005451\nfinal  value 275.005419 \nconverged\n\ntbl_regression(competfit, exponentiate = TRUE,)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    \n      1\n    \n    t\n0.82\n0.75, 0.88\n&lt;0.001\n    t2\n1.00\n1.00, 1.00\n&lt;0.001\n    yearb\n0.88\n0.75, 1.03\n0.12\n    ageb\n1.04\n1.01, 1.08\n0.012\n    surgery\n0.32\n0.11, 0.91\n0.033\n    \n      2\n    \n    t\n0.82\n0.71, 0.94\n0.003\n    t2\n1.00\n1.00, 1.01\n0.052\n    yearb\n0.82\n0.62, 1.07\n0.14\n    ageb\n1.01\n0.96, 1.06\n0.7\n    surgery\n0.54\n0.12, 2.50\n0.4\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "14-R.html#footnotes",
    "href": "14-R.html#footnotes",
    "title": "15  R",
    "section": "",
    "text": "c’est le cas ici, la variable surgery est bien codée (0;1)↩︎"
  },
  {
    "objectID": "15-Stata.html#analyse-non-paramétrique",
    "href": "15-Stata.html#analyse-non-paramétrique",
    "title": "16  Stata",
    "section": "16.1 Analyse non paramétrique",
    "text": "16.1 Analyse non paramétrique\n\n16.1.1 Méthode actuarielle\nContrairement à la formation, l’estimation sera faite sur des intervalles de 30 jours\n\nltable stime died, interval(30) graph ci\n\n\n                 Beg.                                 Std.\n   Interval     total   Deaths   Lost   Survival     error     [95% conf. int.]\n-------------------------------------------------------------------------------\n    0    30       103       22      1     0.7854    0.0406     0.6926    0.8531\n   30    60        80       14      2     0.6462    0.0475     0.5449    0.7305\n   60    90        64       12      0     0.5250    0.0498     0.4232    0.6171\n   90   120        52        5      1     0.4741    0.0499     0.3738    0.5677\n  120   150        46        1      1     0.4636    0.0499     0.3637    0.5575\n  150   180        44        2      0     0.4426    0.0498     0.3435    0.5369\n  180   210        42        3      1     0.4106    0.0495     0.3132    0.5053\n  210   240        38        1      0     0.3998    0.0494     0.3030    0.4945\n  240   270        37        1      1     0.3888    0.0492     0.2928    0.4836\n  270   300        35        2      0     0.3666    0.0488     0.2720    0.4614\n  300   330        33        1      0     0.3555    0.0486     0.2618    0.4502\n  330   360        32        3      1     0.3216    0.0478     0.2308    0.4157\n  360   390        28        0      1     0.3216    0.0478     0.2308    0.4157\n  390   420        27        0      1     0.3216    0.0478     0.2308    0.4157\n  420   450        26        0      2     0.3216    0.0478     0.2308    0.4157\n  480   510        24        0      1     0.3216    0.0478     0.2308    0.4157\n  510   540        23        0      1     0.3216    0.0478     0.2308    0.4157\n  540   570        22        0      1     0.3216    0.0478     0.2308    0.4157\n  570   600        21        1      1     0.3059    0.0479     0.2155    0.4010\n  600   630        19        0      1     0.3059    0.0479     0.2155    0.4010\n  660   690        18        1      1     0.2885    0.0483     0.1982    0.3849\n  720   750        16        1      0     0.2704    0.0485     0.1807    0.3681\n  840   870        15        1      1     0.2518    0.0486     0.1629    0.3506\n  900   930        13        0      1     0.2518    0.0486     0.1629    0.3506\n  930   960        12        0      1     0.2518    0.0486     0.1629    0.3506\n  960   990        11        1      0     0.2289    0.0493     0.1404    0.3304\n  990  1020        10        1      0     0.2060    0.0494     0.1192    0.3093\n 1020  1050         9        1      0     0.1831    0.0489     0.0992    0.2873\n 1140  1170         8        0      1     0.1831    0.0489     0.0992    0.2873\n 1320  1350         7        0      1     0.1831    0.0489     0.0992    0.2873\n 1380  1410         6        1      2     0.1465    0.0510     0.0645    0.2602\n 1560  1590         3        0      2     0.1465    0.0510     0.0645    0.2602\n 1770  1800         1        0      1     0.1465    0.0510     0.0645    0.2602\n-------------------------------------------------------------------------------\n\n\n\n\n\n\n\n\n\nRécupération des quartiles de la durée\nInstallation de la commande qlt\n\nnet install qlt, from(\"https://raw.githubusercontent.com/mthevenin/analyse_duree/master/ado/qlt/\") replace\n\n* help qlt\n\n\nqui ltable stime died, interval(30) saving(base, replace)\nuse base, clear\n\n(SAVASTATA created this dataset on 01AUG2019)\n\n\n\nqlt\n\nDuree pour differents quantiles de la fonction de survie\nDefinition des bornes Stata-ltable\nS(t)=0.90: t=        .\nS(t)=0.75: t=    7.623\nS(t)=0.50: t=   74.729\nS(t)=0.25: t=  849.325\nS(t)=0.10: t=        .\n\n\nAvec la définition des bornes des intervalles de Sas\n\nqlt, sas\n\nDuree pour differents quantiles de la fonction de survie\nDefinition des bornes Sas-lifetest\n\n\nS(t)=0.90: t=   13.977\nS(t)=0.75: t=   37.623\nS(t)=0.50: t=  104.729\nS(t)=0.25: t=  906.993\nS(t)=0.10: t=        .\n\n\n\n\n16.1.2 Méthode Kaplan-Meier\nMode analyse des durées: stset\nLes données doivent être mises en mode analyse de durée avec la commande stset (help stset).\nA minima la commande stset entre la variable de durée en argument principal et la variable de censure/évènement avec failure(nom_var) en option.\n\nstset stime, f(died)\n\nlist id stime died _st _d _t _t0 in 1/10\n\n\nSurvival-time data settings\n\n         Failure event: died!=0 & died&lt;.\nObserved time interval: (0, stime]\n     Exit on or before: failure\n\n--------------------------------------------------------------------------\n        103  total observations\n          0  exclusions\n--------------------------------------------------------------------------\n        103  observations remaining, representing\n         75  failures in single-record/single-failure data\n     31,938  total analysis time at risk and under observation\n                                                At risk from t =         0\n                                     Earliest observed entry t =         0\n                                          Last observed exit t =     1,799\n\n     +-----------------------------------------+\n     | id   stime   died   _st   _d   _t   _t0 |\n     |-----------------------------------------|\n  1. | 15       1      1     1    1    1     0 |\n  2. | 43       2      1     1    1    2     0 |\n  3. | 61       2      1     1    1    2     0 |\n  4. | 75       2      1     1    1    2     0 |\n  5. |  6       3      1     1    1    3     0 |\n     |-----------------------------------------|\n  6. | 42       3      1     1    1    3     0 |\n  7. | 54       3      1     1    1    3     0 |\n  8. | 38       5      1     1    1    5     0 |\n  9. | 85       5      1     1    1    5     0 |\n 10. |  2       6      1     1    1    6     0 |\n     +-----------------------------------------+\n\n\nEstimation de la fonction de survie\nLa commande sts list permet d’afficher le tableau des estimations à chaque moment d’évènement\n\n\n\n\n\n\nNote\n\n\n\nLe tableau des estimateurs n’est reporté que pour le format htl du support\n\n\n\nsts list\n\n\n        Failure _d: died\n  Analysis time _t: stime\n\nKaplan–Meier survivor function\n\n             At                  Survivor      Std.\n  Time     risk   Fail   Lost    function     error     [95% conf. int.]\n------------------------------------------------------------------------\n\n\n     1      103      1      0      0.9903    0.0097     0.9331    0.9986\n     2      102      3      0      0.9612    0.0190     0.8998    0.9852\n     3       99      3      0      0.9320    0.0248     0.8627    0.9670\n     5       96      2      0      0.9126    0.0278     0.8388    0.9535\n     6       94      2      0      0.8932    0.0304     0.8155    0.9394\n     8       92      1      0      0.8835    0.0316     0.8040    0.9321\n     9       91      1      0      0.8738    0.0327     0.7926    0.9247\n    11       90      0      1      0.8738    0.0327     0.7926    0.9247\n    12       89      1      0      0.8640    0.0338     0.7811    0.9171\n    16       88      3      0      0.8345    0.0367     0.7474    0.8937\n    17       85      1      0      0.8247    0.0375     0.7363    0.8857\n    18       84      1      0      0.8149    0.0383     0.7253    0.8777\n    21       83      2      0      0.7952    0.0399     0.7034    0.8614\n    28       81      1      0      0.7854    0.0406     0.6926    0.8531\n    30       80      1      0      0.7756    0.0412     0.6819    0.8448\n    31       79      0      1      0.7756    0.0412     0.6819    0.8448\n    32       78      1      0      0.7657    0.0419     0.6710    0.8363\n    35       77      1      0      0.7557    0.0425     0.6603    0.8278\n    36       76      1      0      0.7458    0.0431     0.6495    0.8192\n    37       75      1      0      0.7358    0.0436     0.6388    0.8106\n    39       74      1      1      0.7259    0.0442     0.6282    0.8019\n    40       72      2      0      0.7057    0.0452     0.6068    0.7842\n    43       70      1      0      0.6956    0.0457     0.5961    0.7752\n    45       69      1      0      0.6856    0.0461     0.5855    0.7662\n    50       68      1      0      0.6755    0.0465     0.5750    0.7572\n    51       67      1      0      0.6654    0.0469     0.5645    0.7481\n    53       66      1      0      0.6553    0.0472     0.5541    0.7390\n    58       65      1      0      0.6452    0.0476     0.5437    0.7298\n    61       64      1      0      0.6352    0.0479     0.5333    0.7206\n    66       63      1      0      0.6251    0.0482     0.5230    0.7113\n    68       62      2      0      0.6049    0.0487     0.5026    0.6926\n    69       60      1      0      0.5948    0.0489     0.4924    0.6832\n    72       59      2      0      0.5747    0.0493     0.4722    0.6643\n    77       57      1      0      0.5646    0.0494     0.4621    0.6548\n    78       56      1      0      0.5545    0.0496     0.4521    0.6453\n    80       55      1      0      0.5444    0.0497     0.4422    0.6357\n    81       54      1      0      0.5343    0.0498     0.4323    0.6261\n    85       53      1      0      0.5243    0.0499     0.4224    0.6164\n    90       52      1      0      0.5142    0.0499     0.4125    0.6067\n    96       51      1      0      0.5041    0.0499     0.4027    0.5969\n   100       50      1      0      0.4940    0.0499     0.3930    0.5872\n   102       49      1      0      0.4839    0.0499     0.3833    0.5773\n   109       48      0      1      0.4839    0.0499     0.3833    0.5773\n   110       47      1      0      0.4736    0.0499     0.3733    0.5673\n   131       46      0      1      0.4736    0.0499     0.3733    0.5673\n   149       45      1      0      0.4631    0.0499     0.3632    0.5571\n   153       44      1      0      0.4526    0.0499     0.3531    0.5468\n   165       43      1      0      0.4421    0.0498     0.3430    0.5364\n   180       42      0      1      0.4421    0.0498     0.3430    0.5364\n   186       41      1      0      0.4313    0.0497     0.3327    0.5258\n   188       40      1      0      0.4205    0.0497     0.3225    0.5152\n   207       39      1      0      0.4097    0.0495     0.3123    0.5045\n   219       38      1      0      0.3989    0.0494     0.3022    0.4938\n   263       37      1      0      0.3881    0.0492     0.2921    0.4830\n   265       36      0      1      0.3881    0.0492     0.2921    0.4830\n   285       35      2      0      0.3660    0.0488     0.2714    0.4608\n   308       33      1      0      0.3549    0.0486     0.2612    0.4496\n   334       32      1      0      0.3438    0.0483     0.2510    0.4383\n   340       31      1      1      0.3327    0.0480     0.2409    0.4270\n   342       29      1      0      0.3212    0.0477     0.2305    0.4153\n   370       28      0      1      0.3212    0.0477     0.2305    0.4153\n   397       27      0      1      0.3212    0.0477     0.2305    0.4153\n   427       26      0      1      0.3212    0.0477     0.2305    0.4153\n   445       25      0      1      0.3212    0.0477     0.2305    0.4153\n   482       24      0      1      0.3212    0.0477     0.2305    0.4153\n   515       23      0      1      0.3212    0.0477     0.2305    0.4153\n   545       22      0      1      0.3212    0.0477     0.2305    0.4153\n   583       21      1      0      0.3059    0.0478     0.2156    0.4008\n   596       20      0      1      0.3059    0.0478     0.2156    0.4008\n   620       19      0      1      0.3059    0.0478     0.2156    0.4008\n   670       18      0      1      0.3059    0.0478     0.2156    0.4008\n   675       17      1      0      0.2879    0.0483     0.1976    0.3844\n   733       16      1      0      0.2699    0.0485     0.1802    0.3676\n   841       15      0      1      0.2699    0.0485     0.1802    0.3676\n   852       14      1      0      0.2507    0.0487     0.1616    0.3497\n   915       13      0      1      0.2507    0.0487     0.1616    0.3497\n   941       12      0      1      0.2507    0.0487     0.1616    0.3497\n   979       11      1      0      0.2279    0.0493     0.1394    0.3295\n   995       10      1      0      0.2051    0.0494     0.1183    0.3085\n  1032        9      1      0      0.1823    0.0489     0.0985    0.2865\n  1141        8      0      1      0.1823    0.0489     0.0985    0.2865\n  1321        7      0      1      0.1823    0.0489     0.0985    0.2865\n  1386        6      1      0      0.1519    0.0493     0.0713    0.2606\n  1400        5      0      1      0.1519    0.0493     0.0713    0.2606\n  1407        4      0      1      0.1519    0.0493     0.0713    0.2606\n  1571        3      0      1      0.1519    0.0493     0.0713    0.2606\n  1586        2      0      1      0.1519    0.0493     0.0713    0.2606\n  1799        1      0      1      0.1519    0.0493     0.0713    0.2606\n------------------------------------------------------------------------\n\n\nOn récupére les quantiles de la durée avec stci. Par défaut la commande affiche la durée médiane. On peut choisir le quantile avec l’arguement p(valeur) en option\n\nstci\n\nstci, p(75)\nstci, p(25)\n\n\n        Failure _d: died\n  Analysis time _t: stime\n\n             | Number of \n             |  subjects         50%      Std. err.    [95% conf. interval]\n-------------+-------------------------------------------------------------\n\n\n       Total |       103         100      38.64425           69        219\n\n        Failure _d: died\n  Analysis time _t: stime\n\n             | Number of \n             |  subjects         75%      Std. err.    [95% conf. interval]\n-------------+-------------------------------------------------------------\n\n\n       Total |       103         979      144.4728          340          .\n\n\n\n        Failure _d: died\n  Analysis time _t: stime\n\n             | Number of \n             |  subjects         25%      Std. err.    [95% conf. interval]\n-------------+-------------------------------------------------------------\n\n\n       Total |       103          36      9.033439           16         51\n\n\nPour afficher le graphique des estimateurs KM, on utilise la commande sts graph. On ajoute ici l’affichage des intervalles de confiance avec l’option ci\n\nsts graph, ci\n\n\n        Failure _d: died\n  Analysis time _t: stime\n\n\n\n\n\n\n\n\n\nLes commandes sts list, stci et sts graph permettent avec l’option by(nom_variable) d’obtenir des comparaisons entre groupes.\n\n\n16.1.3 Comparaison des fonctions de survie (KM)\nOn va comparer les fonctions de survie pour la variable surgery.\nGraphique\nOn ajoute l’option by(surgery) à la commande sts graph\n\nsts graph, by(surgery) ci ci1opts(fc(stc1%40))  ci2opts(fc(stc2%40)) legend(pos(6))\n\n\n        Failure _d: died\n  Analysis time _t: stime\n\n\n\n\n\n\n\n\n\nTests du logrank\nFonction sts test. On affichera ici plusieurs variantes du test.\n\nlocal test `\" \"l\" \"w\" \"tw\" \"p\" \"'\nforeach test2 of local test {\nsts test surgery, `test2'\n}\n\n\n        Failure _d: died\n  Analysis time _t: stime\n\nEquality of survivor functions\nLog-rank test\n\n        |  Observed       Expected\nsurgery |    events         events\n--------+-------------------------\n      0 |        69          60.34\n      1 |         6          14.66\n--------+-------------------------\n  Total |        75          75.00\n\n                  chi2(1) =   6.59\n                  Pr&gt;chi2 = 0.0103\n\n        Failure _d: died\n  Analysis time _t: stime\n\n\n\nEquality of survivor functions\nWilcoxon–Breslow–Gehan test\n\n        |  Observed       Expected       Sum of\nsurgery |    events         events        ranks\n--------+--------------------------------------\n      0 |        69          60.34          623\n      1 |         6          14.66         -623\n--------+--------------------------------------\n  Total |        75          75.00            0\n\n                               chi2(1) =   8.99\n                               Pr&gt;chi2 = 0.0027\n\n        Failure _d: died\n  Analysis time _t: stime\n\nEquality of survivor functions\nTarone–Ware test\n\n        |  Observed       Expected       Sum of\nsurgery |    events         events        ranks\n--------+--------------------------------------\n      0 |        69          60.34    73.105398\n      1 |         6          14.66   -73.105398\n--------+--------------------------------------\n  Total |        75          75.00            0\n\n                               chi2(1) =   8.46\n                               Pr&gt;chi2 = 0.0036\n\n        Failure _d: died\n  Analysis time _t: stime\n\n\n\nEquality of survivor functions\nPeto–Peto–Prentice test\n\n        |  Observed       Expected       Sum of\nsurgery |    events         events        ranks\n--------+--------------------------------------\n      0 |        69          60.34    6.0505875\n      1 |         6          14.66   -6.0505875\n--------+--------------------------------------\n  Total |        75          75.00            0\n\n                               chi2(1) =   8.66\n                               Pr&gt;chi2 = 0.0032\n\n\nComparaison des rmst\nInstallation de la commande strmst2\n\nssc install strmst2\n\n\nComparaison deux à deux (c’est pas plus mal comme cela). Je conseille de mettre les variables sous forme d’indicatrice.\nLes labels de la variable ne sont jamais reportés. Ils sont remplacés par les modalités arm0 et arm1.\nOn peut changer la valeurs maximale de la durée avec l’option tau(valeur).\n\narm1 = Opération\narm0 = pas d’opération\n\nstrmst2 surgery\n\n \nNumber of observations for analysis = 103\n \nThe truncation time, tau, was not specified. Thus, the default tau (the minimum\n&gt;  of the\nlargest observed event time within each group), 995.000, is used.\n\n\n\nRestricted Mean Survival Time (RMST) by arm\n-----------------------------------------------------------\n   Group |  Estimate    Std. Err.      [95% Conf. Interval]\n---------+-------------------------------------------------\n   arm 1 |   734.758     104.187      530.554      938.961\n   arm 0 |   310.168      42.481      226.907      393.429\n-----------------------------------------------------------\n\nBetween-group contrast (arm 1 versus arm 0) \n------------------------------------------------------------------------\n           Contrast  |  Estimate       [95% Conf. Interval]     P&gt;|z|\n---------------------+--------------------------------------------------\nRMST (arm 1 - arm 0) |   424.590      204.064      645.115      0.000\nRMST (arm 1 / arm 0) |     2.369        1.610        3.486      0.000\n------------------------------------------------------------------------"
  },
  {
    "objectID": "15-Stata.html#modèles-à-risques-proportionnels",
    "href": "15-Stata.html#modèles-à-risques-proportionnels",
    "title": "16  Stata",
    "section": "16.2 Modèles à risques proportionnels",
    "text": "16.2 Modèles à risques proportionnels\n\n16.2.1 Modèle de Cox\n\n16.2.1.1 Estimation du modèle\nCommande stcox\nAvec la correction d’Efron (conseillé)\nHR:\n\nstcox year age surgery, nolog noshow efron\n\n\nCox regression with Efron method for ties\n\nNo. of subjects =    103                                Number of obs =    103\nNo. of failures =     75\nTime at risk    = 31,938\n                                                        LR chi2(3)    =  17.63\nLog likelihood = -289.30639                             Prob &gt; chi2   = 0.0005\n\n\n\n------------------------------------------------------------------------------\n\n\n          _t | Haz. ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n\n\n-------------+----------------------------------------------------------------\n\n\n        year |\n\n\n      0.887      0.060    -1.78   0.076        0.778       1.012\n         age |      1.030      0.014     2.19   0.029        1.003       1.058\n     surgery |      0.373      0.163    -2.26   0.024        0.158       0.876\n\n\n------------------------------------------------------------------------------\n\n\nlog(HR):\n\nstcox year age surgery, nolog noshow efron nohr\n\n\nCox regression with Efron method for ties\n\nNo. of subjects =    103                                Number of obs =    103\nNo. of failures =     75\nTime at risk    = 31,938\n                                                        LR chi2(3)    =  17.63\nLog likelihood = -289.30639                             Prob &gt; chi2   = 0.0005\n\n\n\n------------------------------------------------------------------------------\n          _t | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        year |     -0.120      0.067    -1.78   0.076       -0.252       0.012\n         age |      0.030      0.014     2.19   0.029        0.003       0.056\n     surgery |     -0.987      0.436    -2.26   0.024       -1.842      -0.132\n------------------------------------------------------------------------------\n\n\n\n\n16.2.1.2 Test de l’hypothèse PH\nTest Grambsch-Therneau sur les résidus de Schoenfeld\nLe test OLS est exécuté avec la commande estat phtest. Je conseille d’ajouté l’option detail pour récupéré les test à un degré de liberté, le test omnibus n’étant pas fiable.\nPar défaut \\(f(t)=t\\).\n\n* f(t)=t - par défaut \n\nestat phtest, detail\n\n\n* f(t)= 1-km - solution par défaut de R\n      \nestat phtest, detail km\n\n\nTest of proportional-hazards assumption\n\nTime function: Analysis time\n--------------------------------------------------------\n             |        rho     chi2       df    Prob&gt;chi2\n-------------+------------------------------------------\n        year |    0.10162     0.80        1       0.3720\n         age |    0.12937     1.61        1       0.2043\n     surgery |    0.29664     5.54        1       0.0186\n-------------+------------------------------------------\n Global test |                8.76        3       0.0327\n--------------------------------------------------------\n\n\n\nTest of proportional-hazards assumption\n\nTime function: 1 - Kaplan–Meier estimate\n--------------------------------------------------------\n             |        rho     chi2       df    Prob&gt;chi2\n-------------+------------------------------------------\n        year |    0.15920     1.96        1       0.1620\n         age |    0.10907     1.15        1       0.2845\n     surgery |    0.25096     3.96        1       0.0465\n-------------+------------------------------------------\n Global test |                7.99        3       0.0462\n--------------------------------------------------------\n\n\nIntéraction avec une fonction de la durée\nAvec \\(f(t)=t\\)\nL’intéraction s’ajoute directement en option en indiquant la ou les variables concernées (une seule de préférence) avec l’option tvc(nom_variable) et la transformation de la durée (variable t) avec l’option texp(expression). Ici on a choisi \\(ft(t)\\) soit l’expression dans l’option texp(_t)\n\nstcox year age surgery, nolog noshow efron nohr tvc(surgery) texp(_t)\n\n\n\n\n\nCox regression with Efron method for ties\n\nNo. of subjects =    103                                Number of obs =    103\nNo. of failures =     75\nTime at risk    = 31,938\n                                                        LR chi2(4)    =  21.58\nLog likelihood = -287.32903                             Prob &gt; chi2   = 0.0002\n\n\n\n------------------------------------------------------------------------------\n          _t | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nmain         |\n\n\n        year |     -0.123      0.067    -1.84   0.066       -0.254       0.008\n         age |      0.029      0.013     2.15   0.032        0.003       0.055\n     surgery |     -1.755      0.674    -2.60   0.009       -3.077      -0.433\n-------------+----------------------------------------------------------------\ntvc          |\n     surgery |      0.002      0.001     2.02   0.043        0.000       0.004\n------------------------------------------------------------------------------\n\n\nNote: Variables in tvc equation interacted with _t.\n\n\n\n\n16.2.1.3 Introduction d’une variable dynamique (binaire)\nTransformation de la base en format long aux temps d’évènement\nEtape 1\n\nstset stime, f(died) id(id)\n\n\nSurvival-time data settings\n\n           ID variable: id\n         Failure event: died!=0 & died&lt;.\nObserved time interval: (stime[_n-1], stime]\n     Exit on or before: failure\n\n--------------------------------------------------------------------------\n        103  total observations\n          0  exclusions\n--------------------------------------------------------------------------\n        103  observations remaining, representing\n        103  subjects\n         75  failures in single-failure-per-subject data\n     31,938  total analysis time at risk and under observation\n                                                At risk from t =         0\n                                     Earliest observed entry t =         0\n                                          Last observed exit t =     1,799\n\n\nEtape 2\n\nstsplit, at(failure)\n\nstset stime, f(died) id(id)\n\n(62 failure times)\n\n\n(3,470 observations (episodes) created)\n\n\n\nSurvival-time data settings\n\n           ID variable: id\n         Failure event: died!=0 & died&lt;.\nObserved time interval: (stime[_n-1], stime]\n     Exit on or before: failure\n\n--------------------------------------------------------------------------\n      3,573  total observations\n          0  exclusions\n--------------------------------------------------------------------------\n      3,573  observations remaining, representing\n        103  subjects\n         75  failures in single-failure-per-subject data\n     31,938  total analysis time at risk and under observation\n                                                At risk from t =         0\n                                     Earliest observed entry t =         0\n                                          Last observed exit t =     1,799\n\n\nEtape 3\n\ngen tvc = transplant==1 & wait&lt;=_t\nsort id _t\nlist id transplant wait tvc _d _t _t0 if id==10  , noobs\n\n\n  +--------------------------------------------+\n  | id   transp~t   wait   tvc   _d   _t   _t0 |\n  |--------------------------------------------|\n  | 10          1     12     0    0    1     0 |\n  | 10          1     12     0    0    2     1 |\n  | 10          1     12     0    0    3     2 |\n  | 10          1     12     0    0    5     3 |\n  | 10          1     12     0    0    6     5 |\n  |--------------------------------------------|\n  | 10          1     12     0    0    8     6 |\n  | 10          1     12     0    0    9     8 |\n  | 10          1     12     1    0   12     9 |\n  | 10          1     12     1    0   16    12 |\n  | 10          1     12     1    0   17    16 |\n  |--------------------------------------------|\n  | 10          1     12     1    0   18    17 |\n  | 10          1     12     1    0   21    18 |\n  | 10          1     12     1    0   28    21 |\n  | 10          1     12     1    0   30    28 |\n  | 10          1     12     1    0   32    30 |\n  |--------------------------------------------|\n  | 10          1     12     1    0   35    32 |\n  | 10          1     12     1    0   36    35 |\n  | 10          1     12     1    0   37    36 |\n  | 10          1     12     1    0   39    37 |\n  | 10          1     12     1    0   40    39 |\n  |--------------------------------------------|\n  | 10          1     12     1    0   43    40 |\n  | 10          1     12     1    0   45    43 |\n  | 10          1     12     1    0   50    45 |\n  | 10          1     12     1    0   51    50 |\n  | 10          1     12     1    0   53    51 |\n  |--------------------------------------------|\n  | 10          1     12     1    1   58    53 |\n  +--------------------------------------------+\n\n\nEstimation du modèle\n\nstcox year age surgery tvc, nolog noshow efron nohr\n\n\nCox regression with Efron method for ties\n\nNo. of subjects =    103                                Number of obs =  3,573\nNo. of failures =     75\nTime at risk    = 31,938\n                                                        LR chi2(4)    =  17.70\nLog likelihood = -289.27014                             Prob &gt; chi2   = 0.0014\n\n\n\n------------------------------------------------------------------------------\n          _t | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        year |     -0.120      0.067    -1.79   0.074       -0.252       0.012\n         age |      0.030      0.014     2.19   0.029        0.003       0.058\n     surgery |     -0.983      0.437    -2.25   0.024       -1.839      -0.127\n         tvc |     -0.082      0.305    -0.27   0.787       -0.680       0.515\n------------------------------------------------------------------------------\n\n\n\n\n\n16.2.2 Modèle à durée discrète\nVariable de durée = mois\n\n16.2.2.1 Mise en forme de la base\n\nexpand mois\nbysort id: gen t=_n\ngen e = died\nreplace e=0 if t&lt;mois\n\nlist id year age surgery mois t e  in 1/31\n\n(1,024 observations created)\n(399 real changes made)\n\n     +-------------------------------------------+\n     | id   year   age   surgery   mois    t   e |\n     |-------------------------------------------|\n  1. |  1     67    30         0      2    1   0 |\n  2. |  1     67    30         0      2    2   1 |\n  3. |  2     68    51         0      1    1   1 |\n  4. |  3     68    54         0      1    1   1 |\n  5. |  4     68    40         0      2    1   0 |\n     |-------------------------------------------|\n  6. |  4     68    40         0      2    2   1 |\n  7. |  5     68    20         0      1    1   1 |\n  8. |  6     68    54         0      1    1   1 |\n  9. |  7     68    50         0     23    1   0 |\n 10. |  7     68    50         0     23    2   0 |\n     |-------------------------------------------|\n 11. |  7     68    50         0     23    3   0 |\n 12. |  7     68    50         0     23    4   0 |\n 13. |  7     68    50         0     23    5   0 |\n 14. |  7     68    50         0     23    6   0 |\n 15. |  7     68    50         0     23    7   0 |\n     |-------------------------------------------|\n 16. |  7     68    50         0     23    8   0 |\n 17. |  7     68    50         0     23    9   0 |\n 18. |  7     68    50         0     23   10   0 |\n 19. |  7     68    50         0     23   11   0 |\n 20. |  7     68    50         0     23   12   0 |\n     |-------------------------------------------|\n 21. |  7     68    50         0     23   13   0 |\n 22. |  7     68    50         0     23   14   0 |\n 23. |  7     68    50         0     23   15   0 |\n 24. |  7     68    50         0     23   16   0 |\n 25. |  7     68    50         0     23   17   0 |\n     |-------------------------------------------|\n 26. |  7     68    50         0     23   18   0 |\n 27. |  7     68    50         0     23   19   0 |\n 28. |  7     68    50         0     23   20   0 |\n 29. |  7     68    50         0     23   21   0 |\n 30. |  7     68    50         0     23   22   0 |\n     |-------------------------------------------|\n 31. |  7     68    50         0     23   23   1 |\n     +-------------------------------------------+\n\n\n\n\n16.2.2.2 Paramétrisation avec durée quantitative\nLes critères d’information\n\ngen t2=t^2\ngen t3=t^3\n\nqui logit e  t ,  nolog \nestat ic\n\nqui logit e  t t2 ,  nolog \nestat ic\n\nqui logit e  t2 t3 ,  nolog \nestat ic\n\n\nAkaike's information criterion and Bayesian information criterion\n\n-----------------------------------------------------------------------------\n       Model |          N   ll(null)  ll(model)      df        AIC        BIC\n-------------+---------------------------------------------------------------\n           . |      1,127  -275.6841  -250.2606       2   504.5212   514.5758\n-----------------------------------------------------------------------------\nNote: BIC uses N = number of observations. See [R] IC note.\n\n\n\nAkaike's information criterion and Bayesian information criterion\n\n-----------------------------------------------------------------------------\n       Model |          N   ll(null)  ll(model)      df        AIC        BIC\n-------------+---------------------------------------------------------------\n           . |      1,127  -275.6841  -243.0576       3   492.1152   507.1972\n-----------------------------------------------------------------------------\nNote: BIC uses N = number of observations. See [R] IC note.\n\n\n\nAkaike's information criterion and Bayesian information criterion\n\n-----------------------------------------------------------------------------\n       Model |          N   ll(null)  ll(model)      df        AIC        BIC\n-------------+---------------------------------------------------------------\n           . |      1,127  -275.6841  -254.3782       3   514.7563   529.8383\n-----------------------------------------------------------------------------\nNote: BIC uses N = number of observations. See [R] IC note.\n\n\nEstimation du modèle\n\nqui sum year\nqui gen year2 = year - `r(mean)'\nqui sum age\nqui gen age2 = age - `r(mean)'\n\n\nlogit e t t2 t3 year2 age2 surgery, nolog or\n\n\nLogistic regression                                     Number of obs =  1,127\n                                                        LR chi2(6)    =  90.69\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -230.33671                             Pseudo R2     = 0.1645\n\n\n\n------------------------------------------------------------------------------\n           e | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n           t |      0.689      0.057    -4.52   0.000        0.587       0.810\n          t2 |      1.014      0.005     2.83   0.005        1.004       1.024\n          t3 |      1.000      0.000    -2.11   0.035        1.000       1.000\n       year2 |      0.876      0.065    -1.80   0.072        0.758       1.012\n        age2 |      1.034      0.015     2.27   0.023        1.005       1.064\n     surgery |      0.364      0.163    -2.25   0.024        0.151       0.877\n       _cons |      0.440      0.110    -3.29   0.001        0.270       0.718\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds.\n\n\n\n\n16.2.2.3 Paramétrisation avec durée discrète\nPour l’exemple seulement, on procède à un regroupement des intervalles découpés sur les quartiles de la durée\n\nxtile ct4=t, n(4)\nbysort id ct4: keep if _n==_N\n\ntab  ct4 e\n\n(930 observations deleted)\n\n         4 |\n quantiles |           e\n      of t |         0          1 |     Total\n-----------+----------------------+----------\n         1 |        50         53 |       103 \n         2 |        35         11 |        46 \n         3 |        27          5 |        32 \n         4 |        10          6 |        16 \n-----------+----------------------+----------\n     Total |       122         75 |       197 \n\n\n\nlogit e i.ct4  year age surgery,  nolog or\n\n\nLogistic regression                                     Number of obs =    197\n                                                        LR chi2(6)    =  39.30\n                                                        Prob &gt; chi2   = 0.0000\nLog likelihood = -111.23965                             Pseudo R2     = 0.1501\n\n\n\n------------------------------------------------------------------------------\n           e | Odds ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n         ct4 |\n          1  |      1.000  (base)\n          2  |      0.356      0.149    -2.47   0.014        0.157       0.809\n          3  |      0.199      0.108    -2.96   0.003        0.068       0.578\n          4  |      0.619      0.371    -0.80   0.424        0.191       2.005\n             |\n        year |      0.816      0.076    -2.18   0.029        0.680       0.980\n         age |      1.048      0.019     2.53   0.011        1.011       1.087\n     surgery |      0.330      0.166    -2.21   0.027        0.123       0.882\n       _cons |   2.54e+05   1.69e+06     1.87   0.061        0.552    1.17e+11\n------------------------------------------------------------------------------\nNote: _cons estimates baseline odds."
  },
  {
    "objectID": "15-Stata.html#modèles-paramétriques",
    "href": "15-Stata.html#modèles-paramétriques",
    "title": "16  Stata",
    "section": "16.3 Modèles paramétriques",
    "text": "16.3 Modèles paramétriques\nJuste un exemple pour le modèle de weibull\nCommande streg\nPar défaut, le modèle de Weibull est exécuté sous paramétrisation PH. Pour une paramétrisation type AFT, ajouter l’option time.\nPH: défaut\n\n*qui stset stime, f(died) // penser à mettre la base en mode analyse de durée\n\nstreg year age surgery , dist(weibull) nolog noheader\n\n\n        Failure _d: died\n  Analysis time _t: stime\n\n\n\n\n\n------------------------------------------------------------------------------\n          _t | Haz. ratio   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        year |      0.914      0.061    -1.36   0.175        0.802       1.041\n         age |      1.035      0.014     2.47   0.014        1.007       1.063\n     surgery |      0.334      0.145    -2.52   0.012        0.143       0.783\n       _cons |      5.368     25.895     0.35   0.728        0.000   68506.303\n-------------+----------------------------------------------------------------\n       /ln_p |     -0.587      0.093    -6.33   0.000       -0.769      -0.405\n-------------+----------------------------------------------------------------\n           p |      0.556      0.052                         0.464       0.667\n         1/p |      1.798      0.167                         1.499       2.157\n------------------------------------------------------------------------------\nNote: _cons estimates baseline hazard.\n\n\nAFT\n\nstreg year age surgery , dist(weibull) time nolog noheader\n\n\n        Failure _d: died\n  Analysis time _t: stime\n\n\n\n\n\n------------------------------------------------------------------------------\n          _t | Coefficient  Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        year |      0.162      0.122     1.33   0.184       -0.077       0.401\n         age |     -0.062      0.025    -2.49   0.013       -0.110      -0.013\n     surgery |      1.970      0.779     2.53   0.011        0.443       3.498\n       _cons |     -3.022      8.728    -0.35   0.729      -20.129      14.085\n-------------+----------------------------------------------------------------\n       /ln_p |     -0.587      0.093    -6.33   0.000       -0.769      -0.405\n-------------+----------------------------------------------------------------\n           p |      0.556      0.052                         0.464       0.667\n         1/p |      1.798      0.167                         1.499       2.157\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "15-Stata.html#risques-concurrents",
    "href": "15-Stata.html#risques-concurrents",
    "title": "16  Stata",
    "section": "16.4 Risques concurrents",
    "text": "16.4 Risques concurrents\n\n16.4.1 Non paramétrique: estimation des IC\nInstaller les commandes stcompet et stcomlist\n\nssc install stcompet\nssc install stcomlist\n\nLe risque d’intérêt est compet=1, le risque concurrent est compet=2. La commande externe stcompet permet d’estimer les CIF mais ne propose pas d’output des estimateurs 1. On utilise alors la commande stcomlist en précisant le ou les risques concurrent (ici compet=2)\n\nstset stime, failure(compet==1)\nstcomlist, compet1(2)\n\n\nSurvival-time data settings\n\n         Failure event: compet==1\nObserved time interval: (0, stime]\n     Exit on or before: failure\n\n--------------------------------------------------------------------------\n        103  total observations\n          0  exclusions\n--------------------------------------------------------------------------\n        103  observations remaining, representing\n         56  failures in single-record/single-failure data\n     31,938  total analysis time at risk and under observation\n                                                At risk from t =         0\n                                     Earliest observed entry t =         0\n                                          Last observed exit t =     1,799\n\n\n\n            failure:  compet == 1\n competing failures:  compet == 2\n\n    Time       CIF         SE     [95% Conf. Int.]\n--------------------------------------------------\n       1    0.0097     0.0097     0.0009    0.0477\n       2    0.0388     0.0190     0.0127    0.0892\n       3    0.0583     0.0231     0.0239    0.1149\n       5    0.0777     0.0264     0.0363    0.1395\n       6    0.0874     0.0278     0.0429    0.1515\n       8    0.0971     0.0292     0.0497    0.1634\n       9    0.1068     0.0304     0.0566    0.1751\n      12    0.1166     0.0316     0.0638    0.1868\n      16    0.1362     0.0338     0.0785    0.2099\n      18    0.1461     0.0349     0.0860    0.2212\n      21    0.1657     0.0367     0.1014    0.2437\n      32    0.1756     0.0376     0.1093    0.2550\n      37    0.1856     0.0384     0.1173    0.2662\n      40    0.1957     0.0393     0.1254    0.2775\n      43    0.2058     0.0400     0.1337    0.2888\n      45    0.2158     0.0408     0.1420    0.2999\n      50    0.2259     0.0415     0.1503    0.3110\n      51    0.2360     0.0422     0.1588    0.3221\n      53    0.2461     0.0428     0.1673    0.3330\n      58    0.2562     0.0434     0.1759    0.3439\n      61    0.2662     0.0440     0.1845    0.3548\n      66    0.2763     0.0445     0.1932    0.3656\n      69    0.2864     0.0450     0.2020    0.3763\n      72    0.3066     0.0459     0.2197    0.3976\n\n\n      77    0.3167     0.0464     0.2286    0.4082\n      78    0.3267     0.0467     0.2376    0.4187\n      81    0.3368     0.0471     0.2466    0.4292\n      85    0.3469     0.0475     0.2556    0.4396\n      90    0.3570     0.0478     0.2648    0.4500\n      96    0.3671     0.0481     0.2739    0.4604\n     102    0.3771     0.0484     0.2831    0.4707\n     110    0.3874     0.0487     0.2925    0.4812\n     149    0.3980     0.0489     0.3021    0.4920\n     165    0.4085     0.0492     0.3118    0.5027\n     186    0.4193     0.0495     0.3217    0.5137\n     188    0.4301     0.0497     0.3316    0.5246\n     207    0.4408     0.0499     0.3417    0.5354\n     219    0.4516     0.0501     0.3517    0.5462\n     263    0.4624     0.0502     0.3618    0.5570\n     285    0.4846     0.0505     0.3826    0.5791\n     308    0.4957     0.0506     0.3931    0.5900\n     340    0.5068     0.0507     0.4037    0.6009\n     583    0.5221     0.0514     0.4171    0.6168\n     675    0.5401     0.0524     0.4322    0.6361\n     733    0.5580     0.0532     0.4477    0.6548\n     995    0.5808     0.0548     0.4659    0.6795\n    1032    0.6036     0.0559     0.4851    0.7031\n    1386    0.6340     0.0583     0.5083    0.7357\n\n\n            failure:  compet == 2\n competing failures:  compet == 1\n\n    Time       CIF         SE     [95% Conf. Int.]\n--------------------------------------------------\n       3    0.0097     0.0097     0.0009    0.0477\n       6    0.0194     0.0136     0.0038    0.0619\n      16    0.0292     0.0166     0.0079    0.0761\n      17    0.0391     0.0191     0.0128    0.0897\n      28    0.0489     0.0213     0.0182    0.1029\n      30    0.0587     0.0232     0.0240    0.1157\n      35    0.0686     0.0250     0.0302    0.1286\n      36    0.0786     0.0267     0.0367    0.1411\n      39    0.0885     0.0282     0.0435    0.1534\n      40    0.0986     0.0296     0.0504    0.1658\n      68    0.1188     0.0322     0.0650    0.1901\n      80    0.1288     0.0334     0.0724    0.2020\n     100    0.1389     0.0345     0.0800    0.2138\n     153    0.1495     0.0356     0.0880    0.2261\n     334    0.1605     0.0368     0.0964    0.2392\n     342    0.1720     0.0381     0.1052    0.2526\n     852    0.1913     0.0417     0.1175    0.2787\n     979    0.2141     0.0460     0.1320    0.3094\n\n\n\n\n16.4.2 Modèle multinomial à durée discrète\nRappel: Le modèle de Cox cause-specific s’excute facilement. Il suffit de passer la ou les causes concurrente en censure à droite. De même le modèle Fine-Gray (critiqué) est estimable avec la commande usine stcrreg\nAttention le modèle modèle multinomial n’est pas directement relié aux CIF. Il permet néanmoins d’estimer des probabilités conditionnelles qui tiennent pleinement compte de concurrence entre les différentes issues. Pour la variable de durée on utilise la variable mois\n\nqui expand mois\nqui bysort id: gen t=_n\nqui gen t2=t*t\n\nqui sum year\nqui gen year2 = year - `r(mean)'\nqui sum age\nqui gen age2 = age - `r(mean)'\n\ngen e = compet\nreplace e=0 if t&lt;mois\nmlogit e t t2 year2 age2 surgery, rrr noheader\n\n(399 real changes made)\n\n\n\nIteration 0:  Log likelihood = -318.13171  \nIteration 1:  Log likelihood = -285.78811  \nIteration 2:  Log likelihood = -275.20206  \nIteration 3:  Log likelihood = -275.00574  \nIteration 4:  Log likelihood = -275.00542  \nIteration 5:  Log likelihood = -275.00542  \n\n\n------------------------------------------------------------------------------\n           e |        RRR   Std. err.      z    P&gt;|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n\n\n0            |  (base outcome)\n\n\n-------------+----------------------------------------------------------------\n1            |\n           t |      0.816      0.034    -4.91   0.000        0.752       0.885\n          t2 |      1.003      0.001     3.53   0.000        1.001       1.005\n       year2 |      0.879      0.072    -1.57   0.116        0.749       1.032\n        age2 |      1.045      0.018     2.51   0.012        1.010       1.081\n     surgery |      0.318      0.171    -2.13   0.033        0.110       0.913\n       _cons |      0.222      0.052    -6.49   0.000        0.141       0.350\n-------------+----------------------------------------------------------------\n2            |\n           t |      0.817      0.056    -2.93   0.003        0.713       0.935\n          t2 |      1.003      0.002     1.94   0.052        1.000       1.006\n       year2 |      0.816      0.113    -1.47   0.141        0.622       1.070\n        age2 |      1.011      0.025     0.45   0.654        0.964       1.061\n     surgery |      0.541      0.422    -0.79   0.431        0.117       2.496\n       _cons |      0.078      0.029    -6.94   0.000        0.038       0.160\n------------------------------------------------------------------------------\nNote: _cons estimates baseline relative risk for each outcome."
  },
  {
    "objectID": "15-Stata.html#footnotes",
    "href": "15-Stata.html#footnotes",
    "title": "16  Stata",
    "section": "",
    "text": "on doit cependant l’installer pour utiliser la commande suivante↩︎"
  },
  {
    "objectID": "16-Sas.html#analyse-non-paramétrique",
    "href": "16-Sas.html#analyse-non-paramétrique",
    "title": "17  SAS",
    "section": "17.1 Analyse non paramétrique",
    "text": "17.1 Analyse non paramétrique\n\n17.1.1 Méthode actuarielle\nAvec une longueur d’intervalle fixe égale à 30 jours.\nLa durée médiane est donnée par la colonne résidual median time. Sur la première ligne, il s’agit de la durée médiane sur toutes les personnes exposées au risque. Dans les lignes suivante, cette durée médiane est recalculée pour les personnes restant exposées au risque dans chaque intervalle.\n\nproc lifetest data=trans method=lifetable width=30;\ntime stime*died(0);run;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n17.1.2 Méthode Kaplan-Meier\nLe tableau des estimateurs ne sera pas reporté (voir intro du document).\nPour récupérer ces estimateurs, on peut les récupérer via l’instruction output et les exporter, par exemple, dans un tableur.\n\nods exclude Lifetest.Stratum1.ProductLimitEstimates;\nproc lifetest data=trans;\ntime stime*died(0); run;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning sur la durée moyenne reportée Sauf exception ne pas interpréter le tableau donnant la durée moyenne. Se reporter à l’estimation des RMST plus bas.\nComparaison des fonctions de survie\nTests du log rank\n\nods exclude Lifetest.Stratum1.ProductLimitEstimates Lifetest.Stratum2.ProductLimitEstimates ;\nproc lifetest data=trans;\ntime stime*died(0);\nstrata surgery / test=all;\nrun;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparaison des RMST\nDisponible avec le dernier module stat de Sas base (Sas-Stat 15.1 novembre 2018).\n\nods exclude Lifetest.Stratum1.ProductLimitEstimates;\nproc lifetest data=trans rmst plots=(rmst);\ntime stime*died(0);\nstrata surgery; run;"
  },
  {
    "objectID": "16-Sas.html#modèle-de-cox",
    "href": "16-Sas.html#modèle-de-cox",
    "title": "17  SAS",
    "section": "17.2 Modèle de Cox",
    "text": "17.2 Modèle de Cox\n\n17.2.1 Estimation du modèle\n\nproc phreg data=trans;\nmodel stime*died(0) = year age surgery ;\nrun;\n\n\n\n\n\n\n\n\n17.2.2 Tests de l’hypothèse PH\n\n17.2.2.1 Test de Grambsch Therneau\nDemande au moins l’avant dernière version de Sas/Stat (2016?).\nLe test est exécuté directement dans l’instruction phreg (ajouter zph). L’option global permet de récupérer le résultat du test omnibus (attention rejette facilement \\(H_0\\) - hypothèse PH respectée - lorsque le nombre de degrés de liberté est élevé).\n\nods select PHReg.zphTest;\nproc phreg data=trans zph(global noplot);\nmodel stime*died(0) = year age surgery ;\nrun;\n\n\n\n\n\n\nPar défaut SAS utilise la transformation \\(f(t)=t\\) (idem Stata). Pour obtenir l’option par défaut de R \\(f(t) = 1 - KM(t)\\):\n\nods select PHReg.zphTest;\nproc phreg data=trans zph(global noplot transform=km);\nmodel stime*died(0) = year age surgery ;\nrun;\n\n\n\n\n\n\n\n\n17.2.2.2 Interaction avec la durée\nEstimation d’un modèle avec indicatrices\nLa covariable doit être sous forme d’indicatrice (binaire: (0,1)). Ce qui est le cas ici avec la variable surgery.\nExemple avec une covariable X à 3 modalités codée 1,2,3.\nEstimation du modèle de Cox avec l’instruction class (ref: X=1)\n\nproc phreg data=base;\nclass X(ref=\"1\");\nmodel variable_dur*variable_cens(0) = X; run;\n\nEstimation du modèle de Cox avec indicatrices\n\ndata base; set base;\nX1 = X=1;\nX2 = X=2;\nX3 = X=3; run;\n\nproc phreg data=base;\nmodel variable_dur*variable_cens(0) = X2 X3; run;\n\nLa variable d’intéraction (\\(surgeryt = surgery\\times stime\\)) est générée, le temps de l’estimation après l’instruction model.\n\nods select PHReg.ParameterEstimates;\nproc phreg data=trans ;\nmodel stime*died(0) = year age surgery surgeryt ;\nsurgeryt = surgery*stime;\nrun;\n\n\n\n\n\n\n\n\n\n17.2.3 Variable dynamique\nWarning: opération en ‘aveugle’\nContrairement à R et Stata, la base n’a pas à être splittée, on ne peut pas vérifier si la variable dynamique a été correctement créée. La variable dynamique, qui peut être appréhendée comme une variable en intéraction avec la durée, est générée après l’instruction model.\n Ici la tvc prendra la valeur 1 lorsque stime&gt;wait, 0 sinon.\n\nods select PHReg.ParameterEstimates;\nproc phreg data=trans;\nmodel stime*died(0) = year age surgery tvc ;\ntvc = transplant=1 and stime&gt;=wait;\nrun;"
  },
  {
    "objectID": "16-Sas.html#modèle-à-temps-discret",
    "href": "16-Sas.html#modèle-à-temps-discret",
    "title": "17  SAS",
    "section": "17.3 Modèle à temps discret",
    "text": "17.3 Modèle à temps discret\n\n17.3.1 Mise en forme\nOn utilise une boucle pour répliquer les lignes sur la valeur de la durée. La nouvelle variable de durée (t) sous forme de compteur est générée automatiquement.\n\ndata td; set trans; \ndo t=1 to mois; \n     output; \n     end; run;\n     \ndata td; set td;\nif t&lt;mois then died=0;\nt2=t*t;\nt3=t2*t; run;\n\n\n\n17.3.2 Durée continue\nEstimation du modèle\n\nods select Logistic.FitStatistics;\nproc logistic data=td;\nmodel died(ref=\"0\") = t t2 t3 year age surgery  ; run;\n\n\n\n\n\n\n\n\n17.3.3 Durée discrète\nPour l’exemple on va regrouper la durée par ses quartiles. Pour chaque individu, on conserve seulement une observation dans chaque quartile.\n\nproc rank data=td out=td2 groups=4;\nvar t;\nranks tq4;\nrun;\n\ndata td2; set td2;\nid2=put(id, 3.);\ntq42=put(tq4, 1.);\ng=id2 || tq42; run;\n\nproc sort data=td2; by id tq4; run;\n\ndata td2; set td2;\nby g;\nif LAST.g; run;\n\nEstimation\n\nproc logistic data=td2;\nclass tq4 / param=ref;\nmodel died(ref=\"0\") = tq4 year age surgery; run;"
  },
  {
    "objectID": "16-Sas.html#modèles-paramétrique",
    "href": "16-Sas.html#modèles-paramétrique",
    "title": "17  SAS",
    "section": "17.4 Modèles paramétrique",
    "text": "17.4 Modèles paramétrique\nOn utilise la procédure proc lifereg et on indique le type de distribution\n\nproc lifereg data=trans;\nmodel stime*died(0) = year age surgery /D=WEIBULL;\nrun;\n\n\n\n\n\n\n\n\n\n\n\n\nproc lifereg data=trans;\nmodel stime*died(0) = year age surgery /D=LLOGISTIC;\nrun;"
  },
  {
    "objectID": "16-Sas.html#risques-concurrents",
    "href": "16-Sas.html#risques-concurrents",
    "title": "17  SAS",
    "section": "17.5 Risques concurrents",
    "text": "17.5 Risques concurrents\n\n17.5.1 Non paramétrique\nOn indique en option la cause d’intérêt avec eventcode=valeur , les autres étant considérées commes des risques concurrents.\n\nproc lifetest data=trans plots=CIF;\ntime stime*compet(0) / eventcode=1; run;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPour récupérer le test de Gray, on utilise l’instruction strata.\n\nproc lifetest data=trans plots=CIF;\ntime stime*compet(0) / eventcode=1\nstrata surgery; run;\n\n\n\n\n\n\n\n\n\n\n\n\n\n17.5.2 Modèle logistique multinomial à durée discrète\n\ndata td; set trans; \ndo t=1 to mois; \n     output; \n     end; \nrun;\ndata td; set td;\nif t&lt;mois then compet=0;\nt2=t*t\nrun;\n\n\nproc logistic data=td;\nmodel compet(ref=\"0\") = t t2 year age surgery / link=glogit;\nrun;"
  },
  {
    "objectID": "17-Python.html",
    "href": "17-Python.html",
    "title": "18  Python",
    "section": "",
    "text": "19 Package statsmodels\nhttps://www.statsmodels.org/dev/duration.html\nLe package permet d’estimer des fonction de séjour de type Kaplan-Meier et des modèles de Cox."
  },
  {
    "objectID": "17-Python.html#package-lifelines",
    "href": "17-Python.html#package-lifelines",
    "title": "18  Python",
    "section": "18.1 Package lifelines",
    "text": "18.1 Package lifelines\nDocumentation: https://lifelines.readthedocs.io/en/latest/\n\n18.1.1 Non Paramétrique: Kaplan Meier\nEstimateur KM et durée médiane\n\nT = trans['stime']\nE = trans['died']\n\n\nfrom lifelines import KaplanMeierFitter\nkmf = KaplanMeierFitter()\nkmf.fit(T,E)\nprint(kmf.survival_function_)\na = \"DUREE MEDIANE:\"\nb = kmf.median_survival_time_\nprint(a,b)\n\n          KM_estimate\ntimeline             \n0.0          1.000000\n1.0          0.990291\n2.0          0.961165\n3.0          0.932039\n5.0          0.912621\n...               ...\n1400.0       0.151912\n1407.0       0.151912\n1571.0       0.151912\n1586.0       0.151912\n1799.0       0.151912\n\n[89 rows x 1 columns]\nDUREE MEDIANE: 100.0\n\n\n\nkmf.plot()\n\n&lt;AxesSubplot: xlabel='timeline'&gt;\n\n\n\n\n\n\n\n\n\nComparaison des fonctions de survie\n\nax = plt.subplot(111)\nkmf = KaplanMeierFitter()\nfor name, grouped_df in trans.groupby('surgery'):\n    kmf.fit(grouped_df['stime'], grouped_df['died'], label=name)\n    kmf.plot(ax=ax)\n\n\n\n\n\n\n\n\n\nfrom lifelines.statistics import multivariate_logrank_test\nresults = multivariate_logrank_test(trans['stime'], trans['surgery'], trans['died'])\nresults.print_summary()\n\n\n\n\n\n\n\nt_0\n-1\n\n\nnull_distribution\nchi squared\n\n\ndegrees_of_freedom\n1\n\n\ntest_name\nmultivariate_logrank_test\n\n\n\n\n\n\n\ntest_statistic\np\n-log2(p)\n\n\n\n\n0\n6.59\n0.01\n6.61"
  },
  {
    "objectID": "17-Python.html#semi-paramétrique-cox",
    "href": "17-Python.html#semi-paramétrique-cox",
    "title": "18  Python",
    "section": "18.2 Semi paramétrique: Cox",
    "text": "18.2 Semi paramétrique: Cox\n\n18.2.1 Estimation\n\nmodel = 'year + age + C(surgery) -1'\nX = pt.dmatrix(model, trans, return_type='dataframe')\ndesign_info = X.design_info\nYX = X.join(trans[['stime','died']])\nYX.drop(['C(surgery)[0]'], axis=1, inplace=True)\nYX.head()\n\n\nfrom lifelines import CoxPHFitter\ncph = CoxPHFitter()\ncph.fit(YX, duration_col='stime', event_col='died')\ncph.print_summary()\ncph.plot()\n\n\n\n\n\n\n\nmodel\nlifelines.CoxPHFitter\n\n\nduration col\n'stime'\n\n\nevent col\n'died'\n\n\nbaseline estimation\nbreslow\n\n\nnumber of observations\n103\n\n\nnumber of events observed\n75\n\n\npartial log-likelihood\n-289.31\n\n\ntime fit was run\n2023-08-21 11:46:42 UTC\n\n\n\n\n\n\n\ncoef\nexp(coef)\nse(coef)\ncoef lower 95%\ncoef upper 95%\nexp(coef) lower 95%\nexp(coef) upper 95%\ncmp to\nz\np\n-log2(p)\n\n\n\n\nC(surgery)[1]\n-0.99\n0.37\n0.44\n-1.84\n-0.13\n0.16\n0.88\n0.00\n-2.26\n0.02\n5.40\n\n\nyear\n-0.12\n0.89\n0.07\n-0.25\n0.01\n0.78\n1.01\n0.00\n-1.78\n0.08\n3.72\n\n\nage\n0.03\n1.03\n0.01\n0.00\n0.06\n1.00\n1.06\n0.00\n2.19\n0.03\n5.12\n\n\n\n\n\n\n\n\n\nConcordance\n0.65\n\n\nPartial AIC\n584.61\n\n\nlog-likelihood ratio test\n17.63 on 3 df\n\n\n-log2(p) of ll-ratio test\n10.90\n\n\n\n\n\n\n\n\n&lt;AxesSubplot: xlabel='log(HR) (95% CI)'&gt;\n\n\n\n\n\n\n\n\n\n\n\n18.2.2 Tests hypothèse PH\nTest PH: Schoenfeld Méthode 1\n\ncph.check_assumptions(YX,p_value_threshold=0.05)\n\nThe ``p_value_threshold`` is set at 0.05. Even under the null hypothesis of no violations, some\ncovariates will be below the threshold by chance. This is compounded when there are many covariates.\nSimilarly, when there are lots of observations, even minor deviances from the proportional hazard\nassumption will be flagged.\n\nWith that in mind, it's best to use a combination of statistical tests and visual tests to determine\nthe most serious violations. Produce visual plots using ``check_assumptions(..., show_plots=True)``\nand looking for non-constant lines. See link [A] below for a full example.\n\n\n\n\n\n\n\n\n\nnull_distribution\nchi squared\n\n\ndegrees_of_freedom\n1\n\n\nmodel\n&lt;lifelines.CoxPHFitter: fitted with 103 total ...\n\n\ntest_name\nproportional_hazard_test\n\n\n\n\n\n\n\n\ntest_statistic\np\n-log2(p)\n\n\n\n\nC(surgery)[1]\nkm\n4.01\n0.05\n4.47\n\n\nrank\n3.74\n0.05\n4.23\n\n\nage\nkm\n1.18\n0.28\n1.86\n\n\nrank\n1.06\n0.30\n1.72\n\n\nyear\nkm\n2.07\n0.15\n2.73\n\n\nrank\n2.08\n0.15\n2.75\n\n\n\n\n\n\n\n1. Variable 'C(surgery)[1]' failed the non-proportional test: p-value is 0.0452.\n\n   Advice: with so few unique values (only 2), you can include `strata=['C(surgery)[1]', ...]` in\nthe call in `.fit`. See documentation in link [E] below.\n\n---\n[A]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html\n[B]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Bin-variable-and-stratify-on-it\n[C]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Introduce-time-varying-covariates\n[D]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Modify-the-functional-form\n[E]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Stratification\n\n\n\n[]\n\n\nTest PH: Schoenfeld Méthode 2\n\nfrom lifelines.statistics import  proportional_hazard_test \nzph = proportional_hazard_test(cph, YX, time_transform='all')\nzph.print_summary()\n\n\n\n\n\n\n\nnull_distribution\nchi squared\n\n\ndegrees_of_freedom\n1\n\n\nmodel\n&lt;lifelines.CoxPHFitter: fitted with 103 total ...\n\n\ntest_name\nproportional_hazard_test\n\n\n\n\n\n\n\n\ntest_statistic\np\n-log2(p)\n\n\n\n\nC(surgery)[1]\nidentity\n5.54\n0.02\n5.75\n\n\nkm\n4.01\n0.05\n4.47\n\n\nlog\n3.69\n0.05\n4.19\n\n\nrank\n3.74\n0.05\n4.23\n\n\nage\nidentity\n1.61\n0.20\n2.29\n\n\nkm\n1.18\n0.28\n1.86\n\n\nlog\n0.61\n0.44\n1.20\n\n\nrank\n1.06\n0.30\n1.72\n\n\nyear\nidentity\n0.80\n0.37\n1.43\n\n\nkm\n2.07\n0.15\n2.73\n\n\nlog\n1.34\n0.25\n2.02\n\n\nrank\n2.08\n0.15\n2.75\n\n\n\n\n\nTest PH: intéraction\n\nfrom lifelines.utils import to_episodic_format\nfrom lifelines import CoxTimeVaryingFitter\n\nTransformation de la base YX\n\nlong = to_episodic_format(YX, duration_col='stime', event_col='died')\n\nCréation de la variable d’intéraction\n\nlong['surgery_t'] = long['C(surgery)[1]'] * long['stop']\n\nEstimation\n\nctv = CoxTimeVaryingFitter()\nctv.fit(long,\n        id_col='id',\n        event_col='died',\n        start_col='start',\n        stop_col='stop',)\nctv.print_summary(4)\n\n\n\n\n\n\n\nmodel\nlifelines.CoxTimeVaryingFitter\n\n\nevent col\n'died'\n\n\nnumber of subjects\n103\n\n\nnumber of periods\n31938\n\n\nnumber of events\n75\n\n\npartial log-likelihood\n-287.3290\n\n\ntime fit was run\n2023-08-21 11:46:43 UTC\n\n\n\n\n\n\n\ncoef\nexp(coef)\nse(coef)\ncoef lower 95%\ncoef upper 95%\nexp(coef) lower 95%\nexp(coef) upper 95%\ncmp to\nz\np\n-log2(p)\n\n\n\n\nC(surgery)[1]\n-1.7547\n0.1730\n0.6743\n-3.0764\n-0.4331\n0.0461\n0.6485\n0.0000\n-2.6022\n0.0093\n6.7542\n\n\nage\n0.0289\n1.0293\n0.0134\n0.0025\n0.0552\n1.0025\n1.0568\n0.0000\n2.1479\n0.0317\n4.9785\n\n\nyear\n-0.1231\n0.8842\n0.0668\n-0.2541\n0.0079\n0.7756\n1.0080\n0.0000\n-1.8415\n0.0656\n3.9312\n\n\nsurgery_t\n0.0022\n1.0022\n0.0011\n0.0001\n0.0044\n1.0001\n1.0044\n0.0000\n2.0239\n0.0430\n4.5402\n\n\n\n\n\n\n\n\n\nPartial AIC\n582.6581\n\n\nlog-likelihood ratio test\n21.5846 on 4 df\n\n\n-log2(p) of ll-ratio test\n12.0103\n\n\n\n\n\n\n\n\n\n\n18.2.3 Variable dynamique binaire"
  },
  {
    "objectID": "17-Python.html#modèle-à-temps-discret",
    "href": "17-Python.html#modèle-à-temps-discret",
    "title": "18  Python",
    "section": "18.3 Modèle à temps discret",
    "text": "18.3 Modèle à temps discret\n\n18.3.1 Ajustement continu\nModèle logistique estimé avec le paquet statsmodel. La fonction to_episodic_format de lifelines permet de mettre en forme la base.\nPour la durée, on utilisera ici la variable mois (regroupement de stime par intervalle de 30 jours).\n\nimport statsmodels.formula.api as smf #type R formule =&gt; ce qu'on utilisera#\nimport statsmodels.api as sm #type python#\n\nTransformation de la base en format long\n\ntd = pd.read_csv(\"https://raw.githubusercontent.com/mthevenin/analyse_duree/master/bases/transplantation.csv\")\ntd.drop(['id'], axis=1, inplace=True)\ntd['dur'] = td['mois']\ntd = to_episodic_format(td, duration_col='mois', event_col='died')\n\nEvaluation de l’ajustement avec des fonctions quadratiques\n\ntd['t2'] = td['stop']**2\ntd['t3'] = td['stop']**3\nfit1 = smf.glm(formula=  \"died ~ stop\", data=td, family=sm.families.Binomial()).fit()\nfit2 = smf.glm(formula=  \"died ~ stop + t2\", data=td, family=sm.families.Binomial()).fit()\nfit3 = smf.glm(formula=  \"died ~ stop + t2 + t3\", data=td, family=sm.families.Binomial()).fit()\n\nComparaison des AIC\n\nprint(\"AIC pour ajustement t1\")\nprint(fit1.aic)\nprint(\"AIC pour ajustement durée t1 + t2\")\nprint(fit2.aic)\nprint(\"AIC pour ajustement durée t1 + t2 + t3\")\nprint(fit3.aic)\n\nAIC pour ajustement t1\n504.5211512753311\nAIC pour ajustement durée t1 + t2\n492.11522432726747\nAIC pour ajustement durée t1 + t2 + t3\n486.50534103180416\n\n\nEstimation du modèle\n\ntdfit = smf.glm(formula=  \"died ~ stop + t2 + t3 + year + age + surgery\", data=td, family=sm.families.Binomial()).fit()\ntdfit.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ndied\nNo. Observations:\n1127\n\n\nModel:\nGLM\nDf Residuals:\n1120\n\n\nModel Family:\nBinomial\nDf Model:\n6\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-230.34\n\n\nDate:\nMon, 21 Aug 2023\nDeviance:\n460.67\n\n\nTime:\n13:46:50\nPearson chi2:\n1.30e+03\n\n\nNo. Iterations:\n7\nPseudo R-squ. (CS):\n0.07732\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n7.0827\n5.308\n1.334\n0.182\n-3.320\n17.486\n\n\nstop\n-0.3721\n0.082\n-4.516\n0.000\n-0.534\n-0.211\n\n\nt2\n0.0142\n0.005\n2.835\n0.005\n0.004\n0.024\n\n\nt3\n-0.0002\n7.85e-05\n-2.113\n0.035\n-0.000\n-1.2e-05\n\n\nyear\n-0.1327\n0.074\n-1.798\n0.072\n-0.277\n0.012\n\n\nage\n0.0333\n0.015\n2.270\n0.023\n0.005\n0.062\n\n\nsurgery\n-1.0109\n0.449\n-2.254\n0.024\n-1.890\n-0.132\n\n\n\n\n\n\n\n18.3.2 Ajustement discret\nCréation des intervalles pour l’exemple (quantile de la durée en mois)\n\ntd['ct4'] = pd.qcut(td['stop'],[0, .25, .5, .75, 1.]) \ntd['ct4'].value_counts(normalize=True)*100\ntd.ct4 = pd.Categorical(td.ct4)\ntd['ct4'] = td.ct4.cat.codes\n\nPour chaque individu, on conserve une seule observation par intervalle.\n\ntd2 = td \ntd2['t'] = td2['ct4']\ntd2 = td2.sort_values(['id', 'stop'])\ntd2 =  td2.groupby(['id','ct4']).last()\n\nEstimation\n\ntd2fit = smf.glm(formula=  \"died ~ C(t) +  year + age + surgery\", data=td2, family=sm.families.Binomial()).fit()\ntd2fit.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ndied\nNo. Observations:\n197\n\n\nModel:\nGLM\nDf Residuals:\n190\n\n\nModel Family:\nBinomial\nDf Model:\n6\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-111.24\n\n\nDate:\nMon, 21 Aug 2023\nDeviance:\n222.48\n\n\nTime:\n13:46:50\nPearson chi2:\n221.\n\n\nNo. Iterations:\n4\nPseudo R-squ. (CS):\n0.1808\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n12.4467\n6.654\n1.871\n0.061\n-0.594\n25.488\n\n\nC(t)[T.1]\n-1.0334\n0.419\n-2.467\n0.014\n-1.854\n-0.212\n\n\nC(t)[T.2]\n-1.6152\n0.545\n-2.965\n0.003\n-2.683\n-0.547\n\n\nC(t)[T.3]\n-0.4789\n0.599\n-0.799\n0.424\n-1.654\n0.696\n\n\nyear\n-0.2032\n0.093\n-2.181\n0.029\n-0.386\n-0.021\n\n\nage\n0.0469\n0.018\n2.533\n0.011\n0.011\n0.083\n\n\nsurgery\n-1.1102\n0.503\n-2.209\n0.027\n-2.095\n-0.125"
  },
  {
    "objectID": "17-Python.html#modèle-paramétrique-de-type-aft",
    "href": "17-Python.html#modèle-paramétrique-de-type-aft",
    "title": "18  Python",
    "section": "18.4 Modèle paramétrique de type AFT",
    "text": "18.4 Modèle paramétrique de type AFT\n\nfrom lifelines import WeibullAFTFitter, LogLogisticAFTFitter\n\nWeibull\n\naftw = WeibullAFTFitter()\naftw.fit(YX, duration_col='stime', event_col='died')\naftw.print_summary()\n\n\n\n\n\n\n\nmodel\nlifelines.WeibullAFTFitter\n\n\nduration col\n'stime'\n\n\nevent col\n'died'\n\n\nnumber of observations\n103\n\n\nnumber of events observed\n75\n\n\nlog-likelihood\n-488.17\n\n\ntime fit was run\n2023-08-21 11:46:50 UTC\n\n\n\n\n\n\n\n\ncoef\nexp(coef)\nse(coef)\ncoef lower 95%\ncoef upper 95%\nexp(coef) lower 95%\nexp(coef) upper 95%\ncmp to\nz\np\n-log2(p)\n\n\n\n\nlambda_\nC(surgery)[1]\n1.97\n7.17\n0.78\n0.44\n3.50\n1.56\n33.05\n0.00\n2.53\n0.01\n6.45\n\n\nage\n-0.06\n0.94\n0.02\n-0.11\n-0.01\n0.90\n0.99\n0.00\n-2.49\n0.01\n6.28\n\n\nyear\n0.16\n1.18\n0.12\n-0.08\n0.40\n0.93\n1.49\n0.00\n1.33\n0.18\n2.44\n\n\nIntercept\n-3.02\n0.05\n8.73\n-20.13\n14.09\n0.00\n1.31e+06\n0.00\n-0.35\n0.73\n0.46\n\n\nrho_\nIntercept\n-0.59\n0.56\n0.09\n-0.77\n-0.41\n0.46\n0.67\n0.00\n-6.33\n&lt;0.005\n31.93\n\n\n\n\n\n\n\n\n\nConcordance\n0.65\n\n\nAIC\n986.34\n\n\nlog-likelihood ratio test\n18.87 on 3 df\n\n\n-log2(p) of ll-ratio test\n11.75\n\n\n\n\n\n\n\n\nLoglogistique\n\naftl = LogLogisticAFTFitter()\naftl.fit(YX, duration_col='stime', event_col='died')\naftl.print_summary()\n\n\n\n\n\n\n\nmodel\nlifelines.LogLogisticAFTFitter\n\n\nduration col\n'stime'\n\n\nevent col\n'died'\n\n\nnumber of observations\n103\n\n\nnumber of events observed\n75\n\n\nlog-likelihood\n-482.58\n\n\ntime fit was run\n2023-08-21 11:46:51 UTC\n\n\n\n\n\n\n\n\ncoef\nexp(coef)\nse(coef)\ncoef lower 95%\ncoef upper 95%\nexp(coef) lower 95%\nexp(coef) upper 95%\ncmp to\nz\np\n-log2(p)\n\n\n\n\nalpha_\nC(surgery)[1]\n2.27\n9.70\n0.69\n0.92\n3.63\n2.50\n37.56\n0.00\n3.29\n&lt;0.005\n9.95\n\n\nage\n-0.04\n0.96\n0.02\n-0.08\n-0.00\n0.92\n1.00\n0.00\n-2.01\n0.04\n4.48\n\n\nyear\n0.24\n1.27\n0.12\n0.01\n0.47\n1.01\n1.60\n0.00\n2.06\n0.04\n4.66\n\n\nIntercept\n-10.43\n0.00\n8.34\n-26.77\n5.92\n0.00\n372.19\n0.00\n-1.25\n0.21\n2.24\n\n\nbeta_\nIntercept\n-0.18\n0.84\n0.10\n-0.37\n0.01\n0.69\n1.01\n0.00\n-1.86\n0.06\n3.99\n\n\n\n\n\n\n\n\n\nConcordance\n0.66\n\n\nAIC\n975.16\n\n\nlog-likelihood ratio test\n21.69 on 3 df\n\n\n-log2(p) of ll-ratio test\n13.69"
  },
  {
    "objectID": "17-Python.html#kaplan-meier",
    "href": "17-Python.html#kaplan-meier",
    "title": "18  Python",
    "section": "19.1 Kaplan-Meier",
    "text": "19.1 Kaplan-Meier\n\nkm = sm.SurvfuncRight(trans[\"stime\"], trans[\"died\"])\nkm.summary()\n\n\n\n\n\n\n\n\nSurv prob\nSurv prob SE\nnum at risk\nnum events\n\n\nTime\n\n\n\n\n\n\n\n\n1\n0.990291\n0.009661\n103\n1.0\n\n\n2\n0.961165\n0.019037\n102\n3.0\n\n\n3\n0.932039\n0.024799\n99\n3.0\n\n\n5\n0.912621\n0.027825\n96\n2.0\n\n\n6\n0.893204\n0.030432\n94\n2.0\n\n\n...\n...\n...\n...\n...\n\n\n852\n0.250655\n0.048731\n14\n1.0\n\n\n979\n0.227868\n0.049341\n11\n1.0\n\n\n995\n0.205081\n0.049390\n10\n1.0\n\n\n1032\n0.182295\n0.048877\n9\n1.0\n\n\n1386\n0.151912\n0.049277\n6\n1.0\n\n\n\n\n62 rows × 4 columns\n\n\n\nLes test du log-rank sont disponibles avec la fonction survdiff (nom idem R). Au niveau graphique, la programmation semble un peu lourde et mériterait d’être simplifiée (donc non traitée).\nComparaison de S(t) à partir des tests du log-rank\n Résultat: (statistique de test, p-value)\nTest non pondéré\n\nsm.duration.survdiff(trans.stime, trans.died, trans.surgery)\n\n(6.5900123232343875, 0.010255246157888975)\n\n\nBreslow\n\nsm.duration.survdiff(trans.stime, trans.died, trans.surgery, weight_type='gb')\n\n(8.989753779902493, 0.0027149757927903417)\n\n\nTarone-Ware\n\nsm.duration.survdiff(trans.stime, trans.died, trans.surgery, weight_type='tw')\n\n(8.462352726451392, 0.0036257256194570653)"
  },
  {
    "objectID": "17-Python.html#modèle-de-cox",
    "href": "17-Python.html#modèle-de-cox",
    "title": "18  Python",
    "section": "19.2 Modèle de Cox",
    "text": "19.2 Modèle de Cox\n\nmod = smf.phreg(\"stime ~  year + age + surgery \",trans, status='died', ties=\"efron\")\nrslt = mod.fit()\nprint(rslt.summary())\n\n                       Results: PHReg\n=============================================================\nModel:                    PH Reg       Sample size:       103\nDependent variable:       stime        Num. events:       75 \nTies:                     Efron                              \n-------------------------------------------------------------\n         log HR log HR SE   HR      t    P&gt;|t|  [0.025 0.975]\n-------------------------------------------------------------\nyear    -0.1196    0.0673 0.8872 -1.7765 0.0757 0.7775 1.0124\nage      0.0296    0.0135 1.0300  2.1872 0.0287 1.0031 1.0577\nsurgery -0.9873    0.4363 0.3726 -2.2632 0.0236 0.1584 0.8761\n=============================================================\nConfidence intervals are for the hazard ratios"
  }
]