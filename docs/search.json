[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction à l’analyse biographique des durées",
    "section": "",
    "text": "1 Présentation - Bibliographie - Outils"
  },
  {
    "objectID": "index.html#le-support",
    "href": "index.html#le-support",
    "title": "Introduction à l’analyse biographique des durées",
    "section": "Le support",
    "text": "Le support\nMISE A JOUR DU SUPPORT EN COURS\n\nLe support est maintenant intégralement disponible en format pdf en cliquant sur l’icône au dessus de la barre de recherche.\nD’ici la fin septembre 2023:\n\nUne introduction du support à cet endroit.\nQuelques ajouts:\n\nUn nouveau chapitre sur la manipulation des données [OK].\nPar l’exemple, un warning assez soutenu sur des utilisations douteuses de données prospectives (on visera l’EDP - Echantillon Démographique Permanent).\nJ’avais retiré la courte présentation des fonctions de lien probit et surtout complémentaire-loglog dans les modèles à durée discrète….maivaise idée que j’ai eu (voir point précédent).\nJe ne pense pas avoir le temps mais on sait jamais: une première présentation des modèles à pseudo observations dans la section annexe (sinon en fin d’année ou à l’été prochain).\n\n\n\nVersion pdf:\n\nDernière maj: le 21 août 2023\nPour le chapitre programmation, seulement la section dédiée à R a été ajoutée. Pour les 3 autres applications, seulement la version HTML."
  },
  {
    "objectID": "index.html#bibliographie-vf",
    "href": "index.html#bibliographie-vf",
    "title": "Introduction à l’analyse biographique des durées",
    "section": "Bibliographie vf",
    "text": "Bibliographie vf\nLes éléments bibliographiques qui figurent ci-dessous proviennent du champ des sciences sociales. Elle est courte, mais efficace. Quelle que soit la langue, le nombre de cours ou support sont très nombreux en médecine, qui est ici l’espace privilégié de l’ingénierie méthodologique. On trouve également de (trop) nombreux tutoriels à dominante mise en pratique avec R.\nAccès en ligne\n\nCours Gilbert Colletaz (Université d’Orléans - Master d’économétrie).\n\nLe cours est mis à jour tous les ans, applications uniquement avec Sas.\nDernière version 2020: lien\n\nDocument de travail de Simon Quantin (Insee).\n\nCouvre l’ensemble des techniques de base d’analyse des durées en durée dite continue. Il propose surement la meilleure introduction en langue française à la problématique de la fragilité.\nApplication en R seulement (attention au passage de la v3 du package survival)\n2019 - pas de mise à jour: lien\n\n\nOuvrage de référence en démographie:\nL’analyse démographique des biographies de Daniel Courgeau et Eva Lelièvre (Edition de l’Ined - 1989)."
  },
  {
    "objectID": "index.html#outils",
    "href": "index.html#outils",
    "title": "Introduction à l’analyse biographique des durées",
    "section": "Outils",
    "text": "Outils\n\nSupport réalisé sous Rstudio avec l’outil d’édition Quarto\nLangages utilisés pour la partie programmation:\n\nR\nStata v18\nSas\nPython"
  },
  {
    "objectID": "index.html#bibliographie",
    "href": "index.html#bibliographie",
    "title": "Introduction à l’analyse biographique des durées",
    "section": "Bibliographie",
    "text": "Bibliographie\nLes éléments bibliographiques qui figurent ci-dessous proviennent du champ des sciences sociales. Elle est courte, mais efficace. Quelle que soit la langue, le nombre de cours ou support sont très nombreux en médecine, qui est ici l’espace privilégié de l’ingénierie méthodologique. On trouve également de (trop) nombreux tutoriels à dominante mise en pratique avec R.\nAccès en ligne\n\nCours Gilbert Colletaz (Université d’Orléans - Master d’économétrie).\n\nLe cours est mis à jour tous les ans, applications uniquement avec Sas.\nDernière version 2020: lien\n\nDocument de travail de Simon Quantin (Insee).\n\nCouvre l’ensemble des techniques de base d’analyse des durées en durée dite continue. Il propose surement la meilleure introduction en langue française à la problématique de la fragilité.\nApplication en R seulement (attention au passage de la v3 du package survival)\n2019 - pas de mise à jour: lien\n\nLes notes de cours de German Rodriguez (en)\n\nDémographe à l’université de Princeton.\nLes dernières mises à jour doivent dater de 2017-2018: lien\n\n\nOuvrage de référence en démographie:\n\nL’analyse démographique des biographies de Daniel Courgeau et Eva Lelièvre (Edition de l’Ined - 1989). Malheureusement cet ouvrage ne dispose pas de version epub ou pdf disponible en ligne 1."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introduction à l’analyse biographique des durées",
    "section": "",
    "text": "Pour les résident.e.s du campus Condorcet, l’ouvrage est disponible au GED [lien]↩︎"
  },
  {
    "objectID": "10-manipulation.html#calcul-des-variables-danalyses",
    "href": "10-manipulation.html#calcul-des-variables-danalyses",
    "title": "11  Eléments de mise en forme des données",
    "section": "11.1 Calcul des variables d’analyses",
    "text": "11.1 Calcul des variables d’analyses\nOn partira de la base individus-séquences suivante:\n\n\nCode\ndf = data.frame(id  =  c(1, 1, 1, 2),\n                deb =  c(2020, 2023, 2024, 2022),\n                fin =  c(2021, 2024, 2025, NA), \n                  x =  c(1,2,1,2))\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\nx\n\n\n\n\n1\n2020\n2021\n1\n\n\n1\n2023\n2024\n2\n\n\n1\n2024\n2025\n1\n\n\n2\n2022\nNA\n2\n\n\n\n\n\nOn supposera que l’année de collecte, pour toutes les observations, est 2025 2.\nSi cela n’est pas donné dans le module biographique, il peut être intéressant de construire les numéros de séquences des trajectoires.\n\n\nCode\ndf$nseq = 1 \ndf = df %&gt;% group_by(id) %&gt;% mutate(nseq = cumsum(nseq))  \n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\n\n\n\n\n1\n2020\n2021\n1\n1\n\n\n1\n2023\n2024\n2\n2\n\n\n1\n2024\n2025\n1\n3\n\n\n2\n2022\nNA\n2\n1\n\n\n\n\n\nExemple 1 : durée de séjour de la première séquence observée\nSupposons que x traduit un type de relation/union, par exemple x=1 est une relation non cohabitante et x=2 est une relation cohabitante. On s’intéresse à la durée de la première relation, sans distinction entre 1 et 2. Il suffit de séléctionner la première séquence.\n\n\nCode\ndf = filter(df, nseq==1)\n\n\nLa variable de fin va permettre de repérer les informations censurées, et de générer la variable d’évènement. A ce niveau il est donc important de ne pas encore remplacer la date de censure par sa valeur.\n\nSi fin est une valeur manquante: observation censurée.\nSi fin est une valeur renseignée: occurence de l’évènement.\n\n\n\nCode\ndf$e = ifelse(is.na(df$fin), 0,1)\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\ne\n\n\n\n\n1\n2020\n2021\n1\n1\n1\n\n\n2\n2022\nNA\n2\n1\n0\n\n\n\n\n\nPour la variable de durée 3, une repéré les observations censurées, elle est calculée directement avec les variables fin et deb.\n\n\nCode\ndf$dur = ifelse(df$e==1, df$fin - df$deb + 1, 2025 - df$deb + 1)\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\ne\ndur\n\n\n\n\n1\n2020\n2021\n1\n1\n1\n2\n\n\n2\n2022\nNA\n2\n1\n0\n4\n\n\n\n\n\nExemple 2 : changement de métrique temporelle\nToujours avec le même exemple, mais en ajoutant une observation, supposons que l’on dispose également de l’information sur les mois. Sur les mois où l’évènement à eu lieu, mais également sur les mois où l’enquête a été réalisée.\n\n\nCode\ndf2 = data.frame(id  = c(1, 1, 1, 2,3),\n                deb  = c(2020, 2023, 2024, 2022, 2021),\n                debm = c(2,5,3,10,9),\n                fin  = c(2021, 2024, 2025, NA,2021), \n                finm = c(4,2,12,NA,11), \n                x    = c(1,2,1,2,1),\n                enq  = c(2025,2025,2025,2025,2025),\n                enqm = c(4,4,4,5,4))\n\ndf2$nseq = 1 \ndf2 = df2 %&gt;% group_by(id) %&gt;% mutate(nseq = cumsum(nseq))  \n\nkable(df2)\n\n\n\n\n\nid\ndeb\ndebm\nfin\nfinm\nx\nenq\nenqm\nnseq\n\n\n\n\n1\n2020\n2\n2021\n4\n1\n2025\n4\n1\n\n\n1\n2023\n5\n2024\n2\n2\n2025\n4\n2\n\n\n1\n2024\n3\n2025\n12\n1\n2025\n4\n3\n\n\n2\n2022\n10\nNA\nNA\n2\n2025\n5\n1\n\n\n3\n2021\n9\n2021\n11\n1\n2025\n4\n1\n\n\n\n\n\nOn remarque que la nouvelle observation (id=3) a connu l’évènement, ici la fin de la relation, la même année qu’au début d’exposition (le début de la relation)…. mais au bout de 2,6,11 mois???? Commeon dispose de l’information sur les mois de début et de fin cela peut être intéressant de l’exploite. De la même manière si l’enquête a été réalisée la même année, les entretiens n’ont pas eu lieu le même mois. On aura besoin de cette information pour les observations censurées.\nDe nouveau on sélectionne la première séquence, et pour la lisibilité de la base on retire les informations qui ne seront pas ou plus exploitées (nseq, x).\n\n\nCode\ndf2 = filter(df2,nseq==1)\ndf2 = select(df2, -c(x,nseq))\n\nkable(df2)\n\n\n\n\n\nid\ndeb\ndebm\nfin\nfinm\nenq\nenqm\n\n\n\n\n1\n2020\n2\n2021\n4\n2025\n4\n\n\n2\n2022\n10\nNA\nNA\n2025\n5\n\n\n3\n2021\n9\n2021\n11\n2025\n4\n\n\n\n\n\nOn génère la variable censure/évènement (toujours à faire avant la variable de durée) de la même manière que pour l’exemple 1.\n\n\nCode\ndf2$e = ifelse(is.na(df2$fin), 0, 1)\n\nkable(df2)\n\n\n\n\n\nid\ndeb\ndebm\nfin\nfinm\nenq\nenqm\ne\n\n\n\n\n1\n2020\n2\n2021\n4\n2025\n4\n1\n\n\n2\n2022\n10\nNA\nNA\n2025\n5\n0\n\n\n3\n2021\n9\n2021\n11\n2025\n4\n1\n\n\n\n\n\nPour la variable de durée, le principe est de multiplié par 12 la différence entre l’année de fin et l’année de début et d’ajouter la différence entre le mois de fin et le mois de début.\nPour les observations censurées, ici l’année de fin est identique mais les mois varient. En terme de programmation, surtout si avec R on utilise ifelse, il est préférable d’y aller doucement en créant une durée pour les observations qui ont connu l’évènement et une durée pour les observations censurées. Puis de regrouper les deux cas. C’est ce qui est fait dans le code qui suit.\nDurée selon les valeurs de e:\n\n\nCode\ndf2$dur1 = ifelse(df2$e==1, 12*(df2$fin - df2$deb) + (df2$finm - df2$debm),  0) \ndf2$dur0 = ifelse(df2$e==0, 12*(2025 - df2$deb)    + (df2$enqm  - df2$debm), 0) \n\nkable(df2)\n\n\n\n\n\nid\ndeb\ndebm\nfin\nfinm\nenq\nenqm\ne\ndur1\ndur0\n\n\n\n\n1\n2020\n2\n2021\n4\n2025\n4\n1\n14\n0\n\n\n2\n2022\n10\nNA\nNA\n2025\n5\n0\n0\n31\n\n\n3\n2021\n9\n2021\n11\n2025\n4\n1\n2\n0\n\n\n\n\n\nOn regroupe par simple sommation (le else étant 0).\n\n\nCode\ndf2$dur  = df2$dur1 + df2$dur0\n\ndf2 = select(df2, -c(dur1,dur0))\n\nkable(df2)\n\n\n\n\n\nid\ndeb\ndebm\nfin\nfinm\nenq\nenqm\ne\ndur\n\n\n\n\n1\n2020\n2\n2021\n4\n2025\n4\n1\n14\n\n\n2\n2022\n10\nNA\nNA\n2025\n5\n0\n31\n\n\n3\n2021\n9\n2021\n11\n2025\n4\n1\n2\n\n\n\n\n\nOn dispose ainsi des éléments nécessaire pour faire une analyse de durée avec une métrique mensuelle 4.\nExemple 3 : importation d’un début d’expositon externe\nOn repart de la première base\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\n\n\n\n\n1\n2020\n2021\n1\n1\n\n\n1\n2023\n2024\n2\n2\n\n\n1\n2024\n2025\n1\n3\n\n\n2\n2022\nNA\n2\n1\n\n\n\n\n\nOn suppose maintenant que x traduit des situations sur le marché du travail. Par exemple x=1 est un emploi en CDD et x=2 un emploi en CDI. On s’intéresse à la durée entre la fin des études et le premier emploi, quel que soit sont type.\n\nOn ne dispose pas ici de toutes l’information pour calculer la durée, soit la fin des études. Elle peut être donnée dans une base classique regroupant l’ensemble des caractéristiques individuelles de type fixe (année de naissance, sexe…).\nComme on s’intéresse à la durée de recherche du premier emploi, dans le module biographique la date de début va devenir la date de fin.\nPour les observations présente dans la base biographique, il n’y a pas de censure à droite. Mais si on regarde le fichier des caractéristiques générales, fixe:\n\n\n\nCode\netude = data.frame(id = c(1,2,3), fin_etude = c(2020,2021,2023))\nkable(etude)\n\n\n\n\n\nid\nfin_etude\n\n\n\n\n1\n2020\n\n\n2\n2021\n\n\n3\n2023\n\n\n\n\n\nUne nouvelle observation (id=3) apparaît. Au moment de l’enquête, elle n’a pas (encore) trouvé un emploi depuis la fin de ces études. On a donc une observation qui sera censurée.\n\n\n\n\n\n\nNote\n\n\n\nCertaines bases biographiques peuvent être structurées avec des trajectoires strictement continue, l’année (l’âge) de fin étant l’année (l’âge) de début de la trajectoire suivante. Dans ce cas, l’information serait immédiatement disponible, avec la présence d’un nombre de séquences plus important dans la base.\n\n\nOn va devoir:\n\nSélectionner la première sequence d’emploi dans la base df (variable nseq).\nLa fusionner avec la base étude.\n\nAvant la fusion, on peut conserver seulement les informations nécessaires (id, deb). La variable deb va changer également de statut en devenant l’année de fin de la période de recherche d’emploi.\n\n\nCode\ndf = filter(df, nseq==1)\ndf = select(df, -c(fin,x,nseq))\n\ndf = rename(df, fin = deb)\nkable(df)\n\n\n\n\n\nid\nfin\n\n\n\n\n1\n2020\n\n\n2\n2022\n\n\n\n\n\nAprès la fusion:\n\n\nCode\ndf = full_join(etude, df,  by = c('id'))\n\ndf = rename(df, deb = fin_etude)\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\n\n\n\n\n1\n2020\n2020\n\n\n2\n2021\n2022\n\n\n3\n2023\nNA\n\n\n\n\n\nOn a toutes les informations pour générer la variable censure/évènement et la variable de durée:\n\n\nCode\ndf$e = ifelse(is.na(df$fin),0,1)\n\ndf$dur = ifelse(df$e, df$fin - df$deb + 1, 2025 - df$deb + 1)\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ne\ndur\n\n\n\n\n1\n2020\n2020\n1\n1\n\n\n2\n2021\n2022\n1\n2\n\n\n3\n2023\nNA\n0\n3"
  },
  {
    "objectID": "10-manipulation.html#appariement-de-modules-biographiques",
    "href": "10-manipulation.html#appariement-de-modules-biographiques",
    "title": "11  Eléments de mise en forme des données",
    "section": "11.2 Appariement de modules biographiques",
    "text": "11.2 Appariement de modules biographiques\nOn repart de la première base, avec les numéros de séquence.\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\n\n\n\n\n1\n2020\n2021\n1\n1\n\n\n1\n2023\n2024\n2\n2\n\n\n1\n2024\n2025\n1\n3\n\n\n2\n2022\nNA\n2\n1\n\n\n\n\n\n\n11.2.1 Mise en forme d’une base\nPour apparier des informations de plusieurs modules biographiques, on doit transformer les bases en format individus-séquences en format individus-périodes (ici individus années).\n\nEtape 1: allongement sur chaque séquence après avoir générées leur durée\nEtape 2: générer une variable de période (année) sur chaque ligne. Elle servira pour l’appariement.\n\nPourquoi ne pas utiliser la simple différence entre la fin et le début ?\nDurée (fin - début) et allongement de la base:\nOn ne génère pas des variables d’analyse, on aurait besoin de l’information sur l’année de l’enquête pour les informations censurées.\n\n\nCode\ndf$fin[is.na(df$fin)] = 2025\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\n\n\n\n\n1\n2020\n2021\n1\n1\n\n\n1\n2023\n2024\n2\n2\n\n\n1\n2024\n2025\n1\n3\n\n\n2\n2022\n2025\n2\n1\n\n\n\n\n\nAllongement de la base:\n\n\nCode\ndf1 = df\ndf1$dur1 = df1$fin - df1$deb\n\ndf1$dur1b = df1$dur1 # uncount supprime la variable d'origine \ndf1 = uncount(df1,dur1b)\n\nkable(df1)\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\ndur1\n\n\n\n\n1\n2020\n2021\n1\n1\n1\n\n\n1\n2023\n2024\n2\n2\n1\n\n\n1\n2024\n2025\n1\n3\n1\n\n\n2\n2022\n2025\n2\n1\n3\n\n\n2\n2022\n2025\n2\n1\n3\n\n\n2\n2022\n2025\n2\n1\n3\n\n\n\n\n\nPour générer la variable période (année), on a besoin d’un compteur qui sera associé à la variable deb. On doit bien contrôler l’opération par identifiant et numéro de séquence.\n\n\nCode\ndf1$c = 1\ndf1 = df1 %&gt;% group_by(id,nseq) %&gt;% mutate(year = deb  + cumsum(c)) \n\nkable(df1)\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\ndur1\nc\nyear\n\n\n\n\n1\n2020\n2021\n1\n1\n1\n1\n2021\n\n\n1\n2023\n2024\n2\n2\n1\n1\n2024\n\n\n1\n2024\n2025\n1\n3\n1\n1\n2025\n\n\n2\n2022\n2025\n2\n1\n3\n1\n2023\n\n\n2\n2022\n2025\n2\n1\n3\n1\n2024\n\n\n2\n2022\n2025\n2\n1\n3\n1\n2025\n\n\n\n\n\nProblème: les années de début ne sont pas correncte: 2021 au lieu de 2020 pour la première séquence de id=1 par exemple.\n\n\n\n\n\n\nImportant\n\n\n\nOn doit donc impérativement augmenter la différence entre la fin et le début par +1 pour que l’ensemble des périodes (années) soit couvertes.\n\n\nOn reprend donc les opérations précédentes mais avec durée = fin - debut + 1\n\nAllongement de la base avec durée augmentée\n\n\n\nCode\ndf2 = df\ndf2$dur2 = df2$fin - df2$deb + 1\n\ndf2$dur2b = df2$dur2 # uncount supprime la variable d'origine \ndf2 = uncount(df2,dur2b)\n\nkable(df2)\n\n\n\n\n\nid\ndeb\nfin\nx\nnseq\ndur2\n\n\n\n\n1\n2020\n2021\n1\n1\n2\n\n\n1\n2020\n2021\n1\n1\n2\n\n\n1\n2023\n2024\n2\n2\n2\n\n\n1\n2023\n2024\n2\n2\n2\n\n\n1\n2024\n2025\n1\n3\n2\n\n\n1\n2024\n2025\n1\n3\n2\n\n\n2\n2022\n2025\n2\n1\n4\n\n\n2\n2022\n2025\n2\n1\n4\n\n\n2\n2022\n2025\n2\n1\n4\n\n\n2\n2022\n2025\n2\n1\n4\n\n\n\n\n\n\nCréation de la variable year: sur chaque individus-séquences, la somme entre le compteur et l’année de début doit être réduite de 11.\n\n\n\nCode\ndf2$c = 1\ndf2 = df2 %&gt;% group_by(id,nseq) %&gt;% mutate(year = deb  + cumsum(c) - 1)\n\ndf2 = select(df2, -c(deb,fin,dur2))\n\n\nkable(df2)\n\n\n\n\n\nid\nx\nnseq\nc\nyear\n\n\n\n\n1\n1\n1\n1\n2020\n\n\n1\n1\n1\n1\n2021\n\n\n1\n2\n2\n1\n2023\n\n\n1\n2\n2\n1\n2024\n\n\n1\n1\n3\n1\n2024\n\n\n1\n1\n3\n1\n2025\n\n\n2\n2\n1\n1\n2022\n\n\n2\n2\n1\n1\n2023\n\n\n2\n2\n1\n1\n2024\n\n\n2\n2\n1\n1\n2025\n\n\n\n\n\nLes années sont toutes couvertes….mais un peu trop. En effet, lorsque les trajectoires sont continues soit lorsque l’année de fin d’une séquence est identique à l’année de début de la suivante, les années vont être doublonnées. On doit dont supprimer ce doublon.\n\nSuppression des doublons des trajectoires continues.\n\nDe nouveaux on doit faire un choix, soit on priviligie l’année de fin, soit on privilégie l’année de début. Les applications ont des fonctions qui permettent de supprimer les doublons5. On peut le faire manuellement en regardant pour chaque personnes-années le nombre de doublon. Cela se fait facilement à l’aide d’un compteur, ici la variable nyear.\n\n\nCode\ndf2 = df2 %&gt;% group_by(id,year) %&gt;% mutate(nyear = cumsum(c))\n\nkable(df2)\n\n\n\n\n\nid\nx\nnseq\nc\nyear\nnyear\n\n\n\n\n1\n1\n1\n1\n2020\n1\n\n\n1\n1\n1\n1\n2021\n1\n\n\n1\n2\n2\n1\n2023\n1\n\n\n1\n2\n2\n1\n2024\n1\n\n\n1\n1\n3\n1\n2024\n2\n\n\n1\n1\n3\n1\n2025\n1\n\n\n2\n2\n1\n1\n2022\n1\n\n\n2\n2\n1\n1\n2023\n1\n\n\n2\n2\n1\n1\n2024\n1\n\n\n2\n2\n1\n1\n2025\n1\n\n\n\n\n\nSi on souhaite garder l’année de fin on filtre les observations en conservant celles dont nyear=1. Si on souhaite privilégier les années de début on foltre les observations en conservant celles dont nyear=2. Si on souhaite conserver les années de fin de séquence:\n\n\nCode\ndf2 = filter(df2, nyear==1)\n\ndf2 = select(df2, -c(nseq,c,nyear))\n\nkable(df2)\n\n\n\n\n\nid\nx\nyear\n\n\n\n\n1\n1\n2020\n\n\n1\n1\n2021\n\n\n1\n2\n2023\n\n\n1\n2\n2024\n\n\n1\n1\n2025\n\n\n2\n2\n2022\n\n\n2\n2\n2023\n\n\n2\n2\n2024\n\n\n2\n2\n2025\n\n\n\n\n\n\n\n\n\n\n\nEn résumé\n\n\n\n\nA la date (année/âge) de censure remplacer la valeur manquante par sa valeur. Si ultérieurement on a besoin de garder l’information sur la censure - valeur manquante - , on peut générer une variable mirroir de fin.\nSur chaque séquence calculer la durée avec une augmentation de +1.\nCréer une variable période (année) sur chaque ligne. Elle servira à définir la clé d’appariement.\nSupprimer les doublons sur les transition continue \\(fin_t = debut_{t+1}\\).\n\n\n\n\n\n11.2.2 Fusion des informations biographiques\n\n11.2.2.1 Fusion avec l’ensemble des périodes observables\nPour commencer par un exemple plutôt simple, on note que pour id=1 l’année 2022 n’est pas renseignée (trajectoire non continue). Si on reprend un exemple précédent (relations de couple), cette année pourrait être identifiée comme une période sans relation. Une façon simple de boucher ce type “trous”, est d’utiliser les années de naissances des individus, et de créer une base individus-périodes qui couvre toutes les années de vie de l’individu jusqu’à l’enquête. On remontera jusque là, mais on va par exemple considérer que pour id=1 et id=2 ce début de tout est en 2018.\n\n\nCode\ndftout = data.frame(id  =  c(1, 2),\n                    t0  =  c(2018, 2018))\n\nkable(dftout)    \n\n\n\n\n\nid\nt0\n\n\n\n\n1\n2018\n\n\n2\n2018\n\n\n\n\n\n\nOn ajoute l’information sur l’année de l’enquête (2025).\nOn génère la durée\nOn allonge la base\nOn génère la variable année sur chaque ligne (on contrôle seulement sur id)\n\n\n\nCode\ndftout$tmax = 2025\n\ndftout$dur  = dftout$tmax - dftout$t0 + 1\n\ndftout = uncount(dftout,dur)\n\n\ndftout$c = 1\ndftout = dftout %&gt;% group_by(id) %&gt;% mutate(year = t0  + cumsum(c) - 1)\n\ndftout = select(dftout, -c(t0,tmax,c))\n\nkable(dftout)    \n\n\n\n\n\nid\nyear\n\n\n\n\n1\n2018\n\n\n1\n2019\n\n\n1\n2020\n\n\n1\n2021\n\n\n1\n2022\n\n\n1\n2023\n\n\n1\n2024\n\n\n1\n2025\n\n\n2\n2018\n\n\n2\n2019\n\n\n2\n2020\n\n\n2\n2021\n\n\n2\n2022\n\n\n2\n2023\n\n\n2\n2024\n\n\n2\n2025\n\n\n\n\n\nOn peut maintenant apparier cette couverture de toutes les années de vie jusqu’à l’enquête à la base biographique:\n\n\nCode\ndf2 = full_join(df2, dftout, by = c(\"id\",\"year\"))\n\ndf2 = arrange(df2, id, year)\nkable(df2)    \n\n\n\n\n\nid\nx\nyear\n\n\n\n\n1\nNA\n2018\n\n\n1\nNA\n2019\n\n\n1\n1\n2020\n\n\n1\n1\n2021\n\n\n1\nNA\n2022\n\n\n1\n2\n2023\n\n\n1\n2\n2024\n\n\n1\n1\n2025\n\n\n2\nNA\n2018\n\n\n2\nNA\n2019\n\n\n2\nNA\n2020\n\n\n2\nNA\n2021\n\n\n2\n2\n2022\n\n\n2\n2\n2023\n\n\n2\n2\n2024\n\n\n2\n2\n2025\n\n\n\n\n\nPour supprimer les informations qui précèdent la première séquence de la biographie, on peut générer un compteur sur la variable x après avoir remplacer ses valeurs manquantes par des 0. On gardera les lignes pour lesquels ce compteur est supérieur à 1.\n\n\nCode\ndf2$x[is.na(df2$x)] = 0\n\ndf2 = df2 %&gt;% group_by(id) %&gt;% mutate(nx = cumsum(x))\n\nkable(df2)    \n\n\n\n\n\nid\nx\nyear\nnx\n\n\n\n\n1\n0\n2018\n0\n\n\n1\n0\n2019\n0\n\n\n1\n1\n2020\n1\n\n\n1\n1\n2021\n2\n\n\n1\n0\n2022\n2\n\n\n1\n2\n2023\n4\n\n\n1\n2\n2024\n6\n\n\n1\n1\n2025\n7\n\n\n2\n0\n2018\n0\n\n\n2\n0\n2019\n0\n\n\n2\n0\n2020\n0\n\n\n2\n0\n2021\n0\n\n\n2\n2\n2022\n2\n\n\n2\n2\n2023\n4\n\n\n2\n2\n2024\n6\n\n\n2\n2\n2025\n8\n\n\n\n\n\nOn supprime les lignes lorsque nx=0.\n\n\nCode\ndf2 = filter(df2, nx&gt;0)\n\ndf2 = select(df2, -c(nx))\n\nkable(df2)    \n\n\n\n\n\nid\nx\nyear\n\n\n\n\n1\n1\n2020\n\n\n1\n1\n2021\n\n\n1\n0\n2022\n\n\n1\n2\n2023\n\n\n1\n2\n2024\n\n\n1\n1\n2025\n\n\n2\n2\n2022\n\n\n2\n2\n2023\n\n\n2\n2\n2024\n\n\n2\n2\n2025\n\n\n\n\n\n\n\n11.2.2.2 Fusion avec une autre base biographique\nOn peut être amené à fusionner plusieurs modules biographique. Jusqu’à présent, une même année, tous les individus ne pouvaient être que dans une situation, par exemple une seul emploi, un seul lieu de résidence etc… Pour certains phénomènes, une même années ou pendant une période plus longue on peut observer simultanément plusieurs états différent, ou plus classiquement observer une somme d’un même état. On parle ici d’overlapping. Ce type de situation est typiquement celle qu’on observe avec le nombre d’enfants.\nSupposons que le base ci-dessous traduit la naissance et potentiellement le décès des enfants.\n\n\nCode\ndfy = data.frame(id  =  c(1, 2, 2),\n                deb =  c(2022, 2019, 2023),\n                fin =  c(NA, 2024,NA), \n                nseq =  c(1,1,2))\n\nkable(dfy)\n\n\n\n\n\nid\ndeb\nfin\nnseq\n\n\n\n\n1\n2022\nNA\n1\n\n\n2\n2019\n2024\n1\n\n\n2\n2023\nNA\n2\n\n\n\n\n\n\nid=1 a un premier enfant en 2022 qui est toujours en vie au moment de l’enquête (2025)\nid=2:\n\nA un premier enfant en 2019 qui décède en 2024\nA un second enfant en 2023, toujours en vie au moment de l’enquête\nDe la naissance du second enfant au décès du premier, on va donc avoir des doublons (overlapping) sur les années\n\n\nSi on reprend les manipulations précédentes jusqu’à la création de la variable year:\n\n\nCode\ndfy$fin[is.na(dfy$fin)] = 2025\ndfy$dur = dfy$fin - dfy$deb + 1\n\ndfy$durb = dfy$dur  # Uncount supprime la variable d'origine \ndfy = uncount(dfy,durb)\n\ndfy$c = 1\ndfy = dfy %&gt;% group_by(id,nseq) %&gt;% mutate(year = deb  + cumsum(c) - 1)\n\n\nLa variable year est bien renseignée 2 fois pour les années 2023 et 2024.\nOn peut s’intéresser au fait d’avoir ou non un enfant, ou de manière plus générale au nombre d’enfant. En créant cette information, on se donne également le moyen de corriger cet overlapping:\n\nOn peut de nouveau générer un compteur contrôlé par individu année\nEn génerant un total de ligne doublonnée, on récupèrera par exemple ici le nombre d’enfant en vie chaque année.\nEn ne gardant que la ligne ou le compteur est égal à 1, on supprime les doublons tout en gardant l’information sur le nombre d’enfant en vie une année donnée.\n\n\n\nCode\ndfy = dfy %&gt;% group_by(id,year) %&gt;% mutate(ny = cumsum(c))\ndfy = dfy %&gt;% group_by(id,year) %&gt;% mutate(tot_y =  sum(c))\n\nkable(dfy)\n\n\n\n\n\nid\ndeb\nfin\nnseq\ndur\nc\nyear\nny\ntot_y\n\n\n\n\n1\n2022\n2025\n1\n4\n1\n2022\n1\n1\n\n\n1\n2022\n2025\n1\n4\n1\n2023\n1\n1\n\n\n1\n2022\n2025\n1\n4\n1\n2024\n1\n1\n\n\n1\n2022\n2025\n1\n4\n1\n2025\n1\n1\n\n\n2\n2019\n2024\n1\n6\n1\n2019\n1\n1\n\n\n2\n2019\n2024\n1\n6\n1\n2020\n1\n1\n\n\n2\n2019\n2024\n1\n6\n1\n2021\n1\n1\n\n\n2\n2019\n2024\n1\n6\n1\n2022\n1\n1\n\n\n2\n2019\n2024\n1\n6\n1\n2023\n1\n2\n\n\n2\n2019\n2024\n1\n6\n1\n2024\n1\n2\n\n\n2\n2023\n2025\n2\n3\n1\n2023\n2\n2\n\n\n2\n2023\n2025\n2\n3\n1\n2024\n2\n2\n\n\n2\n2023\n2025\n2\n3\n1\n2025\n1\n1\n\n\n\n\n\nIl ne reste plus qu’à supprimer les lignes où ny&gt;1\n\n\nCode\ndfy = filter(dfy, ny==1)\ndfy = select(dfy, -c(ny,deb, fin, dur, nseq, c))\n\nkable(dfy)\n\n\n\n\n\nid\nyear\ntot_y\n\n\n\n\n1\n2022\n1\n\n\n1\n2023\n1\n\n\n1\n2024\n1\n\n\n1\n2025\n1\n\n\n2\n2019\n1\n\n\n2\n2020\n1\n\n\n2\n2021\n1\n\n\n2\n2022\n1\n\n\n2\n2023\n2\n\n\n2\n2024\n2\n\n\n2\n2025\n1\n\n\n\n\n\nAvec une ligne par année, on peut la fusionner avec une autre base biographique en format individus-années (même principe qu’avec la fusion avec la base sur toutes les années de vie).\n\n\nCode\ndf2y = full_join(dfy, df2, by = c(\"id\",\"year\"))\n\ndf2y = arrange(df2y, id,year)\n\ndf2y = select(df2y, c(id,year,x,tot_y))\n\ndf2y$tot_y[is.na(df2y$tot_y)] = 0\ndf2y$x[is.na(df2y$x)]   = 0\n\nkable(df2y)\n\n\n\n\n\nid\nyear\nx\ntot_y\n\n\n\n\n1\n2020\n1\n0\n\n\n1\n2021\n1\n0\n\n\n1\n2022\n0\n1\n\n\n1\n2023\n2\n1\n\n\n1\n2024\n2\n1\n\n\n1\n2025\n1\n1\n\n\n2\n2019\n0\n1\n\n\n2\n2020\n0\n1\n\n\n2\n2021\n0\n1\n\n\n2\n2022\n2\n1\n\n\n2\n2023\n2\n2\n\n\n2\n2024\n2\n2\n\n\n2\n2025\n2\n1"
  },
  {
    "objectID": "10-manipulation.html#sélection-dun-type-de-séquence-et-mise-en-forme-pour-lanalyse",
    "href": "10-manipulation.html#sélection-dun-type-de-séquence-et-mise-en-forme-pour-lanalyse",
    "title": "11  Eléments de mise en forme des données",
    "section": "11.3 Sélection d’un type de séquence et mise en forme pour l’analyse",
    "text": "11.3 Sélection d’un type de séquence et mise en forme pour l’analyse"
  },
  {
    "objectID": "10-manipulation.html#durée-jusquà-la-première-séquence",
    "href": "10-manipulation.html#durée-jusquà-la-première-séquence",
    "title": "11  Eléments de mise en forme des données",
    "section": "11.4 Durée jusqu’à la première séquence",
    "text": "11.4 Durée jusqu’à la première séquence\n\n\nCode\ndf =  data.frame(id  =  c( 1, 1, 1, 2, 3, 3, 4),\n                 deb =  c(2018, 2022, 2024, 2019, 2023, 2024, 2023),\n                 fin =  c(2021, 2024, 2025, NA, 2024, NA, NA), \n                 y  =   c(1, 2, 1, 2, 3, 2, 1),\n                 nseq = c(1, 2, 3, 1, 1, 2, 1)\n                 )\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\n\n\n\n\n1\n2018\n2021\n1\n1\n\n\n1\n2022\n2024\n2\n2\n\n\n1\n2024\n2025\n1\n3\n\n\n2\n2019\nNA\n2\n1\n\n\n3\n2023\n2024\n3\n1\n\n\n3\n2024\nNA\n2\n2\n\n\n4\n2023\nNA\n1\n1\n\n\n\n\n\nOn va s’intéresser à la durée jusqu’à l’occurence de la séquence de type 2 ou 3 (variable y). On considéra que le début de l’exposition est donné par la variable deb sur la première séquence.\n\nid=1: début de l’exposition/observation en 2018, observe l’évènement en 2022.\nid=2: début de l’exposition/observation en 2019, observe l’évènement la même année.\nid=3: début de l’exposition/observation en 2019, observe l’évènement la même année.\nid=4: début de l’exposition/observation en 2023, n’a pas connu l’évènement au moment de l’enquête.\n\nRecupération de l’année de l’évènement\nOn peut repérer la présence d’une des deux séquences d’intérêt avec une indicatrice.\n\n\nCode\ndf$e = ifelse(df$y==2 | df$y==3,1,0)\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne\n\n\n\n\n1\n2018\n2021\n1\n1\n0\n\n\n1\n2022\n2024\n2\n2\n1\n\n\n1\n2024\n2025\n1\n3\n0\n\n\n2\n2019\nNA\n2\n1\n1\n\n\n3\n2023\n2024\n3\n1\n1\n\n\n3\n2024\nNA\n2\n2\n1\n\n\n4\n2023\nNA\n1\n1\n0\n\n\n\n\n\nDe nouveau l’utilisation d’un compteur sur cette variable indicatrice, peut s’avérer utile pour repérer le moment de l’occurence.\n\n\nCode\ndf = df %&gt;% group_by(id) %&gt;% mutate(n  = cumsum(e)) \n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne\nn\n\n\n\n\n1\n2018\n2021\n1\n1\n0\n0\n\n\n1\n2022\n2024\n2\n2\n1\n1\n\n\n1\n2024\n2025\n1\n3\n0\n1\n\n\n2\n2019\nNA\n2\n1\n1\n1\n\n\n3\n2023\n2024\n3\n1\n1\n1\n\n\n3\n2024\nNA\n2\n2\n1\n2\n\n\n4\n2023\nNA\n1\n1\n0\n0\n\n\n\n\n\nPour id=(2,3,4), ce compteur permet d’obtenir l’information souhaitée, à savoir n=0 en situation d’attente/séjour/survie et n=1 l’année de l’évènement. Pour id=1 cependant, l’alternance en y=1 et y=(2,3) ne permet pas de récupérer l’année d’occurence (première fois en 2 ou 3). Cela peut être fait, en faisant un compteur sur le compteur précédent:\n\n\nCode\ndf = df %&gt;% group_by(id) %&gt;% mutate(nn  = cumsum(n)) \n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne\nn\nnn\n\n\n\n\n1\n2018\n2021\n1\n1\n0\n0\n0\n\n\n1\n2022\n2024\n2\n2\n1\n1\n1\n\n\n1\n2024\n2025\n1\n3\n0\n1\n2\n\n\n2\n2019\nNA\n2\n1\n1\n1\n1\n\n\n3\n2023\n2024\n3\n1\n1\n1\n1\n\n\n3\n2024\nNA\n2\n2\n1\n2\n3\n\n\n4\n2023\nNA\n1\n1\n0\n0\n0\n\n\n\n\n\nRécupération des information censurée\nPour récupérer l’information sur les observations qui seront censurée, on peut faire un total sur la variable n ou e: si n=0, l’individu n’aura pas connu l’évènement.\n\n\nCode\ndf = df %&gt;% group_by(id) %&gt;% mutate(N  = sum(n)) \n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne\nn\nnn\nN\n\n\n\n\n1\n2018\n2021\n1\n1\n0\n0\n0\n2\n\n\n1\n2022\n2024\n2\n2\n1\n1\n1\n2\n\n\n1\n2024\n2025\n1\n3\n0\n1\n2\n2\n\n\n2\n2019\nNA\n2\n1\n1\n1\n1\n1\n\n\n3\n2023\n2024\n3\n1\n1\n1\n1\n3\n\n\n3\n2024\nNA\n2\n2\n1\n2\n3\n3\n\n\n4\n2023\nNA\n1\n1\n0\n0\n0\n0\n\n\n\n\n\nPour id=4, N est bien égal à 0.\nRécupération du début de l’exposition\nLe début de l’exposition étant ici l’année de début de la première séquence. On peut facilement récupérer cette sur toute les lignes en la repérant (ici en générant une nouvelle variable avec la fonction ifelse), et en sommant sa valeur sur les autres lignes (=0).\n\n\nCode\n1df$ debexp = ifelse(df$nseq==1, df$deb, 0)\n                    \n2df = df %&gt;% group_by(id) %&gt;% mutate(debexp  = sum(debexp))\n\nkable(df)\n\n\n\n1\n\nLa variable debex est égale à deb si nseq=1, 0 sinon.\n\n2\n\nOn somme cette valeur sur chaque individu pour l’ajouter aux séquences suivantes.\n\n\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne\nn\nnn\nN\ndebexp\n\n\n\n\n1\n2018\n2021\n1\n1\n0\n0\n0\n2\n2018\n\n\n1\n2022\n2024\n2\n2\n1\n1\n1\n2\n2018\n\n\n1\n2024\n2025\n1\n3\n0\n1\n2\n2\n2018\n\n\n2\n2019\nNA\n2\n1\n1\n1\n1\n1\n2019\n\n\n3\n2023\n2024\n3\n1\n1\n1\n1\n3\n2023\n\n\n3\n2024\nNA\n2\n2\n1\n2\n3\n3\n2023\n\n\n4\n2023\nNA\n1\n1\n0\n0\n0\n0\n2023\n\n\n\n\n\nMise en forme finale de la base\nOn peut maintenant conserver les lignes qui nous intéresse à savoir celle où nn=1 (évènement) ou N=0 (censure).\n\n\nCode\ndf = filter(df, nn==1 | N==0)\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne\nn\nnn\nN\ndebexp\n\n\n\n\n1\n2022\n2024\n2\n2\n1\n1\n1\n2\n2018\n\n\n2\n2019\nNA\n2\n1\n1\n1\n1\n1\n2019\n\n\n3\n2023\n2024\n3\n1\n1\n1\n1\n3\n2023\n\n\n4\n2023\nNA\n1\n1\n0\n0\n0\n0\n2023\n\n\n\n\n\nOn dispose déjà de la variable d’évènement/censure (e ou n = (0,1), on finit donc par la variable de durée.\n\n\nCode\ndf$fin[is.na(df$fin)] = 2025\n\ndf$dur = ifelse(df$e==1, df$deb - df$debexp + 1, df$fin - df$debexp + 1)\n\ndf = select(df, c(id,e,dur))\n\nkable(df)\n\n\n\n\n\nid\ne\ndur\n\n\n\n\n1\n1\n5\n\n\n2\n1\n1\n\n\n3\n1\n1\n\n\n4\n0\n3\n\n\n\n\n\nCes informations sont suffisantes pour estimer une fonction de séjour et on peut ajouter, si elles ne sont pas présentes, des covariables fixes issues du fichier des caractéristiques générales. Pour l’ajout de covariables dynamiques, leur ajout n’est pas forcément difficile pour une analyse en durée discrète 6. Pour les analyses type Cox, selon la nature de la variable dynamique, l’opération (quel que soit le logiciel utilisé) risque d’être plus ou moins compliquée."
  },
  {
    "objectID": "10-manipulation.html#durée-de-séjour-dans-la-séquence-dintérêt-et-variables-danalyse",
    "href": "10-manipulation.html#durée-de-séjour-dans-la-séquence-dintérêt-et-variables-danalyse",
    "title": "11  Eléments de mise en forme des données",
    "section": "11.5 Durée de séjour dans la séquence d’intérêt et variables d’analyse",
    "text": "11.5 Durée de séjour dans la séquence d’intérêt et variables d’analyse\nEn première ou deuxième analyse, on peut également voir s’intéresser à la durée de séjour dans l’état précédent. Par exemple, si l’analyse précédent consistait à regarder la durée de séjour dans le premier emploi, on pourrait regarder ensuite la durée jusqu’à sa reprise.\nCela va un peu (voir plus) se compliquer. On va repartir de la base de départ précédente en ajoutant une observation.\n\n\nCode\ndf =  data.frame(id  =  c( 1, 1, 1, 2, 3, 3, 4, 5, 5, 5 , 5),\n                 deb =  c(2018, 2022, 2024, 2019, 2023, 2024, 2023, 2019, 2021, 2023, 2024),\n                 fin =  c(2021, 2024, 2025, NA, 2024, NA, NA, 2021, 2023, 2024, NA), \n                 y  =   c(1, 2, 1, 2, 3, 2, 1, 1, 2, 1,3),\n                 nseq = c(1, 2, 3, 1, 1, 2, 1, 1, 2, 3, 4)\n)\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\n\n\n\n\n1\n2018\n2021\n1\n1\n\n\n1\n2022\n2024\n2\n2\n\n\n1\n2024\n2025\n1\n3\n\n\n2\n2019\nNA\n2\n1\n\n\n3\n2023\n2024\n3\n1\n\n\n3\n2024\nNA\n2\n2\n\n\n4\n2023\nNA\n1\n1\n\n\n5\n2019\n2021\n1\n1\n\n\n5\n2021\n2023\n2\n2\n\n\n5\n2023\n2024\n1\n3\n\n\n5\n2024\nNA\n3\n4\n\n\n\n\n\nFiltrage des observations hors champs\nOn peut déjà supprimer les observations hors champs, à savoir ici id=4 qui n’a pas connu l’évènement dont on analyse la durée.\n\n\nCode\n1df$e23 = ifelse(df$y==2 | df$y==3,1,0)\n\ndf = df %&gt;%  group_by(id) %&gt;% mutate(n23  = cumsum(e23)) \n2df = filter(df, n23!=0)\n\nkable(df)\n\n\n\n1\n\nNom de la variable e23 pour repérer la présence de l’évènement dont on analyse la durée.\n\n2\n\nCe compteur est suffisant car l’observation n’a qu’une ligne.\n\n\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne23\nn23\n\n\n\n\n1\n2022\n2024\n2\n2\n1\n1\n\n\n1\n2024\n2025\n1\n3\n0\n1\n\n\n2\n2019\nNA\n2\n1\n1\n1\n\n\n3\n2023\n2024\n3\n1\n1\n1\n\n\n3\n2024\nNA\n2\n2\n1\n2\n\n\n5\n2021\n2023\n2\n2\n1\n1\n\n\n5\n2023\n2024\n1\n3\n0\n1\n\n\n5\n2024\nNA\n3\n4\n1\n2\n\n\n\n\n\nRécupération de l’évènement analysé\nIci l’évènement sera un retour dans l’état y=1. Il y a de nouveau une possibilité de censure à droite si une observation reste dans l’état 2 ou 3 jusqu’au moment de l’enquête.\nIl peut être utile d’utiliser des variables décalées pour repérer les changements d’état d’une séquence à une autre. Ces décalages sont appelées lead ou lag:\n\nlead: \\(x_t = x_{t+1}\\)\nlag: \\(x_t = x_{t-1}\\)\n\nOn va utilise ici des lead et donc pouvoir repérer les changements d’état d’une séquence à une autre. Comme on s’intéresse au retour à l’état 1:\n\n\nCode\n1df$e = ifelse(df$y==1,1,0)\n\n2df = df %&gt;%  group_by(id) %&gt;% mutate(diff_e  = e - lead(e))\n\nkable(df)\n\n\n\n1\n\ne est une indicatrice qui repère l’état 1\n\n2\n\nOn fait redescendre la valeur de e sur la séquence précédente, et on calcule la difference.\n\n\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne23\nn23\ne\ndiff_e\n\n\n\n\n1\n2022\n2024\n2\n2\n1\n1\n0\n-1\n\n\n1\n2024\n2025\n1\n3\n0\n1\n1\nNA\n\n\n2\n2019\nNA\n2\n1\n1\n1\n0\nNA\n\n\n3\n2023\n2024\n3\n1\n1\n1\n0\n0\n\n\n3\n2024\nNA\n2\n2\n1\n2\n0\nNA\n\n\n5\n2021\n2023\n2\n2\n1\n1\n0\n-1\n\n\n5\n2023\n2024\n1\n3\n0\n1\n1\n1\n\n\n5\n2024\nNA\n3\n4\n1\n2\n0\nNA\n\n\n\n\n\nPour chaque dernière séquence la valeur du lag est une valeur manquante. On repère l’évènement avec une valeur de -1 (transition de 0 à 1). On ne peut pas encore filtrer les informations car il va falloir récupérer la fin de la séquence, mais on peut déjà construire l’information.\n\n\nCode\ndf$e = ifelse(df$diff_e==-1,1,0)\ndf$e[is.na(df$e)] = 0\ndf = df %&gt;%  group_by(id) %&gt;% mutate(e  = sum(e)) \n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ny\nnseq\ne23\nn23\ne\ndiff_e\n\n\n\n\n1\n2022\n2024\n2\n2\n1\n1\n1\n-1\n\n\n1\n2024\n2025\n1\n3\n0\n1\n1\nNA\n\n\n2\n2019\nNA\n2\n1\n1\n1\n0\nNA\n\n\n3\n2023\n2024\n3\n1\n1\n1\n0\n0\n\n\n3\n2024\nNA\n2\n2\n1\n2\n0\nNA\n\n\n5\n2021\n2023\n2\n2\n1\n1\n1\n-1\n\n\n5\n2023\n2024\n1\n3\n0\n1\n1\n1\n\n\n5\n2024\nNA\n3\n4\n1\n2\n1\nNA\n\n\n\n\n\nRécupération de l’année final avec succesion d’états de même type\nLa difficulté ici est apportée seulement par id=3. Jusqu’à 2025, on a successivement l’état 2 puis 3. Il va donc falloir récupérer cette dernière année de succession de 2 et 3, jusqu’à la censure ou jusqu’à un retour dans l’état 1. S’il n’y avait pas ce genre de situation, l’utilisation de la variable diff_e aurait été suffisante pour récupérer l’année de fin lorsqu’on a plusieurs séquences (situations pour id=1,5).\nOn va de nouveau utiliser un lead, mais sur la variable e23.\n\n\nCode\n1df = select(df, -c(nseq, diff_e))\n\n2df = df %&gt;%  group_by(id) %&gt;% mutate(lead_e23 = lead(e23, n = 1, default = NA))\n\n3df$idem = ifelse(df$e23 == df$lead_e23, 1, 0)\ndf$idem[is.na(df$idem)]=0\n4df = df %&gt;%  group_by(id) %&gt;% mutate(idem  = sum(idem))\n\n\nkable(df)\n\n\n\n1\n\nOn supprime les colonnes non utilisées pour gagner ici de la lisibilité\n\n2\n\nlead sur la variable e23.\n\n3\n\nLa variable idem permet de repérer une suite d’état 2 et 3. On ne passe pas ici par une variable de différence (le faire par prudence si on le souhaite).\n\n4\n\nIci le total est égal à 1. Si on avait eu une séquence supplémentaire de 3, il serait égal à 2. L’important ici est de repérer la situation, soit 0 ou supérieur à 0.\n\n\n\n\n\n\n\nid\ndeb\nfin\ny\ne23\nn23\ne\nlead_e23\nidem\n\n\n\n\n1\n2022\n2024\n2\n1\n1\n1\n0\n0\n\n\n1\n2024\n2025\n1\n0\n1\n1\nNA\n0\n\n\n2\n2019\nNA\n2\n1\n1\n0\nNA\n0\n\n\n3\n2023\n2024\n3\n1\n1\n0\n1\n1\n\n\n3\n2024\nNA\n2\n1\n2\n0\nNA\n1\n\n\n5\n2021\n2023\n2\n1\n1\n1\n0\n0\n\n\n5\n2023\n2024\n1\n0\n1\n1\n1\n0\n\n\n5\n2024\nNA\n3\n1\n2\n1\nNA\n0\n\n\n\n\n\nOn doit maintenant récupérer la dernière année de fin des situations où idem&gt;0, et la placer sur la première.\n\n\nCode\n1df$fin[is.na(df$fin)] = 2025\n2df$lead_e23[is.na(df$lead_e23)]   = -10\n\n3df$truefin = ifelse((df$lead_e23 != df$e23) & df$idem&gt;0, df$fin,0)\n\n4df = df %&gt;% group_by(id) %&gt;% mutate(truefin = sum(truefin))\ndf$fin = ifelse(df$idem&gt;0, df$truefin, df$fin)\n\ndf = select(df, -c(y,e23,lead_e23,idem))\n\nkable(df)\n\n\n\n1\n\nOn remplace l’année de la censure par sa valeur (important pour id=3).\n\n2\n\nPour régler un problème de gestion des NA avec ifelse. A tester avec if_else ou case_when.\n\n3\n\nOn recupère la valeur de l’année de fin lorsqu’il y a une succession d’états de même nature pour l’analyse.\n\n4\n\non remplace la valeur dans la variable fin en cas de succession seulement.\n\n\n\n\n\n\n\nid\ndeb\nfin\nn23\ne\ntruefin\n\n\n\n\n1\n2022\n2024\n1\n1\n0\n\n\n1\n2024\n2025\n1\n1\n0\n\n\n2\n2019\n2025\n1\n0\n0\n\n\n3\n2023\n2025\n1\n0\n2025\n\n\n3\n2024\n2025\n2\n0\n2025\n\n\n5\n2021\n2023\n1\n1\n0\n\n\n5\n2023\n2024\n1\n1\n0\n\n\n5\n2024\n2025\n2\n1\n0\n\n\n\n\n\nOn peut [enfin] sélectionner et conserver une seule ligne par individu et générer la variable de durée\n\n\nCode\ndf= select(df,-truefin)\n\ndf = df %&gt;%  group_by(id) %&gt;% mutate(nn23     = cumsum(n23)) \ndf = filter(df, n23==nn23)\n\ndf$dur= df$fin - df$deb + 1 \n\ndf = select(df, -c(n23,nn23))\n\nkable(df)\n\n\n\n\n\nid\ndeb\nfin\ne\ndur\n\n\n\n\n1\n2022\n2024\n1\n3\n\n\n2\n2019\n2025\n0\n7\n\n\n3\n2023\n2025\n0\n3\n\n\n5\n2021\n2023\n1\n3"
  },
  {
    "objectID": "10-manipulation.html#footnotes",
    "href": "10-manipulation.html#footnotes",
    "title": "11  Eléments de mise en forme des données",
    "section": "",
    "text": "Des éléments de manipulation/programmation pour un exemple volontairement très compliqué sont donnés dans méthodes =&gt; notes méthodologiques. Ayant été fait en 2015, le code pour R est largement out of date↩︎\nIci on a une enquête réalisée une même année pour toute les observations, ce n’est pas toujours le cas. De même au lieu de l’année, si les datations avaient été données par l’âge, au moment de l’enquête l’âge varierait d’une personne à une autre. Ces datations différentes (année ou âge) peuvent être présentes dans chaque module biographique d’une enquête, ou dans le fichier des caractéristiques fixes. Dans ce cas l’information devra être récupérée↩︎\nLa mesure est ici discrète/groupée, il me semble toujours préférable d’allonger les durées à +1. On démarre donc toujours un premier janvier pour terminer un 31 décembre sur l’information est donnée par des année. Ici t=1 représente la première année après la sortie des études. Une personne qui aura eu un emploi durant cette année, l’aura eu durant cette première année, que ce soit 2 semaines après ou 11 mois après. Si on disposait des mois, cela pourrait être intéressant de modifier cette métrique temporelle. Voir exemple 3↩︎\nContrairement au durée annuelle je n’ai pas ajouté 1 à chaque durée, ce qui est de nouveau envisageable par exemple si on veut explicitement indiquer les évènements qui ont lieu le premier mois. Pour id=3 la relation a t-elle durée du 1er septembre au 30 novembre, ou du 30 septembre au 1er novembre?? On a toujours un problème de précision, mais ici d’une trentaine de jours↩︎\navec R par exemple la fonction unique de dplyr↩︎\nEn conservant l’information sur les années, on transformera la base en format individu-période et on procédera à une fusion des informations↩︎"
  },
  {
    "objectID": "11-concurrent.html#problématique",
    "href": "11-concurrent.html#problématique",
    "title": "12  Risques concurrents",
    "section": "12.1 Problématique",
    "text": "12.1 Problématique\nOn étudie un processus dont l’occurence a plusieurs modalités, types ou causes:\n\nLa mortalité par cause de décès, les types de sortie du chômage: formation, emploi, radiation.\nLes types de sortie de l’emploi: chômage, longue maladie, sortie du marché du travail hors retraite.\nLes lieux de migration ou les espaces de mobilité résidentielle\nLes types de rupture d’union: séparation-divorce, veuvage).\n\nRappel: Déjà abordé dans la partie théorie, avec un recueil de données de type prospectif les “perdu.e.s de vue” peuvent difficilement être assimilés à des sorties d’observation non informatives (censures).\nL’analyse des risques concurrents est un cas particulier des modèles multi-états avec différents risques considérés comme absorbants.\nEn présence de risques concurrents, l’estimation de Kaplan-Meier ne peut se faire que sous l’hypothèse d’indépendance entre chacun des risques. Sinon l’estimateur de Kaplan-Meier n’est plus une probabilité. Une estimation de type KM d’un évènement en concurrence avec d’autres impose que ces derniers soient traités comme des censures à droites non informatives. Mais il n’est pas possible de tester cette hypothèse."
  },
  {
    "objectID": "11-concurrent.html#risques-cause-specific-et-biais-sur-les-estimateurs-km",
    "href": "11-concurrent.html#risques-cause-specific-et-biais-sur-les-estimateurs-km",
    "title": "12  Risques concurrents",
    "section": "12.2 Risques cause-specific et biais sur les estimateurs KM",
    "text": "12.2 Risques cause-specific et biais sur les estimateurs KM\nSi les risques ne sont pas indépendants les uns par rapport aux autres, la somme des estimateurs de (1-KM) pour chaque risque n’est pas égale - elle est supérieure - à l’estimateur de (1-KM) où les risques concurrents sont regroupés en un évènement unique. Par exemple les décès si on analyse ses causes.\nLe risque calculé en considérant les risques concurrents comme des censures à droite est appelé “cause-specific risk.\nCause specific risk\nPour le risque de type \\(k\\), le risque cause-spécific en \\(t_i\\) est égal à:\n\\[h_k(t_i)=\\frac{d_{i,k}}{R_i}\\] Où \\(d_{i,k}\\) est le nombre d’évènement de type \\(k\\) survenu en \\(t_i\\) et \\(R_i\\) la population soumise en \\(t_i\\).\n Conséquence: si les risques ne sont pas indépendants, la fonction de survie estimée avec la méthode Kaplan Meier n’exprime plus une probabilité.\nExemple sur les décès causés par une malformation cardiaque\nDans la base d’origine, il n’y a pas directement cette dimension de risque concurrent, même si on trouve dans la littérature médicale des études prenant le décès rapide post greffe comme un risque de ce type. Les données étant assez anciennes, avec beaucoup de décès post-opératoire, je ne me suis pas « risquer » à générer directement un risque concurrent sur cette information. Une sortie concurrente a donc été simulée sans plus de précision (variable compet), que l’on considèrera non strictement indépendante à la cause d’intérêt. Ce risque entre donc en concurrence avec la cause du décès directement liée à la malformation cardiaque, que la personne ait été transplantée ou non.\n\n\n\n\n\n\n           |    Survival Status\n           |       (1=dead) \n    compet |         0          1 |     Total\n-----------+----------------------+----------\n         0 |        28          0 |        28 \n         1 |         0         56 |        56 \n         2 |         0         19 |        19 \n-----------+----------------------+----------\n     Total |        28         75 |       103 \n\n\nVariable compet:\ncause 1 =&gt; décès directement provoquer par la malformation: compet=1 cause 2 =&gt; autre cause compet=0 =&gt; censure à droite\nLorsqu’on a analysé le décès par la méthode KM, la proportion de survivant.e.s était de 15%.\nSi on applique la méthode de Kaplan Meier à la cause 1 en traitant la cause 2 comme une censure à droite (\\(n=18+29=48\\)), puis en sommant les deux estimateurs, la fonction de répartition excède 100% au bout de 1000 jours environs. La proportion de survivant.e.s est donc négative.\n\n\n\nFonction de répartition avec une cause concurrente traitée comme une censure à droite"
  },
  {
    "objectID": "11-concurrent.html#estimations-en-présence-de-risques-concurrents-cif",
    "href": "11-concurrent.html#estimations-en-présence-de-risques-concurrents-cif",
    "title": "12  Risques concurrents",
    "section": "12.3 Estimations en présence de risques concurrents (CIF)",
    "text": "12.3 Estimations en présence de risques concurrents (CIF)\n\n12.3.1 Estimation non paramétrique\n\nUtiliser l’estimateur de Nelson Aalen: il s’agit du risque instantané cumulé. Comme il ne s’agit pas d’une probabilité, il a été longtemps utilisé comme mesure de l’incidence en présence de risques concurrents dans une logique dite cause specific.\n\n\\[H_k (t_i)=\\sum_{t_i\\leq t}\\left(\\frac{e_{i,k}}{n_i}\\right) \\]\n\nActuellement, l’estimateur le plus utilisé est la fonction dite d’incidence cumulée - CIF- de Kalbfleisch-Prentice et Marubini-Valscchi:\n\nIl repose sur une probabilité tout en supportant la non indépendance des risques.\nSon interprétation est identique à la fonction de répartition \\(F(t)=1-S(t)\\). Cette fonction est donc croissante.\nIl est possible de tester les différences entres CIF: test de Gray (R, SAS) ou test de Pepe-Mori (Stata).\n\n\nCIF (Cumulative Incidence Function)\n\nSi \\(h_k(t_i)\\) est le risque cause-spécific en \\(t_i\\) et \\(S(t_i-1)\\) l’estimateur de Kaplan-Meier en \\(t_i-1\\) lorsque tous les risques sont regroupés en un évènement unique, l’incidence cumulée pour le risque \\(k\\) en \\(t_i\\) est égale à:\n\n\\[IC_k(t_i)= \\sum_{t_i\\leq t}S(t_i-1)h_k(t_i)\\]\n\nLes valeurs prises par cette fonction pour la cause \\(k\\) ne dépendent donc pas seulement des individus ayant observé l’évènement à partir de cette seule cause, mais aussi du nombre de personnes qui n’ont pas encore observés l’évènement à partir des autres causes identifiées. Cette dernière information est donnée par \\(S(t_i-1)\\).\nL’incidence cumulée peut ainsi s’interpréter, simplement, comme la proportion d’individus qui sont sortis du risque jusqu’en \\(t_i\\) en raison de la cause \\(k\\).\n\n\n\n\nRisques concurrent: estimation de la CIF\n\n\n\n\n\n            failure:  compet == 1\n competing failures:  compet == 2\n\n    Time       CIF         SE     [95% Conf. Int.]\n--------------------------------------------------\n       1    0.0097     0.0097     0.0009    0.0477\n       2    0.0388     0.0190     0.0127    0.0892\n       3    0.0583     0.0231     0.0239    0.1149\n\n\n       5    0.0777     0.0264     0.0363    0.1395\n       6    0.0874     0.0278     0.0429    0.1515\n       8    0.0971     0.0292     0.0497    0.1634\n       9    0.1068     0.0304     0.0566    0.1751\n      12    0.1166     0.0316     0.0638    0.1868\n      16    0.1362     0.0338     0.0785    0.2099\n      18    0.1461     0.0349     0.0860    0.2212\n      21    0.1657     0.0367     0.1014    0.2437\n      32    0.1756     0.0376     0.1093    0.2550\n      37    0.1856     0.0384     0.1173    0.2662\n      40    0.1957     0.0393     0.1254    0.2775\n      43    0.2058     0.0400     0.1337    0.2888\n      45    0.2158     0.0408     0.1420    0.2999\n      50    0.2259     0.0415     0.1503    0.3110\n      51    0.2360     0.0422     0.1588    0.3221\n      53    0.2461     0.0428     0.1673    0.3330\n      58    0.2562     0.0434     0.1759    0.3439\n      61    0.2662     0.0440     0.1845    0.3548\n      66    0.2763     0.0445     0.1932    0.3656\n      69    0.2864     0.0450     0.2020    0.3763\n      72    0.3066     0.0459     0.2197    0.3976\n      77    0.3167     0.0464     0.2286    0.4082\n      78    0.3267     0.0467     0.2376    0.4187\n      81    0.3368     0.0471     0.2466    0.4292\n      85    0.3469     0.0475     0.2556    0.4396\n      90    0.3570     0.0478     0.2648    0.4500\n      96    0.3671     0.0481     0.2739    0.4604\n     102    0.3771     0.0484     0.2831    0.4707\n     110    0.3874     0.0487     0.2925    0.4812\n     149    0.3980     0.0489     0.3021    0.4920\n     165    0.4085     0.0492     0.3118    0.5027\n     186    0.4193     0.0495     0.3217    0.5137\n     188    0.4301     0.0497     0.3316    0.5246\n     207    0.4408     0.0499     0.3417    0.5354\n     219    0.4516     0.0501     0.3517    0.5462\n     263    0.4624     0.0502     0.3618    0.5570\n     285    0.4846     0.0505     0.3826    0.5791\n     308    0.4957     0.0506     0.3931    0.5900\n     340    0.5068     0.0507     0.4037    0.6009\n     583    0.5221     0.0514     0.4171    0.6168\n     675    0.5401     0.0524     0.4322    0.6361\n     733    0.5580     0.0532     0.4477    0.6548\n     995    0.5808     0.0548     0.4659    0.6795\n    1032    0.6036     0.0559     0.4851    0.7031\n    1386    0.6340     0.0583     0.5083    0.7357\n\n\n            failure:  compet == 2\n competing failures:  compet == 1\n\n    Time       CIF         SE     [95% Conf. Int.]\n--------------------------------------------------\n       3    0.0097     0.0097     0.0009    0.0477\n       6    0.0194     0.0136     0.0038    0.0619\n      16    0.0292     0.0166     0.0079    0.0761\n      17    0.0391     0.0191     0.0128    0.0897\n      28    0.0489     0.0213     0.0182    0.1029\n      30    0.0587     0.0232     0.0240    0.1157\n      35    0.0686     0.0250     0.0302    0.1286\n      36    0.0786     0.0267     0.0367    0.1411\n      39    0.0885     0.0282     0.0435    0.1534\n      40    0.0986     0.0296     0.0504    0.1658\n      68    0.1188     0.0322     0.0650    0.1901\n      80    0.1288     0.0334     0.0724    0.2020\n     100    0.1389     0.0345     0.0800    0.2138\n     153    0.1495     0.0356     0.0880    0.2261\n     334    0.1605     0.0368     0.0964    0.2392\n     342    0.1720     0.0381     0.1052    0.2526\n     852    0.1913     0.0417     0.1175    0.2787\n     979    0.2141     0.0460     0.1320    0.3094\n\n\nEn présence du risque concurrent, et traité comme tel, la moitié des personnes sont décédées suite à la malformation cardiaque au bout de 308 jours (200 jours avec une estimation de type « cause specific »).\nOn peut vérifier que la somme des estimateurs permet d’obtenir la survie toutes causes confondues. Il n’y a pas de surprise à cela, dans l’estimateur Marubini-Valscchi la survie d’ensemble intervient comme un facteur de pondération du quotient d’intensité dite « cause-specific ».\n\n\n\n\n\n\nR-Stata-Sas-Python\n\n\n\nL’estimation avec des risques de type « cause-specific » demande juste de recoder la variable évènement/censure, en glissant les risques concurrents en censure à droite.\nPour l’estimation des CIF (risque de sous répartition):\n\nR: la librairie cmprsk permet d’estimer simplement les incidences cumulées avec la fonction cuminc.\nSas: maintenant directement estimable avec proc lifetest. Il suffit d’indiquer le ou les risques d’intérêt dans l’instruction indiquant la variable de durée et de censure avec l’option failcode=valeur.\nStata: Estimation avec la commande externe stcompet. La commande génère des variables qui demande des manipulations supplémentaires pour afficher les résultats sous forme de tableau par exemple. On peut utiliser et préférer la commande externe stcomlist.\nPython: le wrapper de R (cmprsk) ne fonctionne plus à ce jour à défaut de mise à jour [2022].\n\n\n\n\n\n12.3.2 Compararaison des CIF\n\nTest d’homogénéité de Gray: est basé sur une autre mesure du risque en évènement concurrent. Sur le principe, identique à la philosopjie des test du logrank. Il s’agit du « subdistribution risks (« risque de sous-répartition », A.Latouche). Son interprétation n’est pas aisée car les personnes ayant observé un risque concurrent sont remises dans le Risk Set. Mais il est directement lié à l’estimation des CIF. Disponible avec SAS et R. Il est également sensible l’hypothèse de proportionnalité et à la distribution des censures à droites entre les groupes comparés. A ma connaissance il n’y a pas de variantes pondérées.\nTest de Pepe & Mori: teste directement deux courbes d’incidences et seulement 2. Je n’ai pas le recul nécessaire sur cette alternative, qui n’est implémenté que dans Stata.\n\n\nTest de Gray pour la variable surgery\n\n\nRisques\nChi2\nP&gt;Chi2\n\n\n\n\nCause1\n5.783\n0.0161\n\n\nCause2\n0.129\n0.7191\n\n\n\n\nTest de Pepe-Mori pour la variable surgery\n\n\nRisques\nChi2\nP&gt;Chi2\n\n\n\n\nCause1\n6.203\n0.0127\n\n\nCause2\n1.880\n0.7038\n\n\n\n\n\n\n\n\n\nR-Stata-Sas-Python\n\n\n\n\nSas: le test de Gray est estimé si on ajoute l’option strata=nom_variable à la proc lifetest sous risque concurrent (voir encadré précédent). Le test de Pepe-Mori est disponible via une macro externe (%compcif: non testée) :\nStata: Le test de Gray n’est pas disponible, il faut passer par une exécution de la fonction cuminc de la librairie R cmprsk directement dans stata (voir la commande rsource). Pour faire plus simple, on peut estimer le modèle de Fine-Gray avec une seule variable (discrète). Le résultat est comparable à celui du test (voir plus bas). Le test de Pepe-Mori est disponible via la commande externe stpepemori.\nR: On ajoute une variable à la fonction cuminc de la librairie cmprsk. Pas de test de Pepe-Mori sur les fonctions d’incidence à ma connaissance.\nPython: ne pas essayer d’utiliser la librairie cmprsk qui n’est pas mis à jour et ne fonctionne plus."
  },
  {
    "objectID": "11-concurrent.html#modèles",
    "href": "11-concurrent.html#modèles",
    "title": "12  Risques concurrents",
    "section": "12.4 Modèles",
    "text": "12.4 Modèles\n\n12.4.1 Modèles Semi paramétriques\nCette présentation sera plutôt brève. Dans le domaine des sciences sociales, je préconise plutôt l’utilisation d’un modèle multinomial à temps discret de type logistique. Le modèle de Cox en présence de risques concurrent n’est valable que dans une logique de risques « cause-specific », le modèle de Fine et Gray bien que directement relié à l’estimation des incidences cumulées, repose sur une définition du risque (de sous répartition) dont l’interprétation n’est pas naturelle. Il est également soumis à l’hypothèse de proportionnalité des risques.\nModélisation des risques « cause-specific » : Cox\nModèle de Cox «standard» pour chaque évènement, les évènements concurrents sont traités comme des censures à droite. Aucune interprétation sur les fonctions d’incidence ne peut-être faite.\nModèle de Fine-Gray: subdistribution hazard regression\nModèle de type semi-paramétrique avec une redéfinition du risque lié à l’estimation des fonctions d’incidence (voir test de Gray). La différence avec le Cox classique réside dans le calcul du risk-set : les évènements concurrents ne sont pas considérés comme des censures, on laisse les individus leur « survivre » jusqu’à la durée maximale observée dans l’échantillon. L’interprétation n’est donc pas très intuitive (Fine et Gray le soulignent). Ce modèle est relativement contreversé. Il ne sera donc pas exécuté pour l’application\n\nPour les questions liées à l’interprétation de ces deux types de modèles, se reporter à: https://onlinelibrary.wiley.com/doi/epdf/10.1002/sim.7501\n\n\n\n\n\n\nR-Stata-Sas-Python\n\n\n\n\nR: on utilise la fonction crr du package cmprsk.\nSas: même principe que pour l’estimation non paramétrique, on ajoute l’option eventcode=valeur à l’instruction model de la proc phreg.\nStata: on utilise la commande interne stcrreg.\nPython : ne pas essayer d’utiliser la librairie cmprsk qui n’est pas mis à jour et ne fonctionne donc plus.\n\n\n\n\n\n12.4.2 Modèle à temps discret\n\nIl s’agit d’une extension du modèle à temps discret à évènement unique (toutes causes regroupées) avec ici le modèle logistique multinomial.\nS’il ne permet pas une interprétation sur les fonctions d’incidences, les risques concurrents ne sont pas traitées comme des censures à droite.\nLe modèle multinomial repose sur une hypothèse dite « d’indépendance des alternatives non pertinentes » (IIA). Cela peut donc paraitre contradictoire d’utiliser ce modèle pour des évènements qui sont supposés non indépendants. Néanmoins la dépendance entre risques concurrents n’est pas non plus stricte et cette hypothèse d’IIA, seulement testable par le bon sens, est souvent illustrée par l’exemple des couleurs des bus dans le choix du mode de transport, ou les couleurs de chaussure dans les études marketing. Soit est une situation relativement limite.\nEn terme de lecture, les estiupateurs du modèle logistique multinomial peuvent directement s’interpréter comme des rapports de risque (ou relative risk ratio).\nEn sciences sociales, il me semble que ce type de modèle soit à privilégier.\nOn peut également envisager un modèle de type probit multinomial, mais on peut rencontrer des problèmes d’estimations (repose sur la loi normale multivariée). Prévoir un regroupement des causes concurrentes, et dans tous les cas de figure ne pas dépasser trois causes.\nNiveau lecture, on peut utiliser une méthode de standardisation, de type AME (Average Marginal Effect).\n\nPour l’application, nous avons pris le mois (30 jours) comme métrique temporelle. On rappelle que les valeurs des estimateurs sont fictives en raison de la simulation des évènement pour le risque concurrent (cause2)\n\n\nTable 12.1: Modèle logistique multinomial avec risques concurrent\n\n\n\n\n(a) Cause 1\n\n\nCause 1\nRRR\np&gt;|z|\n95% IC\n\n\n\n\n\\(t\\)\n0.816\n0.000\n0.752 - 0.885\n\n\n\\(t^2\\)\n1.003\n0.000\n1.001 - 1.005\n\n\n\\(year\\)\n0.879\n0.116\n0.749 - 1.032\n\n\n\\(age\\)\n1.045\n0.012\n1.010 - 1.081\n\n\n\\(surgery\\)\n0.318\n0.033\n0.110 - 0.913\n\n\n\\(constante\\)\n0.231\n0.000\n0.148 - 0.360\n\n\n\n\n\n\n(b) Cause 2\n\n\nCause 2\nRRR\np&gt;|z|\n95% IC\n\n\n\n\n\\(t\\)\n0.817\n0.003\n0.713 - 0.935\n\n\n\\(t^2\\)\n1.003\n0.052\n1.000 - 1.006\n\n\n\\(year\\)\n0.816\n0.141\n0.622 - 1.070\n\n\n\\(age\\)\n1.011\n0.654\n0.964 - 1.061\n\n\n\\(surgery\\)\n0.541\n0.431\n0.117 - 2.496\n\n\n\\(constante\\)\n0.076\n0.000\n0.037 - 0.157\n\n\n\n\n\n\nNotes:\n\nOn a utilisé le terme RRR - Relative Risk Ratio - pour la colonne raportant les estimations. Dans un cadre de risque concurrent il est un peu difficile d’utiliser formellement la notion de hazard rate tel qu’il a été difini plus haut, enfin les modèles multinomiaux ne reportent pas formellement des Odds Ratios dont l’utilisation devrait être réservé exclusivement à une alternative binaire.\nles variables year et age ont été centrées sur leur valeur moyenne pour donner aux constantes des valeurs acceptables.\nPour faciliter la lecture on peut utiliser une méthode de standardisation de type AME (Average Marginal Effect)."
  },
  {
    "objectID": "01-presentation.html#questions",
    "href": "01-presentation.html#questions",
    "title": "2  L’analyse biographique des durées",
    "section": "2.1 Questions",
    "text": "2.1 Questions\nOn dispose de données dites “longitudinales”, et on cherche à appréhender l’occurence d’un évènement au sein d’une population. Les problématiques se basent sur les questions suivantes:\n\n\nObserve-t-on la survenue de l’évènement pour l’ensemble des individus?\nQuelle est la durée jusqu’à la survenue de l’évènement?\nQuels sont les facteurs qui favorisent la survenue de cet évènement? Facteurs fixes ou facteurs pouvant apparaitre/changer au cours de la période d’observation: variables dynamiques (TVC: Time Varying Covariate)"
  },
  {
    "objectID": "01-presentation.html#terminologies",
    "href": "01-presentation.html#terminologies",
    "title": "2  L’analyse biographique des durées",
    "section": "2.2 Terminologies",
    "text": "2.2 Terminologies\n\n\n\nFrançais\nAnglais\n\n\n\n\nAnalyse des durées\nDuration analysis\n\n\nAnalyse de survie/séjour\nSurvival analysis\n\n\nAnalyse de fiabilité\nFailure time data analysis\n\n\nAnalyse des transitions\nEvent-history analysis\n\n\n\nPour ce support, le choix de son titre me pose toujours problème pour éviter qu’il soit trop à rallonge. Si j’avais à trancher, il devrait un peu s’éterniser sous l’appelation Introduction à l’analyse biographique des durées en présence de données censurée (à droite1)."
  },
  {
    "objectID": "01-presentation.html#exemples-danalyse",
    "href": "01-presentation.html#exemples-danalyse",
    "title": "2  L’analyse biographique des durées",
    "section": "2.3 Exemples d’analyse",
    "text": "2.3 Exemples d’analyse\n\nNuptialité, Mise en couple: cohabiter, décohabiter, se marier, Rompre une union …\nLogement: Changement de statut (locataire &lt;=&gt; propriétaire), mobilité résidentielle/migration …\nEmploi: Trouver un 1er emploi, changer d’emploi, entrée ou sortie du chômage …\nFécondité: Avoir un premier enfant, avoir un nouvel enfant …\nMortalité: Décéder après diagnostic, survivre après l’administration un traitement, rechute…"
  },
  {
    "objectID": "01-presentation.html#elements-nécessaire-à-lanalyse",
    "href": "01-presentation.html#elements-nécessaire-à-lanalyse",
    "title": "2  L’analyse biographique des durées",
    "section": "2.4 Elements nécessaire à l’analyse",
    "text": "2.4 Elements nécessaire à l’analyse\n\nUn processus temporel\n\nUne échelle de mesure ou métrique temprelle: minutes, heures, jours, mois, années….\nUne origine commune définissant un évènement de départ 2: naissance, mariage si on analyse la séparation, …..\nUne définition précise de l’évènement d’étude.\n\nUne durée entre le début et la fin de la période d’observation, si nécessaire avec la fin de la période d’exposition au risque. Cette durée doit être généralement calculée à l’aide des informations de datation.\n\nUne population soumise au risque de connaître l’évènement (Risk Set)\nDes variables explicatives ou covariables\n\nFixes: sexe/genre, génération, niveau de diplôme le plus élevé,……\nDynamiques (TVC: Time varying covariates):\n\nMesurées à tout moment entre le début et la sortie de l’observation: statut matrimonial, taille du ménage, statut d’activité…\nPour les modèles, à l’exception du semi-paramétrique de Cox, en présence de données censurée la durée ou une transformation de celle-ci est une variable dynamique qui est également introduite comme variable indépendante pour assurer le bon ajustement des données. L’introduction directe d’une fonction de la durée comme variable dépendante ne peut se faire qu’en absence d’observation censurée, en particulier à droite."
  },
  {
    "objectID": "01-presentation.html#types-danalyses",
    "href": "01-presentation.html#types-danalyses",
    "title": "2  L’analyse biographique des durées",
    "section": "2.5 Types d’analyses",
    "text": "2.5 Types d’analyses\n[A faire]"
  },
  {
    "objectID": "01-presentation.html#footnotes",
    "href": "01-presentation.html#footnotes",
    "title": "2  L’analyse biographique des durées",
    "section": "",
    "text": "ce qui est déjà bien suffisant↩︎\nAttention, dans le cadre des données prospectives ou de suivi, cela ne peut pas être le moment de l’inclusion à la base données↩︎"
  },
  {
    "objectID": "08-discret.html#organisation-des-données",
    "href": "08-discret.html#organisation-des-données",
    "title": "9  Modèle à durée discrète",
    "section": "9.1 Organisation des données",
    "text": "9.1 Organisation des données\nFormat long\nLes données doivent être en format long: pour chaque individu on a une ligne par durée observée ou par intevalle de durées jusqu’à l’évènement ou la censure. On retrouve le split des données du modèle de Cox, mais généralisé à des intervalles où aucun évènement n’est observé. Avec des données de type discrètes ou groupées, phénomène classique en sciences sociales, il y a souvent peu de différence entre un allongement aux temps d’évènement et aux temps d’observation (voir encadré plus loin sur le modèle d Cox à temps discret).\nDurée\nLa durée est dans un premier temps construite sous forme d’un simple compteur, par exemple \\(t=1,2,3,4,5...\\) (des valeurs non entières sont possibles). Le choix de la forme fonctionnelle de la durée sera présentée plus tard.\nVariable évènement/censure\nSi l’individu a connu l’évènement, elle prend la valeur 0 avant celui-ci. Au moment de l’évènement sa valeur est égale à 1. Pour les observations censurées, la variable prend toujours la valeur 0.\nApplication\nOn reprend les données de la base transplantation, mais les durées ont été regroupées par période de 30 jours. Il n’y a pas de durée mesurée comme nulle, on a considéré que les 30 premiers jours représentaient, le premier mois d’exposition. Cette variable de durée se nomme mois.\nFormat d’origine\n\nDurée discrète: données en format d’origine\n\n\nid\nyear\nage\nsurgery\nmois\ndied\n\n\n\n\n1\n67\n30\n0\n2\n1\n\n\n\nLa personne décède lors du deuxième intervalle de 30 jours\nFormat long et variables pour l’analyse\n\nDurée discrète: données en format long\n\n\nid\nyear\nage\nsurgery\nmois\ndied\nt\n\n\n\n\n1\n67\n30\n0\n2\n0\n1\n\n\n1\n67\n30\n0\n2\n1\n2"
  },
  {
    "objectID": "08-discret.html#ajustement-de-la-durée",
    "href": "08-discret.html#ajustement-de-la-durée",
    "title": "9  Modèle à durée discrète",
    "section": "9.2 Ajustement de la durée",
    "text": "9.2 Ajustement de la durée\nUn des principaux enjeux réside dans la paramétrisation de la durée:\n\nElle peut-être modélisée sous forme de fonction d’une variable de type quantitative/continue.\nElle peut-être modélisée comme variable discrète, de type indicatrice \\({0;1}\\), sur tous les points d’observation ou sous forme de regroupements. Il doit y avoir au moins un évènement observé dans chaque intervalle.\n\n\n\n\n\n\n\nLe modèle de Cox à durée discrète/groupée\n\n\n\nCox est également à l’origine du modèle à durée discrète (je crois également en 1972). Par rapport aux pratiques courantes, la différence repose sur les bornes intervalles, identiques à celles définies pour l’estimation de la courbe de survie KM ou du modèle semi-paramétrique, à savoir une définition sur les moments d’évènement. Avec un ajustement de la durée reposant sur des indicatrices (ajustement sur des variables discrètes), ce modèle est quasiment identique au modèle semi paramétrique.\nOn peut remarquer que dans les sciences sociales, avec des durées assez fortement groupées, les intervalles directement observés et les intervalles définis aux moments d’évènements sont souvent identiques et s’ils ne le sont pas c’est souvent en tout début ou en toute fin de la période d’observation/exposition. En cas d’ajustement de la durée par des indicatrices, la définition des bornes des intervalles aux moments des évènements permet de s’assurer au moins une occurence de l’évènement.\n\n\n\n9.2.1 Ajustement avec une fonction quantitative de la durée\nLe modèle étant paramétrique, on doit trouver une fonction qui ajuste le mieux les données. Toutes transformations de la variable est possible: \\(f(t)=a\\times t\\), \\(f(t)=a\\times log(t)\\)……formes quadratiques. Les ajustements sous forme de splines cubiques tendent à se développer ces dernières années.\nPour sélectionner cette fonction, on peut tester différents modèles sans covariable additionnelle, et sélectionner la forme dont le critère d’information de type vraisemblance pénalisée (AIC, BIC) est le plus faible, avec au moins des différences de -6 ou -8.\nExemple:\nOn va tester les paramétrisations suivantes: une forme linéraire stricte \\(f(t)=a\\times t\\) et des effets quadratiques d’ordres 2 et 3: \\(f(t)=a_1\\times t + a_2\\times t^{2}\\) et \\(f(t)=a_1\\times t + a_2\\times t^{2} + a_3\\times t^{3}\\).\n\n\n\nProbabilité de décéder avec 3 ajustements de la durée\n\n\nCritères AIC\n\n\n\n\\(f(t)\\)\nAIC\n\n\n\n\n\\(a\\times t\\)\n504\n\n\n\\(a_1\\times t + a_2\\times t^{2}\\)\n492\n\n\n\\(a_1\\times t + a_2\\times t^{2} + a_3\\times t^{3}\\)\n486\n\n\n\nOn peut utiliser la troisième forme à savoir \\(a_1\\times t + a_2\\times t^{2} + a_3\\times t^{3}\\) 3.\nEstimation du modèle avec toutes les covariables\n\nModèle logistique à durée discrète (\\(f(t)\\) continue)\n\n\n\n\n\n\n\n\n\n\nVariables\nOR - RR\nStd. err\nz\nP&gt;|z|\n95% IC\n\n\n\n\n\\(t\\)\n0.678\n0.057\n-4.52\n0.000\n0.587 ; 0.810\n\n\n\\(t^2\\)\n1.014\n0.005\n+2.83\n0.005\n1.004 ; 1.024\n\n\n\\(t^3\\)\n1.000\n0.000\n-2.11\n0.035\n1.000 ; 1.000\n\n\n\\(year\\)\n0.876\n0.015\n-1.80\n0.072\n0.758 ; 1.012\n\n\n\\(age\\)\n1.034\n0.163\n+2.27\n0.023\n1.005 ; 1.064\n\n\n\\(surgery\\)\n0.364\n0.110\n-2.25\n0.024\n0.151 ; 0.877\n\n\n\n\n\n\n\n\n\n\nConstante\n0.440\n0.110\n-3.29\n0.001\n0.270 ; 0.718\n\n\n\nRemarque: les variables year et age ont été centrée sur leur moyenne pour rendre la constante interprétable. La constante reporte donc l’Odds de décéder lors des 30 premiers jours d’une personne dont l’âge et l’année à l’entrée dans le registre est égal à l’âge et à l’année moyenne et qui n’a pas été opéré préalablement.\nSi maintenant on estime un modèle de Cox sur ces données journalières groupées, on remarque que les résultats obtenus, et ce n’est pas une surprise, sont très proches.\n\nModèle de Cox\n\n\n\n\n\n\n\n\n\n\nVariables\nOR - RR\nStd. err\nz\nP&gt;|z|\n95% IC\n\n\n\n\n\\(year\\)\n0.878\n0.059\n-1.93\n0.053\n0.769 ; 1.002\n\n\n\\(age\\)\n1.029\n0.014\n+2.13\n0.033\n1.002 ; 1.057\n\n\n\\(surgery\\)\n0.379\n0.165\n-2.22\n0.026\n0.111 ; 0.892\n\n\n\n\n\n9.2.2 Ajustement discret\n\nIl s’agit d’introduire la variable de durée dans le modèle comme une variable catégorielle (indicatrices).\nDémarche pas conseillé si on a beaucoup de points d’observation, ce qui est le cas ici.\nA l’inverse, si peu de points d’observation la paramétrisation avec une durée continue n’est pas conseillé.\nLa correction de la non proportionnalité peut être plus compliquée à mettre en oeuvre.\n\nOn va supposer que l’on ne dispose que de 4 intervalles d’observation. Pour l’exemple, on va créer ces points à partir des quartiles de la durée, et conserver pour chaque personne une seule observation par intervalle.\n\n\\(t=1\\): Entre le début de l’exposition et 4 mois.\n\\(t=2\\): Entre 5 mois et 11 mois .\n\\(t=3\\): Entre 12 mois et 23 mois.\n\\(t=4\\): 24 mois et plus.\n\nOn va estimer le risque globalement sur l’intervalle. La base sera plus courte que la précédente (197 observations pour 103 individus). Il ne sera plus possible ici d’interpréter les résultats en termes de rapport de probabilité, l’évènement devenant trop fréquent à l’intérieur de chaque intervalle.\n\nModèle logistique à durée discrète (\\(f(t)\\) indicatrices)\n\n\n\n\n\n\n\n\n\n\nVariables\nOR - RR\nStd. err\nz\nP&gt;|z|\n95% IC\n\n\n\n\n\\(0-4 mois\\)\n2.811\n1.177\n+2.47\n0.014\n1.237 ; 6.387\n\n\n\\(5-11 mois\\)\nref\n-\n-\n-\n-\n\n\n\\(12-23 mois\\)\n0.559\n0.346\n-0.94\n0.347\n0.166 ; 1.881\n\n\n\\(24-46 mois\\)\n1.741\n1.159\n+0.83\n0.405\n0.472 ; 6.417\n\n\n\\(year\\)\n0.816\n0.076\n-2.18\n0.029\n0.680 ; 0.980\n\n\n\\(age\\)\n1.048\n0.019\n+2.53\n0.011\n1.011 ; 1.087\n\n\n\\(surgery\\)\n0.330\n0.166\n-2.21\n0.027\n0.123 ; 0.882\n\n\n\n\n\n\n\n\n\n\nConstante\n0.407\n0.151\n2.43\n0.015\n0.198 ; 0.840\n\n\n\nOn trouve des résultats proches de ceux éstimés avec un ajustement continu de la durée. C’est normal, la dirée fait office de variable d’ajustement peu ou pas corrélée avec les autres variables introduites.\n\n\n\nVariables\nAjustement discret\nAjustement continu\n\n\n\n\n\\(year\\)\n0.816\n0.876\n\n\n\\(age\\)\n1.048\n1.034\n\n\n\\(surgery\\)\n0.330\n0.364\n\n\n\n\n\n\n\n\n\nLien avec des modèles usuels à durée continue\n\n\n\nSi la durée discrète/groupée sous tend une durée continue (ce qui est clairement le cas ici):\n\nOn l’a déjà souligné, l’ajustement avec des durées sous forme d’indicatrices correspond au modèle à durée discrète défini par Cox, et le plus proche du modèle semi-paramétrique. Il est également assimilable à un modèle de type exponential piecewice constant (donc un modèle de poisson).\nSi l’ajustement se fait en utilisation une transformation de la durée par une fonction:\n\n\\(f(t)= log(t)\\) correspond à un modèle de Weibull à risque proportionnel 4.\n\nSi l’ajustement se fait avec des splines cubiques, le modèle à durée discrète correspond à un modèle de type Parmar-Royston. Avec une forme quadratique classique, on peut également obtenir cette correspondance."
  },
  {
    "objectID": "08-discret.html#proportionnalité-des-risques",
    "href": "08-discret.html#proportionnalité-des-risques",
    "title": "9  Modèle à durée discrète",
    "section": "9.3 Proportionnalité des risques",
    "text": "9.3 Proportionnalité des risques\n\nFormellement un modèle logistique à temps discret repose sur une hypothèse d’Odds proportionnel [Odds ratios constants pendant la durée d’observation]. Contrairement au modèle de Cox, l’estimation des probabilités (risque) n’est pas biaisée si l’hypothèse PH n’est pas respectée, les paramètres estimés sont considérés au pire comme des approximation.\nComme pour le modèle de Cox, la correction de la non proportionnalité peut se faire en intégrant une interaction avec la durée dans le modèle.\n\nAvec un ajustement continue, on remarque de nouveau que le résultat du modèle est de nouveau très proche de celui estimé avec un modèle de Cox.\n\nModèle logistique à durée discrète avec correction de la non proportionnalité\n\n\n\n\n\n\n\n\n\n\nVariables\nOR - RR\nStd. err\nz\nP&gt;|z|\n95% CI\n\n\n\n\n\\(t\\)\n0.702\n0.059\n-4.2\n0.000\n0.595 ; 0.828\n\n\n\\(surgery(t=0)\\)\n0.155\n0.108\n-2.67\n0.008\n0.039 ; 0.609\n\n\n\\(surgery\\times t\\)\n1.072\n0.036\n2.08\n0.037\n1.004 ; 1.145\n\n\n\\(t^2\\)\n1.013\n0.005\n2.37\n0.018\n1.002 ; 1.023\n\n\n\\(t^3\\)\n1.00\n0.000\n-1.71\n0.086\n1.000 ; 1.000\n\n\n\\(year\\)\n0.872\n0.064\n-1.86\n0.062\n0.755 ; 1.007\n\n\n\\(age\\)\n1.033\n0.015\n2.23\n0.026\n1.004 ; 1.063\n\n\n\\(constante\\)\n0.445\n0.112\n-3.22\n0.001\n0.272 ; 0.728\n\n\n\nSi on avait omis les variables year et age du modèle:\n\n\n\nProbabilité de décéder après correction de la non proportionnalité pour la variable surgery"
  },
  {
    "objectID": "08-discret.html#footnotes",
    "href": "08-discret.html#footnotes",
    "title": "9  Modèle à durée discrète",
    "section": "",
    "text": "la distribution des probabilités sous cette loi n’est pas, contrairement aux lois normale ou logistique, symétrique. Dans ce qui suit, il ne serait pas conseillé de l’utiliser dans l’application avec l’ajustement sous forme d’indicatrice avec seulement 4 intervalles↩︎\nje remercie Emilie Counil et Nargès Gouhoubi pour m’avoir informé de leur existence↩︎\nCe n’est pas le cas ici, mais si on sélectionne une forme cubique, je conseille vivement de regarder les probabilités conditionnelle obtenues, en particulier en fin de périodes d’observation/exposition si peut d’individu reste soumis au risque. On peut rencontrer des problèmes d’overfitting avec des probabilités conditionnelles estimées trop proche de 1. Pour les personnes qui suivent la formation, c’est le cas avec les données des TP↩︎\nvoir la courte section sur les modèle paramétriques usuels↩︎"
  },
  {
    "objectID": "03-theorie.html#temps-et-durée",
    "href": "03-theorie.html#temps-et-durée",
    "title": "4  La théorie",
    "section": "4.1 Temps et durée",
    "text": "4.1 Temps et durée\nLe temps est une dimension (la quatrième), la durée est sa mesure. La durée est tout simplement calculée par la différence, pour une échelle temporelle donnée, entre la fin et le début d’une période d’exposition ou d’observation.\nOn distingue généralement deux types de mesure de la durée : continue et discrète/groupée. Ces deux notions ne possèdent pas réellement de définition, la différence s’explique plutôt par la présence ou non de simultanéité dans l’occurrence des évènements. Le temps étant intrasèquement continu car deux évènements ne peuvent pas avoir lieu en « même temps ». C’est donc l’échelle temporelle choisie ou imposée par l’analyse et les données qui pourra rendre cette mesure continue ou discrète/groupée. Pour un physicien travaillant sur la théorie de la relativité avec des horloges atomiques, une minute (voire une seconde) est une mesure très groupée pour ne pas dire grossière du temps, alors que pour un géologue c’est une mesure continue. Pour ces deux disciplines, cette échelle de mesure n’est pas adaptée à leur domaine. Le choix de l’échelle temporelle doit être pertinent par rapport aux objectifs de l’analyse même si on dispose des informations très fines (dates de naissance exactes). Etudier la fécondité avec une métrique journalière n’aurait pas de sens.\nIl existe des situations où les durées sont par nature discrète, lorsqu’un évènement ne peut avoir lieu qu’à un moment précis (date d’anniversaire des contrats pour l’analyse des résiliation). Généralement dans les sciences sociales avec un recueil de données de type rétrospectif, les mesures dites discrètes sont plutôt de nature groupées. Pour une même année, on considèrera indifféremment des évènements qui se produiront un premier janvier et un 31 décembre d’une même année.\n\n\n\n\n\n\nImportant\n\n\n\n\nDurée continue : absence (ou très peu) d’évènements simultanés\n\nDurée discrète/groupée : présence d’évènements simultanés (en grand nombre)"
  },
  {
    "objectID": "03-theorie.html#le-risk-set",
    "href": "03-theorie.html#le-risk-set",
    "title": "4  La théorie",
    "section": "4.2 Le Risk Set",
    "text": "4.2 Le Risk Set\n\n\nIl s’agit de la population soumise ou exposée au risque lorsque \\(T=t_i\\).\n\n\nCette population varie dans le temps car:\n\n\nCertaines personnes ont connu l’évènement, donc ne peuvent plus être soumises au risque (ex: décès si on analyse la mortalité).\nCertaines personnes sortent de l’observation sans avoir (encore) observé l’évènement: décès si on analyse un autre type d’évènement, perdu.e.s de vue, fin de l’observation dans un recueil rétrospectif."
  },
  {
    "objectID": "03-theorie.html#la-censure",
    "href": "03-theorie.html#la-censure",
    "title": "4  La théorie",
    "section": "4.3 La Censure",
    "text": "4.3 La Censure\n\n\n\n\n\n\nImportant\n\n\n\nUne observation est dite censurée lorsque la durée d’observation est inférieure à la durée d’exposition au risque\n\n\n\n4.3.1 Censure à droite\nDéfinition\nCertains individus n’auront pas (encore) connu l’évènement à la date de l’enquête après une certaine durée d’exposition. On a donc besoin d’un marqueur permettant de déterminer que les individus n’ont pas observé l’évènement sur la période d’étude.\nPourquoi une information est-elle censurée (à droite)?\n\nFin de l’étude, date de l’enquête.\nPerdu de vue, décès si autre évènement étudié.\n\nEn pratique (important)\n\nNe pas exclure ces observations, sinon on surestime la survenue de l’évènement.\nNe pas les considérer a-priori comme sorties de l’exposition sans avoir connu l’évènement. Elles peuvent connaître l’évènement après la date de l’enquête ou en étant perdues de vue. Sinon on sous-estime la durée moyenne de survenue de l’évènement.\n\nExemple\nOn effectue une enquête auprès de femmes : On souhaite mesurer l’âge à la première naissance. Au moment de l’enquête, une femme est âgée de 29 ans n’a pas (encore) d’enfant.\nCette information sera dite «censurée».\nElle est clairement encore soumise au risque après la date de l’enquête. Au niveau de l’analyse, elle sera soumise au risque à partir de ses premières règles jusqu’au moment de l’enquête.\nHypothèse fondamentale\nLes observations censurées ont vis à vis du phénomène observé le même comportement que les observations non censurées. On dit que la censure est non informative. Elle ne dépend pas de l’évènement analysé. Normalement le problème ne se pose pas dans les recueil retrospectif. \nProblème posé par la censure informative\nPar exemple en analysant des décès avec un recueil prospectif, si un individu est perdu de vue en raison d’une dégradation de son état de santé, l’indépendance entre la cause de la censure et le décès ne peut plus être assurée.\nA l’Ined l’exploitation du registre des personnes atteintes de mucoviscidose (G.Bellis) donne une autre illustration de ce phénomène. Chaque année un nombre significatif de personnes sortent du registre (pas de résultats aux examens annuels). Si certain.e.s perdu.e.s de vue s’expliquent par des déménagements, émigration ou par un simple problème d’enregistrement des informations, on note qu’ils/elles sont nombreu.se.s à présenter une forme « légère » de la maladie. L’information pouvant être donnée ici par la mutation du gène. Comme il n’est pas recommandé de supprimer ou de traiter ces observations comme des censures à droite non informatives, on peut les appréhender comme un risque concurrent au décès ou à tout autre évènement analysé à partir de ce registre (voir section dédiée).\nLes graphiques suivant représentent, en temps calendaire et après sa transformation en durée, la logique des censures à droite. Le recueil des informations est ici de nature prospectives.\n\nTrait plein : durée observée\nPointillés : durée censurée\nBulle : moment de l’évènement\n\n\n\n\n\n\nSchéma évènement/censure en temps calendaire\n\n\n\n\n\nSchéma évènement/censure sous forme de durée\n\n\n\n\n\n\n4.3.2 Censure à gauche, troncature et censure par intervalle\nCensure à gauche\nL’évènement s’est produit avant le début période d’observation. Typique des données prospectives, de type registre, avec des âges d’inclusion différenciés.\nCensure par intervalle Un évènement peut se produire entre 2 temps d’observations sans qu’on puisse les observer (ex: en criminologie récidive d’un delit entre deux arrestations). Un phénomène de censure à droite avec perdu.e de vue peut se transformer en censure par intervalle lorsque la personne réapparait et est de nouveau incluse dans les données.\nTroncature\nPar l’exemple, on analyse la survie d’une population. Seule la survie des individus vivants à l’inclusion peut être analysée (troncature à gauche). On peut également trouver un phénomène de troncature lorsqu’on mesure la durée à partir ou jusqu’à un certain seuil niveau.\nCes situations sont généralement plutôt bien contrôlées dans les recueils rétrospectifs. Elles sont assez courantes lorsque le recueil est de type prospectif.\nDurée d’observation supérieure à la durée d’exposition\nA l’inverse des individus peuvent sortir de l’exposition avant la fin de la période d’observation, et il convient donc de corriger la durée de cette sortie.\nUn exemple simple : si au moment de l’enquête une femme sans enfant a 70 ans, cela n’a pas de sens de continuer de l’exposer au risque au-delà d’un certain âge. Si on ne dispose pas d’information sur l’âge à la ménopause on peut tronquer la durée un peu au-delà de l’âge le plus élevé à la première naissance observée dans les données."
  },
  {
    "objectID": "03-theorie.html#les-grandeurs",
    "href": "03-theorie.html#les-grandeurs",
    "title": "4  La théorie",
    "section": "4.4 Les grandeurs",
    "text": "4.4 Les grandeurs\n\n4.4.1 Les grandeurs utilisées\n\nLa fonction de survie: \\(S(t)\\)\nLa fonction de répartition: \\(F(t)\\)\nLa fonction de densité: \\(f(t)\\)\nLe risque instantané: \\(h(t)\\)\nLe risque instantané cumulé: \\(H(t)\\)\n\nRemarques:\n\nToutes ces grandeurs sont mathématiquement liées les unes par rapport aux autres. En connaître une permet d’obtenir les autres.\n\nAu niveau formel on se placera ici du point de vue où la durée mesurée est strictement continue. Cela se traduit, entre autre, par l’absence d’évènements dits “simultanés”.\nLes expressions qui vont suivre ne sont pas des estimateurs, mais des grandeurs dont on précisera les propriétés. Les techniques d’estimations devront respecter ces propriétés .\n\n\n\n\n4.4.2 La fonction de Survie \\(S(t)\\)\nDans ce type d’analyse, il est courant d’analyser la courbe dite de survie. Hors contexte de mortalité on peut préférer la notion de courbe de de séjour (Courgeau, Lelièvre).\nLa fonction de survie donne la proportion de la population qui n’a pas encore connue l’évènement après une certaine durée \\(t\\). Elle y a donc “survécu”.\nFormellement, la fonction de survie est la probabilité de survivre au-delà de \\(t\\), soit:\n\n\\[S(t) = P(T&gt;t)\\]\nPropriétés:\n\n\\(S(0)=1\\)\n\\(\\lim\\limits_{t\\to{\\propto}}S(t)=0\\)\n\nLa fonction de survie est donc strictement non croissante.\n\n\n4.4.3 La fonction de répartition \\(F(t)\\)\nC’est la probabilité de connaitre l’évènement jusqu’en \\(t\\), soit:\n\n\\[F(t)=P(T\\leq{t})\\]\nSoit: \\(F(t) = 1 - S(t)\\)\nLa fonction de survie et la fonction de répartition sont donc deux grandeurs strictement complémentaires et décrivent la même information.\nPropriétés:\n\n\\(F(0)=0\\)\n\\(\\lim\\limits_{t\\to{\\propto}}F(t)=1\\)\n\n\n\n4.4.4 La fonction de densité \\(f(t)\\)\n\nPour une valeur de \\(t\\) donnée, la fonction de densité de l’évènement donne la distribution des moments où les évènement ont eu lieu. Elle est donnée dans un premier temps par la probabilité de connaitre l’évènement dans un petit intervalle de temps \\(dt\\). Si \\(dt\\) est proche de 0 (temps continu) alors cette probabilité tend également vers 0. On norme donc cette probabilité par \\(dt\\). Rappel: on est toujours ici dans la théorie.\n\nEn temps continu, la fonction de densité est donnée par la dérivée de la fonction de répartition: \\(f(t)=F'(t)=-S'(t)\\).\n\nFormellement la fonction de densité \\(f(t)\\) s’écrit:\n\\[f(t)=\\lim\\limits_{dt\\to{0}}  \\frac{P(t\\leq{T}&lt; t + dt)}{dt}\\]"
  },
  {
    "objectID": "03-theorie.html#le-risque-instantané-ht",
    "href": "03-theorie.html#le-risque-instantané-ht",
    "title": "4  La théorie",
    "section": "4.5 Le risque instantané \\(h(t)\\)",
    "text": "4.5 Le risque instantané \\(h(t)\\)\nConcept fondamental de l’analyse des durées:\n\\[h(t)=\\lim\\limits_{dt\\to{0}} \\frac{P(t\\leq{T}&lt; t + dt | T\\geq{t})}{dt}\\]\n\n\\(P(t\\leq{T}&lt; t + dt | T\\geq{t})\\) donne la probabilité de survenue de l’évènement sur l’intervalle \\([t,t+dt[\\) conditionnellement à la survie au temps \\(t\\).\n\nEn divisant par \\(dt\\), La quantité obtenue donne alors un nombre moyen d’évènements que connaîtrait un individu durant un très court intervalle de temps.\nA priori cette quantité n’est pas une probabilité. C’est la nature de l’évènement, en particulier sa non récurrence, et la métrique temporelle choisie ou disponible qui peut la rendre assimilable à une probabilité. Tout comme la densité, on est plutôt dans la définition d’un taux (d’où l’expression hazard rate en anglais).\n\nOn peut également écrire:\n\\(h(t)=\\frac{f(t)}{S(t)}=\\frac{F'(t)}{S(t)}=-\\frac{S'(t)}{S(t)}\\) 1\nOn remarque que cette fonction de risque (ou hazard rate) n’est pas une probabilité car \\(\\frac{f(t)}{S(t)}\\) ne peut pas contraindre la valeur obtenue à ne pas être supérieure à 1.\n\n4.5.1 Le risque cumulé \\(H(t)\\)\nLe risque cumulé est égal à :\n\\[H(t)=\\int_{0}^{t} h(u)du = -log(S(t))\\]\nOn peut alors réécrire toutes les autres quantités à partir de celle-ci:\n\n\\(S(t)=e^{-H(t)}\\)\n\n\\(F(t)=1-e^{-H(t)}\\)\n\n\\(f(t)=h(t)\\times{e^{-H(t)}}\\)\n\nExemple avec la loi exponentielle (risque constant)\nSi on pose que le risque est strictement constant au cours du temps: \\(h(t)=a\\). Cette forme du risque suit une loi exponentielle. Cette situation est, par exemple, typique des processus dits sans mémoire comme la durée de vie des ampoules:\n\n\\(h(t)=a\\)\n\n\\(H(t)=a\\times{t}\\)\n\n\\(S(t)=e^{-a\\times{t}}\\)\n\n\\(F(t)=1-e^{-a\\times{t}}\\)\n\n\\(f(t)=a\\times{e^{-a\\times{t}}}\\)\n\n\n\n\nGrandeurs de la loi exponentielle avec h(t)=0.04\n\n\nApplication: risque et échelles temporelles:\nAttention on sort ici très clairement de la durée continue, il s’agit seulement de manipuler les concepts et de voir la dépendance de la mesure du risque à l’échelle temporelle choisie ou disponible.\n\nDurant les mois d’hiver, entre le 1er janvier et le 1er avril [3 mois], la probabilité d’attraper un rhume chaque mois est de 48% (il s’agit bien d’un risque). Quelle est le risque d’attraper le rhume durant la saison froide?\n\\(\\frac{0.48}{1/3}=1.44\\). On peut donc s’attendre a attraper 1.44 rhume durant la période d’hiver.\nOn passe une année en vacances dans une région où la probabilité de décéder chaque mois est évaluée à 33%. Quelle est le risque de décéder pendant cette année sabbatique? \\(\\frac{0.33}{1/12}=3.96\\)\n\nLe risque peut donc être supérieur à 1. C’est donc plutôt un taux tel qu’il est défini généralement. En soit cela ne pose pas de problème comme il s’agit d’un nombre moyen d’évènements espérés (exemple: taux de fécondité), mais pour des évènements qui ne peuvent pas se répéter, évènements dits absorbants, l’interprétation n’est pas très intuitive.\nLe risque étant constant, on peut prendre son inverse qui mesure la durée moyenne (espérée) jusqu’à l’occurence de l’évènement.\nOn retrouve donc un concept classique en analyse démographique comme l’espérance de vie (survie): la question n’est pas de savoir si on va mourir ou non, ce risque indépendamment de la durée étant par définition égal à 1, mais jusqu’à quand on peut espérer survivre.\n\nPour le rhume, la durée moyenne est de \\((1.44^{-1}=0.69\\) du trimestre hivernal, soit approximativement le début du mois de mars.\nPour l’année sabbatique, la durée moyenne de survie est de \\(3.96^{-1}=0.25\\) d’une année soit 3 mois après l’arrivée dans la région.\n\nExercice\n\nOn a une population de 100 cochons d’Inde.\nOn analyse leur mortalité (naturelle).\n\nIci l’analyse est en temps discret.\nLa durée représente le nombre d’année de vie.\nIl n’y a pas de censure à droite.\n\n\n\n\nDurée\nNombre de décès\n\n\n\n\n1\n1\n\n\n2\n1\n\n\n3\n3\n\n\n4\n9\n\n\n5\n30\n\n\n6\n40\n\n\n7\n10\n\n\n8\n3\n\n\n9\n2\n\n\n10\n1\n\n\n\nN=100\nA quel âge le risque de mourir des cochons d’Inde est-il le plus élevé? Quelle est la valeur de ce risque?"
  },
  {
    "objectID": "03-theorie.html#remarques-complémentaires",
    "href": "03-theorie.html#remarques-complémentaires",
    "title": "4  La théorie",
    "section": "4.6 Remarques complémentaires",
    "text": "4.6 Remarques complémentaires\n\n4.6.1 Formes typiques de la fonction de survie\nUne des propriétés de la fonction de survie ou de séjour est qu’elles tendent vers 0. A la lecture du graphique suivant, cela peut correspondre à la forme de la courbe S2, bien que le % de survivant tend à baisser de moins en moins à mesure que la durée augmente. Deux cas limites doivent être considéré.\n\n\n\nFonction de survie: 3 situtation typiques\n\n\n\nS1: très peu d’évènements et la fonction de séjour suit une asymptote nettement supérieur à 0 ( \\(\\lim\\limits_{t\\to{\\propto}}S(t)=a\\) avec \\(a&gt;0\\)). La question est plus délicate car on interroge l’exposition au risque d’une partie de l’échantillon ou, dit autrement on peut penser qu’une fraction est immunisé au risque. Cette problématique est rapidement posée en fin de formation.\nS2: la situation attendue\nS3: La survie tombe à 0 très/trop rapidement: il n’y a donc pas ou presque pas de durée (par exemple presque tout l’échantillon observe l’évènement la première année de l’exposition). Les méthodes en temps continue ne sont a priori pas adaptées à ce genre de situation. Si on dispose d’une information plus fine pour dater les évènements, la fonction de séjour pourra reprendre une forme plus “standard”. Dans le graphique, \\(S(t=1)=0.4\\) , \\(S(t=2)=0.025\\), mais si on dispose par exemple de 10 points d’observations supplémentaires dans chaque durée groupée:\n\n\n\n\nFonction de survie et modification de la métrique temporelle\n\n\n\n\n4.6.2 Absence de censures à droites\nLes méthodes qui vont être présentées plus tard gèrent la présence de censures à droite. S’il n’y en a pas, elle restent néanmoins parfaitement valables. L’absence de censure facilite certaines analyses, par exemple celles des fonctions de séjour où le calcul direct des durées moyennes est rendu possible.\n\n\n4.6.3 Utilisation des pondérations dans un schema retrospectif avec des biographies longues\nUne question assez récurrente concerne l’utilisation des poids de sondage dans les analyses de durées avec longueurs biographiques souvent assez longues. Leur utilisation ne me semble pas recommandée voire à exclure sauf exceptions. En effet les pondérations sont générées au moment de l’enquête, alors que les évènements étudiés peuvent remonter dans un passé plus ou moins lointain pour une partie de la population analysée. Si on regarde de plus près, la création de poids longitudinaux ne résoudrait pas grand chose , les pondérations devant être recalculées à chaque moment d’observation ou à chaque moment où des évènements se produisent. Par ailleurs on mélangerait régulièrement à un instant donné des personnes issues de générations différentes ce qui rend impossible tout calage sur des caractéristiques d’un population. Supposons une personne âgée de 25 ans et un personne âgée de 70 ans au moment de l’enquête en 2022, avec un début d’observation à l’âge de 18 ans . A 20 ans (\\(t=2\\)), pour la première personne les caractéristiques de la population sont celles de 2017, pour celle de 70 ans celles de 1972. On fait comment??????"
  },
  {
    "objectID": "03-theorie.html#footnotes",
    "href": "03-theorie.html#footnotes",
    "title": "4  La théorie",
    "section": "",
    "text": "La relation \\(h(t)=\\frac{f(t)}{S(t)}\\) et donc \\(f(t)= h(t) \\times S(t)\\) est intéressante et importante car elle permet d’écrire la vraisemblance du processus probabiliste permettant d’estimer les paramètres des différentes analyses. On voit déjà sa proximité avec la fonction de masse de Bernouilli: \\(f(y_i) = p^{y_i} \\times (1-p)^{1-y_i}\\). Se reporté à la section qui décrit qui la vraisemblance partielle de Cox pour s’en faire une idée plus précise.↩︎"
  },
  {
    "objectID": "04-fsurvie.html#les-fonctions-de-survieséjour",
    "href": "04-fsurvie.html#les-fonctions-de-survieséjour",
    "title": "5  Estimations des fonctions de survie",
    "section": "5.1 Les fonctions de survie/séjour",
    "text": "5.1 Les fonctions de survie/séjour\n\n5.1.1 Les variables d’analyse\nOn a un échantillon aléatoire de \\(n\\) individus avec:\n\nDes indicateurs de fin d’épisode \\(e_1,e_2,....,e_k\\) avec \\(e_i=0\\) si censure à droite et \\(e_i=1\\) si évènement observé pendant la période d’observation.\nDes durées d’exposition au risque \\(t_1,t_2,....,t_k\\) jusqu’à l’évènement ou la censure.\nEn théorie, il ne peut pas y avoir d’évènement en \\(t=0\\).\n\n\n\n5.1.2 Calcul de la fonction de survie\nRappel: La fonction de survie donne la probabilité que l’évènement survienne après \\(t_i\\), soit \\(S(t_i)=P(T&gt;t_i)\\). Pour survivre en \\(t_i\\), il faut donc avoir survécu en \\(t_{i-1}\\), \\(t_{i-2}\\), …., \\(t_{1}\\).\nLa fonction de survie renvoie donc des probabilités conditionnelles: on survit en \\(t_i\\) conditionnellement au fait d’y avoir survécu avant. Il s’agit donc d’un produit de probabilités.\nSoit \\(d_i=\\sum e_i\\) le nombre d’évènements observé en \\(t_i\\) et \\(r_i\\) la population encore soumise au risque en \\(i\\). On peut mesurer l’intensité de l’évènement en \\(t_i\\) en calculant le quotient \\(q(t_i)=\\frac{d_i}{r_i}\\).\nSi le temps est strictement continu on devrait toujours avoir \\(q(t_i)=\\frac{1}{r_i}\\).\n\\(S(t_i) = (1 - \\frac{d_i}{r_i})\\times{S(t_{i-1})} = S(t_i) = (1 - q(t_i))\\times{S(t_{i-1})}\\). En remplaçant \\(S(t_{i-1})\\) par sa valeur: \\(S(t_i) = (1 - \\frac{d_i}{r_i})\\times(1 - \\frac{d_{i-1}}{r_{i-1}})\\times{S(t_{i-2})}\\).\nAu final, en remplaçant toutes les expressions de la survie jusqu’en \\(t_0\\) (\\(S(0)=1\\)):\n\\[S(t_i)=\\displaystyle \\prod_{t_i\\leq{k}} (1-q(t_i))\\]\n\n\n\n\n\n\nApplication pour la suite du support\n\n\n\n\nOn va analyser le risque de décéder (la survie) de personnes souffrant d’une insuffisance cardiaque. Le début de l’exposition est leur inscription dans un registre d’attente pour une greffe du coeur.\nLes covariables sont dans un premier temps toutes fixes: l’année (year) et l’âge (age) à l’entrée dans le registre, et le fait d’avoir été opéré pour un pontage aorto-coronarien avant l’inscription (surgery).\nLe début de l’exposition au risque est l’entrée dans le registre, la durée est mesurée en jour (stime). La variable évènement/censure est le décès (died). Les durées de la variable stime ont été regroupée par période de 30 jours pour réaliser des analyses à durée discrete. Cette nouvelle variable de durée a été appelé mois.\nL’introduction d’une dimension dynamique, la greffe, est donnée par les informations contenues dans les variables transplant et wait.\nLa variable compet est une information simulée pour réaliser des analyses en risques concurrents.\nLes bases en format .csv, .sas7bdat et .dta sont disponibles dans ce dépôt [lien]\n\nExtrait de la base:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nyear\nage\ndied\nstime\nsurgery\ntransplant\nwait\nmois\ncompet\n\n\n\n\n15\n68\n53\n1\n1\n0\n0\n0\n1\n1\n\n\n43\n70\n43\n1\n2\n0\n0\n0\n1\n1\n\n\n61\n71\n52\n1\n2\n0\n0\n0\n1\n1\n\n\n75\n72\n52\n1\n2\n0\n0\n0\n1\n1\n\n\n102\n74\n40\n0\n11\n0\n0\n0\n1\n0\n\n\n74\n72\n29\n1\n17\n0\n1\n5\n1\n2"
  },
  {
    "objectID": "04-fsurvie.html#la-méthode-actuarielle",
    "href": "04-fsurvie.html#la-méthode-actuarielle",
    "title": "5  Estimations des fonctions de survie",
    "section": "5.2 La méthode actuarielle",
    "text": "5.2 La méthode actuarielle\n\nEstimation sur des intervalles définies par l’utilisateur.\nMéthode dite «continue», estimation en milieu d’intevalle.\nMéthode apropriée lorsque la durée est mesurée de manière discrète/groupée.\nMéthode, hélas, quasiment abandonnée dans les sciences sociales où les durées sont plus rarement mesurées de manière exacte. L’absence de test de comparaison des fonctions de survie n’y est pas étranger, tout comme le lien de la méthode suivante (Kaplan-Meier) avec le modèle de Cox.\nContrairement à la méthode de Cox, la méthode actuarielle permet de calculer les quantiles de la durée.\n\n\n5.2.1 Estimation\nEchelle temporelle\nLa durée est divisée en \\(J\\) intervalles, en choisissant \\(J\\) points: \\(t_0&lt;t_1&lt;...&lt;t_J\\) avec \\(t_{J+1}=\\infty\\).\nCalcul du Risk set\n\nA \\(t_{min}=0\\), \\(n_0=n\\) individus soumis au risque: \\(r_0=n_0\\).\nLe nombre d’exposé.e.s au risque sur un intervalle est calculé en soustrayant la moitié des cas censurés sur la longueur de l’intervalle: \\(r_i=n_i- 0.5\\times{c_i}\\), avec \\(n_i\\) le nombre de personnes soumises au risque au début de l’intervalle et \\(c_i\\) le nombre d’observations censurées sur la longueur de l’intervalle. On suppose donc que les observations censurées \\(c_i\\) sont sorties de l’observation uniformément sur l’intervalle. Les cas censurés le sont en moyenne au millieu de l’intervalle. \n\nCalcul de \\(S(t_i)\\)\nOn applique la méthode de la section précédente avec:\n\\[q(t_i)=\\frac{d_i}{n_i - 0.5\\times c_i}\\]\nCalcul de la durée médiane (ou autre quantiles)\nRappel: en raison de la présence de censures à droite, le dernier intervalle étant ouvert jusqu’à la dernière sortie d’observation, il n’est pas conseillé de calculer des durées moyennes. On préfère utiliser la médiane ou tout autre quantile lorsqu’ils sont calculables.\nDéfinition: il s’agit de la durée telle que \\(S(t_i)=0.5\\).\nCalcul: Comme on applique une méthode continue et monotone à l’intérieur d’intervalles, on ne peut pas calculer directement un point de coupure qui correspond à 50% de survivants. On doit donc trouver ce point par interpolation linéaire dans l’intervalle \\([t_i;t_{i+1}[\\) avec \\(S(t_{i+1})\\leq0.5\\) et \\(S(t_{i})&gt;0.5\\).\nR-Stata-Sas-Python\n\nRStataSasPython\n\n\nLes fonctions de survie avec la méthode dite actuarielle sont estimables avec le package discSurv. Avec le temps, il s’est étoffé, on peut maintenant paramatrer des intervalles (programmation pénible), mais les quantiles de la durée ne sont toujours pas estimables, ce qui est bien dommage.\n\n\nCommande ltable, avec en option la paramétrisation des intervalles de durées. Voir la commande externe qlt (MT) qui calcule les durées médianes (+ autres quartiles) et qui recalcule la fonction de séjour avec une définition des intervalles de durées identique à celle de SAS.\n\n\nSous une proc lifetest avec en option method=lifetable. On peut paramétrer les intervalles d’estimation avec l’option width.\n\n\nA l’heure actuelle, aucune fonction à ma connaissance\n\n\n\n\n\n5.2.2 Application\nLes résultats qui suivent ont été estimés avec Stata en retenant la définition des bornes de Sas, plus pertinente à mon sens, avec des intervalles fixes de 30 jours.\n\n\nAfficher le tableau des estimateurs\n     +----------------------------------------------+\n     |   t0     t1   survival CI 95% low  CI 95% up |\n     |----------------------------------------------|\n  1. |    0     30          1          .          . |\n  2. |   30     60   .7853659   .6925991   .8530615 |\n  3. |   60     90   .6461871   .5449008   .7304808 |\n  4. |   90    120    .525027   .4232338   .6170507 |\n  5. |  120    150   .4740535   .3737563   .5677139 |\n     |----------------------------------------------|\n  6. |  150    180   .4636348   .3637283   .5575485 |\n  7. |  180    210   .4425605   .3435417   .5368989 |\n  8. |  210    240   .4105681   .3132064   .5052779 |\n  9. |  240    270   .3997637   .3030412   .4945301 |\n 10. |  270    300   .3888113   .2927645   .4836136 |\n     |----------------------------------------------|\n 11. |  300    330   .3665935   .2720434   .4613676 |\n 12. |  330    360   .3554846   .2617823   .4501585 |\n 13. |  360    390   .3216289   .2308275   .4157428 |\n 14. |  390    420   .3216289   .2308275   .4157428 |\n 15. |  420    450   .3216289   .2308275   .4157428 |\n     |----------------------------------------------|\n 16. |  480    510   .3216289   .2308275   .4157428 |\n 17. |  510    540   .3216289   .2308275   .4157428 |\n 18. |  540    570   .3216289   .2308275   .4157428 |\n 19. |  570    600   .3216289   .2308275   .4157428 |\n 20. |  600    630   .3059397   .2154747   .4009653 |\n     |----------------------------------------------|\n 21. |  660    690   .3059397   .2154747   .4009653 |\n 22. |  720    750   .2884574   .1981834   .3848506 |\n 23. |  840    870   .2704288   .1806664   .3680736 |\n 24. |  900    930   .2517786   .1628919   .3505543 |\n 25. |  930    960   .2517786   .1628919   .3505543 |\n     |----------------------------------------------|\n 26. |  960    990   .2517786   .1628919   .3505543 |\n 27. |  990   1020   .2288896   .1404089   .3303913 |\n 28. | 1020   1050   .2060007   .1191749   .3093143 |\n 29. | 1140   1170   .1831117   .0991601   .2873401 |\n 30. | 1320   1350   .1831117   .0991601   .2873401 |\n     |----------------------------------------------|\n 31. | 1380   1410   .1831117   .0991601   .2873401 |\n 32. | 1560   1590   .1464894   .0645215   .2602391 |\n 33. | 1770   1800   .1464894   .0645215   .2602391 |\n 34. | 1800      .   .1464894   .0645215   .2602391 |\n     +----------------------------------------------+\n\n\n\nQuantiles de la fonction de séjour type actuarielle - Bornes Sas\n\n\nS(t)\nt\n\n\n\n\n0.90\n13.977\n\n\n0.75\n37.623\n\n\n0.50\n104.729\n\n\n0.25\n906.993\n\n\n0.10\n.\n\n\n\n\n\n\nCourbe de survie: estimation méthode actuarielle\n\n\nLecture des résultats: 102 jours après leur inscription dans le registre d’attente pour une greffe, 50% des malades sont toujours en vie. Au bout de 914 jours, 75% sont décédés."
  },
  {
    "objectID": "04-fsurvie.html#la-méthode-de-kaplan-meier",
    "href": "04-fsurvie.html#la-méthode-de-kaplan-meier",
    "title": "5  Estimations des fonctions de survie",
    "section": "5.3 La méthode de Kaplan-Meier",
    "text": "5.3 La méthode de Kaplan-Meier\n\nL’approche qui exploite toute l’information disponible est celle dite de Kaplan-Meier (KM).\nIl y a autant d’intervalles que de durées où l’on observe au moins un évènement.\nAu lieu d’utiliser des intervalles prédéterminés, l’estimateur KM va définir un intervalle entre chaque évènement enregistré.\nLa fonction de survie estimée par la méthode KM est une fonction en escalier (stairstep), d’où une estimation dite “discrète”.\nPour chaque intervalle, on compte le nombe d’évènements et le nombre de censures.\nMéthode adaptée pour une mesure de la durée de type continue.\n\n\n5.3.1 Estimation\nDéfinition du Risk Set (\\(r_i\\))\nS’il y a à la fois des évènements et des censures à une durée \\(t_i\\), les observations censurées sont considérées comme exposées au risque à ce moment, comme si elles étaient censurées très rapidement après. C’est la principale caractéristique de cette méthode, appelé également l’estimateur product-limit\n\\[r_i=r_{i-1}-d_{i-1}-c_{i-1}\\]\n\nCalcul de \\(q_i\\)\n On applique la méthode de la section précédente avec:\n\\[q_i=\\frac{d_i}{r_{i-1}-d_{i-1}-c_{i-1}}\\] Remarque: la variance de l’estimateur est obtenu par la méthode dite de Greenwood. Il n’y a pas d’intérêt particulier de la décrire dans ce support.\nRécupération de la médiane\nIl n’y a pas de méthode pour calculer directement la durée médiane (ou tout autre quantile) contrairement à l’approche actuarielle.\nLa définition retenue est conventionnelle. On va prendre la valeur de la durée qui se situe juste en dessous de 50% de survivant.e.s. Elle est donc définie tel que \\(S(t)\\leq0.5\\). Attention, il n’est pas impossible que le % de survivant.e.s soit bien en deçà de 50% pour l’obtention cette durée médiane.\nR-Stata-Sas-Python\n\nRStataSASPython\n\n\nLes estimateurs sont obtenus avec fonction survfit du package survival. On peut obtenir des rendus graphiques de meilleures qualité avec le package survminer (fonction ggsurvplot)\n\n\nAprès avoir appelé les variables de durée et de censure en mode survival avec stset), le tableau des estimateurs est obtenu avec la commande sts list et le graphique avec sts graph.\n\n\nL’estimation de Kaplan-Meier est affichée par défaut par la proc lifetest. Warning : le tableau affiché par SAS est particulièrement pénible à lire voire illisible, en particulier lorsque le nombre de censures est élevé, une ligne étant ajoutée pour chaque observation censurée. Je conseille de ne pas afficher cette partie de l’output (se reporter à la section SAS du chapitre programmation). On récupère pour le reste de l’output les valeurs de la durée pour S(t) =(.75,.5,.25) ainsi que le graphique, ce qui est suffisant.\n\n\nLes resultats sont donnés dans la librairie lifeline par des fonctions au nom interminable. Je conseille plutôt l’utilisation de la librairie statmodels (se reporter à la section dédiée à Python).\n\n\n\n\n\n5.3.2 Application\nOn reprend l’exemple précédent.\n\n\nAfficher le tableau des estimateurs\nTime    Total   Fail   Lost           Function     Error     [95% Conf. Int.]\n-------------------------------------------------------------------------------\n     1      103      1      0             0.9903    0.0097     0.9331    0.9986\n     2      102      3      0             0.9612    0.0190     0.8998    0.9852\n     3       99      3      0             0.9320    0.0248     0.8627    0.9670\n     5       96      2      0             0.9126    0.0278     0.8388    0.9535\n     6       94      2      0             0.8932    0.0304     0.8155    0.9394\n     8       92      1      0             0.8835    0.0316     0.8040    0.9321\n     9       91      1      0             0.8738    0.0327     0.7926    0.9247\n    11       90      0      1             0.8738    0.0327     0.7926    0.9247\n    12       89      1      0             0.8640    0.0338     0.7811    0.9171\n    16       88      3      0             0.8345    0.0367     0.7474    0.8937\n    17       85      1      0             0.8247    0.0375     0.7363    0.8857\n    18       84      1      0             0.8149    0.0383     0.7253    0.8777\n    21       83      2      0             0.7952    0.0399     0.7034    0.8614\n    28       81      1      0             0.7854    0.0406     0.6926    0.8531\n    30       80      1      0             0.7756    0.0412     0.6819    0.8448\n    31       79      0      1             0.7756    0.0412     0.6819    0.8448\n    32       78      1      0             0.7657    0.0419     0.6710    0.8363\n    35       77      1      0             0.7557    0.0425     0.6603    0.8278\n    36       76      1      0             0.7458    0.0431     0.6495    0.8192\n    37       75      1      0             0.7358    0.0436     0.6388    0.8106\n    39       74      1      1             0.7259    0.0442     0.6282    0.8019\n    40       72      2      0             0.7057    0.0452     0.6068    0.7842\n    43       70      1      0             0.6956    0.0457     0.5961    0.7752\n    45       69      1      0             0.6856    0.0461     0.5855    0.7662\n    50       68      1      0             0.6755    0.0465     0.5750    0.7572\n    51       67      1      0             0.6654    0.0469     0.5645    0.7481\n    53       66      1      0             0.6553    0.0472     0.5541    0.7390\n    58       65      1      0             0.6452    0.0476     0.5437    0.7298\n    61       64      1      0             0.6352    0.0479     0.5333    0.7206\n    66       63      1      0             0.6251    0.0482     0.5230    0.7113\n    68       62      2      0             0.6049    0.0487     0.5026    0.6926\n    69       60      1      0             0.5948    0.0489     0.4924    0.6832\n    72       59      2      0             0.5747    0.0493     0.4722    0.6643\n    77       57      1      0             0.5646    0.0494     0.4621    0.6548\n    78       56      1      0             0.5545    0.0496     0.4521    0.6453\n    80       55      1      0             0.5444    0.0497     0.4422    0.6357\n    81       54      1      0             0.5343    0.0498     0.4323    0.6261\n    85       53      1      0             0.5243    0.0499     0.4224    0.6164\n    90       52      1      0             0.5142    0.0499     0.4125    0.6067\n    96       51      1      0             0.5041    0.0499     0.4027    0.5969\n   100       50      1      0             0.4940    0.0499     0.3930    0.5872\n   102       49      1      0             0.4839    0.0499     0.3833    0.5773\n   109       48      0      1             0.4839    0.0499     0.3833    0.5773\n   110       47      1      0             0.4736    0.0499     0.3733    0.5673\n   131       46      0      1             0.4736    0.0499     0.3733    0.5673\n   149       45      1      0             0.4631    0.0499     0.3632    0.5571\n   153       44      1      0             0.4526    0.0499     0.3531    0.5468\n   165       43      1      0             0.4421    0.0498     0.3430    0.5364\n   180       42      0      1             0.4421    0.0498     0.3430    0.5364\n   186       41      1      0             0.4313    0.0497     0.3327    0.5258\n   188       40      1      0             0.4205    0.0497     0.3225    0.5152\n   207       39      1      0             0.4097    0.0495     0.3123    0.5045\n   219       38      1      0             0.3989    0.0494     0.3022    0.4938\n   263       37      1      0             0.3881    0.0492     0.2921    0.4830\n   265       36      0      1             0.3881    0.0492     0.2921    0.4830\n   285       35      2      0             0.3660    0.0488     0.2714    0.4608\n   308       33      1      0             0.3549    0.0486     0.2612    0.4496\n   334       32      1      0             0.3438    0.0483     0.2510    0.4383\n   340       31      1      1             0.3327    0.0480     0.2409    0.4270\n   342       29      1      0             0.3212    0.0477     0.2305    0.4153\n   370       28      0      1             0.3212    0.0477     0.2305    0.4153\n   397       27      0      1             0.3212    0.0477     0.2305    0.4153\n   427       26      0      1             0.3212    0.0477     0.2305    0.4153\n   445       25      0      1             0.3212    0.0477     0.2305    0.4153\n   482       24      0      1             0.3212    0.0477     0.2305    0.4153\n   515       23      0      1             0.3212    0.0477     0.2305    0.4153\n   545       22      0      1             0.3212    0.0477     0.2305    0.4153\n   583       21      1      0             0.3059    0.0478     0.2156    0.4008\n   596       20      0      1             0.3059    0.0478     0.2156    0.4008\n   620       19      0      1             0.3059    0.0478     0.2156    0.4008\n   670       18      0      1             0.3059    0.0478     0.2156    0.4008\n   675       17      1      0             0.2879    0.0483     0.1976    0.3844\n   733       16      1      0             0.2699    0.0485     0.1802    0.3676\n   841       15      0      1             0.2699    0.0485     0.1802    0.3676\n   852       14      1      0             0.2507    0.0487     0.1616    0.3497\n   915       13      0      1             0.2507    0.0487     0.1616    0.3497\n   941       12      0      1             0.2507    0.0487     0.1616    0.3497\n   979       11      1      0             0.2279    0.0493     0.1394    0.3295\n   995       10      1      0             0.2051    0.0494     0.1183    0.3085\n\n[Résultats non reportés à partir de t=1000 ]\n-------------------------------------------------------------------------------\n\n\nLa durée durée médiane de survie est \\(t=100\\). Elle correspond à \\(S(t)=0.4940\\).\n\nQuantiles de la fonction de séjour type Kaplan-Meier\n\n\nS(t)\nt\n\n\n\n\n0.90\n6\n\n\n0.75\n36\n\n\n0.50\n100\n\n\n0.25\n979\n\n\n0.1\n.\n\n\n\n\n\n\n\n\nCourbe de survie: estimation méthode Kaplan-Meier\n\n\n\n\n\nCourbe de survie: méthode Kaplan-Meier + CI\n\n\n\n\n\n\n5.3.3 Quantités associées à l’estimateur Kaplan-Meier..\nLe risque cumulé: estimateur de Nelson AAlen\nIl est simplément égal à:\n\\[H(t)=\\sum_{t_i\\leq k}q(t_i)\\]\n\n\n\nRisque cumulé: estimateur Nelson-Aalen\n\n\nLe risque ou taux de hasard instantané\nNécessite l’estimateur de risque cumulé de Nelson-Aalen. Le risque est obtenu en lissant les différences - toujours positive - entre \\(H(t)\\) par la méthode dite du kernel (cf estimation de la densité des distributions). Elle permet d’obtenir une fonction continue avec la durée (paramétrables sur les largeurs des fenêtres de lissage). D’autres méthodes de lissage sont maintenant possibles, et de plus en plus utilisées, en particulier celles utilisant des splines.\n\n\n\nRisque instantané: estimateur du Kernel\n\n\n\n\n\n\n\n\nNote\n\n\n\nIl n’est pas inutile de noter qu’il n’y a pas de formule toute faite pour obtenir des valeurs du risque instantané. Ce type de méthode par lissage est pleinement paramétrable, par exemple sa fenêtre, ce qui implique que son profil varie d’un paramétrage à l’autre. Le graphique précédent a été fait avec Stata, si on utilisait le package muhaz les différences de paramétrage par défaut font que les courbes ne se confondent pas."
  },
  {
    "objectID": "05-fsurvie_comp.html#tests-du-log-rank",
    "href": "05-fsurvie_comp.html#tests-du-log-rank",
    "title": "6  Tests de comparaison",
    "section": "6.1 Tests du log-rank",
    "text": "6.1 Tests du log-rank\nIl s’agit d’une série de tests qui répondent à la même logique, la seule différence réside dans le poids accordé au début ou à la fin de la période d’observation. Par ailleurs ces différents tests sont plus ou moins sensibles à la distribution des censures à droites entre les sous échantillons et à la non proportionalité des risques.\nDans leur logique, ces tests entrent dans le cadre des tests d’indépendance du Khi2, même si formellement ils relèvent des techniques dites de rang.\nIl s’agira donc de comparer des effectifs observés à des effectifs espérés à chaque moment d’évènement. La principale différence réside dans le calcul de la variance de la statistique du test qui, ici, suit assez logiquement une loi hypergéométrique [proche loi binomiale mais avec tirage avec remise].\n\n6.1.1 Principe de calcul de la statistique de test\n\nEffectifs observés en \\(t_i\\): \\(o_{i1}\\) et \\(o_{i2}\\) sont égaux à \\(d_{i1}\\) et \\(d_{i2}\\), et leur somme pour tous les temps d’évènement à \\(O_1\\) et \\(O_2\\).\nEffectifs expérés (hypothèse nulle \\(H_0\\)): comme pour une statistique du \\(\\chi^2\\) on se base sur les marges, avec le risque set (\\(R_i\\)) en \\(t_i\\) pour dénombrer les effectifs, soit \\(e_{i1}=R_{i1}\\times\\frac{d_i}{R_i}\\) et \\(e_{i2}=R_{i2}\\times\\frac{d_2}{R_i}\\). Leur somme pour tous les temps d’évènement est égale à \\(E_1\\) et \\(E_2\\). Le principe de calcul des effectifs observés reposent donc sur l’hypothèse d’un rapport des risques toujours égal à 1 au cours du temps (hypothèse fondamentale de risques proportionnels).\nStatistique du log-rank: \\((O_1 - E_1) = -(O_2 - E_2)\\).\nStatistique de test: sous \\(H_0\\), \\(\\frac{(O_1 - E_1)^2}{\\sum{v_i}}\\), avec \\(v_i\\) la variance de \\((o_{i1} - e_{i2})\\), suis un \\(\\chi^2(1)\\). Si on teste simultanément la différence de \\(g\\) fonctions de survie, ce qui n’est pas une bonne idée en passant, la statistique de test suis un \\(\\chi^2(g-1)\\).\n\n\n\n6.1.2 Les principaux tests log-rank\nLe principe de construction des effectifs observés et espérés reste le même dans chaque test, les différences résident dans les pondérations (\\(w_i\\)) qui prennent en compte, de manière différente, la taille de la population soumise au risque à chaque durée où au moins un évènement est observé.\n\nTest du log-rank: \\(w_i=1\\)\nIl accorde le même poids à toutes les durées d’évènement. C’est le test standard, le plus utilisé.\nTest de Wilconxon-Breslow-Grehan: \\(w_i=R_i\\)\nLes écarts entre effectifs observés et espérés sont pondérés par la population soumise à risque en \\(t_i\\). Le test accorde plus de poids au début de la période analysée, et il est sensible aux différences de distributions entre les strates des observations censurées.\nTest de Tarone-Ware: \\(w_i=\\sqrt{R_i}\\)\nVariante du test précédent, il atténue le poids accordé aux évènements au début de la période d’observation. Il est par ailleurs moins sensible au problème de la distribution des censures entre les strates.\nTest de Peto-Peto : \\(w_i=S_i\\)\nLa pondération est une variante de la fonction de survie KM (avec \\(R_i=R_i+1\\)). Le test n’est pas sensible au problème de distribution des censures.\nTest de Fleming-Harington: \\(w_i=(S_i)^p\\times(1-S_i)^{q}\\) avec \\(0\\leq{p}\\leq{1}\\) Il permet de paramétrer le poids accordé au début où à la fin de temps d’observation. Si \\(p=q=0\\) on retrouve le test de base non pondéré.\n\nEn pratique/remarques:\n\nLes tests du log-rank sont sensibles à l’hypothèse de risques proportionnels (voir modèle semi-paramétrique de Cox). En pratique si des courbes de séjours se croisent, il est fortement déconseillé de les utiliser. Cela ne signifie pas que si les courbes ne se croisent pas, l’hypothèse de proportionalité des risques est respectée : des rapports de risque peuvent au cours du temps s’intensifier, se réduire ou, le cas échant s’inverser, ce qui est typique d’un croisement.\nEffectuer un test global (multiple/omnibus) sur un nombre important de groupes (ou &gt;2) peut rendre le test très facilement significatif. Il peut être intéressant de tester des courbes deux à deux (idem qu’une régression avec covariable discrète), en conservant un seul degré de liberté. Des méthodes de correction du test multiple sont possibles ou disponibles si on utilise R.\n\nR-Stata-Sas-Python\n\nRStataSasPython\n\n\nOn utilise la fonction survdiff de la librairie survival. Le résultat du test de Peto-Peto est affiché par défaut (rho=1). Si on souhaite utiliser le test non pondéré, on ajoute l’option rho=0. Pour obtenir le résultat d’un test multiple corrigé (plus d’un degré de liberté), on peut utiliser la fonction pairwise_survdiff du package survminer. Cette fonction permet également d’obtenir des tests 2 à 2.\nJe conseille de rester sur l’option Peto-Peto et dans le cas d’une variable à plus de deux modalités, d’utiliser la fonction de survminer pairwise_survdiff.\n\n\nOn utilise la commande sts test avec le nom de la version du test: peto, wilcoxon . Sans préciser le nom de la variante, le test non pondéré est exécuté.\n\n\nLe test non pondéré et la version Wilcoxon sont données avec l’option strata de la proc lifetest. Attention : ne jamais utiliser la version LR Test qui est biaisée. Pour obtenir d’autres versions du test du log-rank, on ajoute /test=all à l’option strata.\n\n\nAvec la librairie lifelines, on utilise la fonction logrank_test. Quatre variantes sont disponibles (Wilcoxon, Tarone-Ware, Peto-Peto et Fleming-Harrigton). On peut également utiliser la fonction duration.survdiff de statmodels (non pondéré, Wilcoxon - appelé ici Breslow- et Tarone-Ware).\n\n\n\n\n\n6.1.3 Application\nOn compare ici l’effet du pontage coronarien sur le risque de décéder depuis l’inscription dans le registre de greffe.\n\n\n\n\n\n\nRésultats des tests du logrank\n\n\nTest\ndf\nChi2\nP&gt;Chi2\n\n\n\n\nNon pondéré\n1\n6.59\n0.0103\n\n\nWilcoxon (Breslow)\n1\n8.99\n0.0027\n\n\nTarone-Ware\n1\n8.46\n0.0036\n\n\nPeto-Peto\n\n8.66\n0.0033\n\n\n\nLes résultats font apparaître que l’opération permet d’augmenter la durée de survie des personnes. Il apparait que la p-value est plus élevée pour test non pondérée. Cela peut-il s’expliquer en regardant les deux courbes de séjours? Qu’en est-il de la proportionalité des risques ???? …. Réponse pendant la formation."
  },
  {
    "objectID": "05-fsurvie_comp.html#comparaison-des-rmst",
    "href": "05-fsurvie_comp.html#comparaison-des-rmst",
    "title": "6  Tests de comparaison",
    "section": "6.2 Comparaison des RMST",
    "text": "6.2 Comparaison des RMST\nRMST: Restricted Mean of Survival Time\nLa comparaison des RMST est une alternative pertinente aux tests du log-rank car elle ne repose pas sur des hypothèses contraignantes (proportionnalité des risques, distribution des censures), et permet une lecture vivante basée sur des espérances de séjour et non sur la lecture d’une simple p-value traduisant l’homogénéité ou non des fonctions de séjour. Par ailleurs les comparaisons sont souples, on peut choisir un ou plusieurs points d’horizon pour alimenter l’analyse.\nPrincipe\n\nL’aire sous la fonction de survie représente la durée moyenne d’attente jusqu’à l’évènement, soit une espérance de survie.\nEn présence de censure à droite, il faut borner la durée maximale \\(t^*&lt;\\infty\\). L’espérance de survie s’interprète donc sur un horizon fini. On est très proche d’une mesure en analyse démographique type « espérance de vie partielle ».\n\\(RMST =\\int_0^{t^*}S(t)dt\\).\nOn peut facilement comparer les RMST de deux groupes, en termes de différence ou de ratio.\nPar défaut on définit généralement \\(t^*\\) à partir le temps du dernier évènement observé. Il est néanmoins possible de calculer le RMST sur des intervalles plus court, ce qui lui permet une véritable souplesse au niveau de l’analyse.\n\nR-Stata-Sas-Python\nAttention, selon les logiciels la durée max par défaut n’est pas la même. Pour R et Sas, il s’agit du dernier évènement observé sur l’ensemble de l’échantillon, alors que Stata prend la durée qui correspond au dernier évènement observé le plus court des deux groupes . Cela affectera légèrement la valeur des Rmst estimées par défaut.\nPour l’exemple, la durée maximale utilisée par R est de 1407 jours alors que pour Stata elle est de 995 jours.\n\nRStataSASPython\n\n\nLibrairie SurvRm2. Programmée par les mêmes personnes que la commande Stata, la fonction proposée n’est pas très souple.\n\n\nCommande externe strmst2. La plus ancienne fonction proposée par les logiciels. Au final plus limitée que la solution Sas. J’ai programmé une commande, diffrmst, qui représente graphiquement les estimations des Rmst pour chaque temps d’évènement, leurs différences et les p-value issues des comparaisons.\n\n\nDisponible depuis la version 15.1 de SAS/Stat (fin 2018). Les estimations et le résultat du test de comparaison sont récupérables très simplement dans une proc lifetest, avec en option **plots=(rmst)** . Bien que sortie tardivement par rapport Stata et R, les résultats sont particulièrement complets.\n\n\nEstimation un peu pénible. A partir de l’estimateur KM obtenu avec la fonction KaplanMeierFitter de lifelines, on peut obtenir les RMST avec la fonction restricted_mean_survival_time. On peut tracer les fonctions, en revanche le test de comparaison n’est pas implémenté.\n\n\n\nApplication\nAvec \\(tmax=1407\\):\n\nEstimation des Rmst pour la variable surgery\n\n\nGroupes\nRMST\nStd. Err\n95% CI\n\n\n\n\n\\(surgery=1\\)\n884.576\n187.263\n517.546 - 1251.605\n\n\n\\(surgery=0\\)\n379.148\n61.667\n258.282 - 500.014\n\n\n\n\nDifférences entre Rmst pour la variable surgery\n\n\n\n\n\n\n\n\nTypes de contraste\nEcarts RMST\nP&gt;|z|\n95% CI\n\n\n\n\n\\(Rmst(surgery1 - surgery0)\\)\n505.428\n0.010\n517.546 - 1251.605\n\n\n\\(Rmst\\left(\\frac{surgery1}{surgery0}\\right)\\)\n2.333\n0.002\n1.383 - 3.937\n\n\n\nIci \\(t^*\\) est égal à 1407 jours, soit la durée qui correspond au dernier décès observé.\nSur un horizon de 1407 jours, ces individus opérés d’un pontage peuvent espérer vivre 884 jours en moyenne, contre 379 jours pour les autres. La durée moyenne de survie est donc 2.3 fois plus importante pour les personnes opérées (rapport des Rmst = 2.3 ), ce qui correspond à une différence de 379 jours.\nLe tableau et le graphique suivant donnent les valeurs des Rmst et les écarts de la variable surgery en faisant varier \\(tmax\\) sur chaque jour où au moins un décès a été observé. Il a été réalisé avec Stata, la durée maximale utilisée a été paramétrée à 1407 jours (idem R, Sas).\nComme le premier décès observé pour les personnes opéré se situe le 165eme jours, il est tout à fait normal que pour ce groupe de personnes la valeur de la Rmst soit identique au jour de décès des individus non opérés.\n\n\n\n\n\n\nNote\n\n\n\nPour la version pdf, seulement une dizaine de points a été sélectionné en raison de la longueur du tableau\n\n\n\n\nAfficher le tableau\n  +--------------------------------------------------------------------------+\n  | _time     _rmst1     _rmst0      _diff    95%CI lower 95%CI upper  pvalue|\n  |--------------------------------------------------------------------------|\n  |--------------------------------------------------------------------------|\n  |     1          1          1          0           0          0          . |\n  |     2          2   1.989011    .010989   -.0104304   .0324084   .3146368 |\n  |     3          3   2.945055   .0549451   -.0009099      .1108   .0538507 |\n  |     5          5   4.791209   .2087912    .0549289   .3626535   .0078217 |\n  |     6          6   5.692307   .3076923    .0995576   .5158269   .0037617 |\n  |--------------------------------------------------------------------------|\n  |     8          8    7.45055   .5494505    .2224352   .8764658   .0009908 |\n  |     9          9   8.318682   .6813186    .2913915   1.071246   .0006156 |\n  |    11         11   10.03297   .9670329    .4461406   1.487925   .0002741 |\n  |    12         12   10.89011    1.10989    .5212656   1.698514   .0002193 |\n  |    16         16   14.27415   1.725846    .8588567   2.592834   .0000956 |\n  |--------------------------------------------------------------------------|\n  |    17         17   15.08677    1.91323    .9769751   2.849484    .000062 |\n  |    18         18   15.88825   2.111745     1.10533   3.118161   .0000391 |\n  |    21         21   18.25931   2.740688    1.516228   3.965148   .0000115 |\n  |    28         28   23.63593   4.364065    2.598741   6.129389   1.26e-06 |\n  |    30         30   25.14985    4.85015    2.924714   6.775586   7.93e-07 |\n  |--------------------------------------------------------------------------|\n  |    31         31   25.89568   5.104324    3.098862   7.109787   6.08e-07 |\n  |    32         32    26.6415   5.358499    3.272218   7.444779   4.80e-07 |\n  |    35         35   28.84508   6.154923    3.825102   8.484744   2.24e-07 |\n  |    36         36    29.5683   6.431698    4.020457   8.842939   1.71e-07 |\n  |    37         37   30.28023   6.719774    4.227545   9.212004   1.26e-07 |\n  |--------------------------------------------------------------------------|\n  |    39         39   31.68147   7.318526    4.664121   9.972931   6.52e-08 |\n  |    40         40    32.3708   7.629202    4.893653   10.36475   4.60e-08 |\n  |    43         43   34.37097   8.629034    5.651719   11.60635   1.34e-08 |\n  |    45         45   35.68181   9.318189    6.177435   12.45894   6.07e-09 |\n  |    50         50   38.90242   11.09758    7.539261    14.6559   9.80e-10 |\n  |--------------------------------------------------------------------------|\n  |    51         51   39.53524   11.46476    7.822028   15.10748   6.89e-10 |\n  |    53         53   40.77829   12.22171    8.410756   16.03267   3.27e-10 |\n  |    58         58   43.82939   14.17061    9.933216     18.408   5.58e-11 |\n  |    61         61   45.62615   15.37385    10.87721   19.87049   2.07e-11 |\n  |    66         66   48.56425   17.43575    12.50275   22.36874   4.28e-12 |\n  |--------------------------------------------------------------------------|\n  |    68         68   49.71689   18.28311      13.175   23.39121   2.30e-12 |\n  |    69         69   50.27061   18.72939    13.53629   23.92248   1.56e-12 |\n  |    72         72   51.89787   20.10213    14.65526   25.54901   4.71e-13 |\n  |    77         77   54.49696   22.50304    16.63758   28.36851   5.51e-14 |\n  |    78         78   55.00547   22.99453    17.04528   28.94377   3.57e-14 |\n  |--------------------------------------------------------------------------|\n  |    80         80   55.99991   24.00009    17.88509   30.11509   1.44e-14 |\n  |    81         81   56.48582   24.51418    18.31722   30.71113   8.88e-15 |\n  |    85         85   58.38429   26.61571    20.09202    33.1394   1.33e-15 |\n  |    90         90   60.70087   29.29913    22.36311   36.23515   2.22e-16 |\n  |    96         96   63.41296   32.58704    25.15034   40.02373          0 |\n  |--------------------------------------------------------------------------|\n  |   100        100   65.17583   34.82418    27.05229   42.59606          0 |\n  |   102        102   66.03465   35.96535     28.0274   43.90329          0 |\n  |   109        109   68.96146   40.03855     31.5203   48.55679          0 |\n  |   110        110   69.37957   40.62043     32.0176   49.22326          0 |\n  |   131        131   77.91607   53.08393    42.67269   63.49517          0 |\n  |--------------------------------------------------------------------------|\n  |   149        149   85.23307   63.76693    51.71735   75.81651          0 |\n  |   153        153   86.81125   66.18875     53.7781    78.5994          0 |\n  |   165        165   91.40231   73.59769    60.11884   87.07655          0 |\n  |   180   178.6364   97.14113   81.49523    66.43188   96.55859          0 |\n  |   186   184.0909   99.43666   84.65425     68.8452   100.4633          0 |\n  |--------------------------------------------------------------------------|\n  |   188   185.7273   100.2018   85.52544    69.46069   101.5902          0 |\n  |   207   201.2727   107.2365    94.0362    75.10156   112.9708          0 |\n  |   219   211.0909   111.5314   99.55952    78.49335   120.6257          0 |\n  |   263   247.0909   126.7362   120.3547     90.2882   150.4212   4.22e-15 |\n  |   265   248.7273   127.4026   121.3246    90.82352   151.8258   6.44e-15 |\n  |--------------------------------------------------------------------------|\n  |   285   265.0909   134.0671   131.0238     96.0849   165.9628   1.98e-13 |\n  |   308   283.9091   141.1416   142.7675    102.6688   182.8662   2.99e-12 |\n  |   334   305.1818   148.8057   156.3761    110.3439   202.4082   2.77e-11 |\n  |   340   310.0909   150.4975   159.5934    112.1866   207.0003   4.16e-11 |\n  |   342   311.7273   151.0358   160.6915    112.8294   208.5536   4.69e-11 |\n  |--------------------------------------------------------------------------|\n  |   370   332.0909   158.5717   173.5192    119.5852   227.4532   2.87e-10 |\n  |   397   351.7273   165.8385   185.8887    125.7134   246.0641   1.41e-09 |\n  |   427   373.5454   173.9127   199.6327    132.2074    267.058   6.51e-09 |\n  |   445   386.6364   178.7573   207.8791    135.9845   279.7736   1.45e-08 |\n  |   482   413.5454   188.7155     224.83    143.5408   306.1191   5.93e-08 |\n  |--------------------------------------------------------------------------|\n  |   515   437.5454   197.5971   239.9483    150.1031   329.7935   1.65e-07 |\n  |   545   459.3636   205.6714   253.6923    155.9623   351.4222   3.62e-07 |\n  |   583        487   215.8987   271.1013    163.2743   378.9283   8.32e-07 |\n  |   596   494.8788   219.3976   275.4812    164.6331   386.3293   1.11e-06 |\n  |   620   509.4243   225.8569   283.5673     166.951   400.1836   1.88e-06 |\n  |--------------------------------------------------------------------------|\n  |   670   539.7273    239.314   300.4133    171.1152   429.7114   5.27e-06 |\n  |   675   542.7576   240.6597   302.0979    171.4897   432.7061   5.80e-06 |\n  |   733   577.9091    254.969   322.9401     176.861   469.0191   .0000147 |\n  |   841   643.3636   279.1917   364.1719    187.8796   540.4643   .0000515 |\n  |   852   650.0303   281.6588   368.3715    188.9058   547.8372   .0000575 |\n  |--------------------------------------------------------------------------|\n  |   915   688.2121   294.2187   393.9934    196.3119   591.6749   .0000937 |\n  |   941   703.9697   299.4022   404.5675    199.2493   609.8857   .0001125 |\n  |   979        727    306.978    420.022    203.4389   636.6051   .0001441 |\n  |   995   734.7576   310.1678   424.5898    204.0643   645.1152   .0001609 |\n  |  1032   748.2121   317.5443   430.6678    202.7468   658.5889   .0002127 |\n  |--------------------------------------------------------------------------|\n  |  1141   787.8485   335.6531   452.1953    200.7097    703.681   .0004248 |\n  |  1321    853.303   365.5577   487.7454    191.5434   783.9473   .0012492 |\n  |  1386   876.9394   376.3565   500.5829    186.9499   814.2158   .0017585 |\n  |  1400   882.0303   378.2173    503.813    186.4392   821.1869   .0018625 |\n  |  1407   884.5757   379.1476   505.4281    186.1745   824.6817   .0019162 |\n  +--------------------------------------------------------------------------+\n\n\n\n\n\nComparaison des Rmst à chaque jour où au moins un décès est observé"
  },
  {
    "objectID": "07-cox.html#le-modèle-semi-paramétrique-de-cox",
    "href": "07-cox.html#le-modèle-semi-paramétrique-de-cox",
    "title": "8  Le modèle de Cox",
    "section": "8.1 Le modèle semi-paramétrique de Cox",
    "text": "8.1 Le modèle semi-paramétrique de Cox\n\n8.1.1 La vraisemblance partielle et estimation des paramètres\nOn se situe dans une situation où la durée est mesurée sur une échelle strictement continue. Il ne peut donc y avoir qu’un seul évènement observé en \\(t_i\\) (idem pour la censure).\nOn peut représenter le processus aléatoire d’une analyse de survie en présence de censure à droite, avec l’équation de vraisemblance suivante:\n\\[L_i=f(t_i)^{\\delta_i}S(t_i)^{1-\\delta_i}\\]\n\n\\(f(t_i)\\) est la valeur de la fonction de densité en \\(t_i\\)\n\\(S(t_i)\\) est la valeur de la fonction de survie en \\(t_i\\)\n\\(\\delta_i=1\\) si l’évènement est observé: \\(L_i=f(t_i)\\)\n\\(\\delta_i=0\\) si l’observation est censurée: \\(L_i=S(t_i)\\)\n\nVraisemblance partielle de Cox\nComme \\(f(t_i)=h(t_i)\\times S(t_i)\\) 1, on obtient: \\(L_i=[h(t_i)S(t_i)]^{\\delta_i}S(t_i)^{1-\\delta_i} = h(t_i)^{\\delta_i}S(t_i)\\).\nPour \\(i=1,2,.....,n\\), la vraisemblance s’ecrit donc: \\(L_i=\\prod_{i=1}^{n}h(t_i)^{\\delta_i}S(t_i)\\).\nOn peut réécrire cette vraisemblance en la multipliant et en la divisant par: \\(\\sum_{j\\in R_i}h(t_i)\\), où \\(j\\in R_i\\) est l’ensemble des observations soumises au risque en \\(t_i\\).\n\\[L=\\prod_{i=1}^{n}\\left[h(t_i)\\frac{\\sum_{j\\in R}h(t_i)}{\\sum_{j\\in R}h(t_i)}\\right]^{\\delta_i}S(t_i)= \\prod_{i=1}^{n}\\left[\\frac{h(t_i)}{\\sum_{j\\in R_i}h(t_i)}\\right]^{\\delta_i}\\sum_{j\\in R_i}h(t_i)^{\\delta_i}S(t_i)\\]\nLa vraisemblance partielle retient seulement le premier terme de la vraisemblance, soit:\n\\[PL=\\prod_{i=1}^{n}\\left[\\frac{h(t_i)}{\\sum_{j\\in R}h(t_i)}\\right]^{\\delta_i}\\]\nUne fois remplacée la valeur de \\(h(t_i)\\) par son expression en tant que modèle à risques proportionnels, la vraisemblance partielle ne dépendra plus de la durée. Mais elle va dépendre de l’ordre d’arrivée des évènements, c’est à dire leur rang.\nRemarque: pour les observations censurées(\\(\\delta_i=0\\)), \\(PL=1\\). Toutefois, ces censures à droite entrent dans l’expression \\(\\sum_{j\\in R}h(t_i)\\) tant qu’elles sont soumises au risque.\nEn remplaçant donc \\(h(t_i)\\) par l’expression \\(h_0(t)e^{X_i^{'}b}\\):\n\\[PL=\\prod_{i=1}^{n}\\left[\\frac{h_0(t)e^{X_{i}^{'}b}}{\\sum_{j\\in R_i}h_0(t)e^{X_{j}^{'}b}}\\right]^{\\delta_i} =\\prod_{i=1}^{n}\\left[\\frac{e^{X_i^{'}b}}{\\sum_{j\\in R_i}e^{X_{j}^{'}b}}\\right]^{\\delta_i}\\]\nL’expression \\(\\frac{e^{Xb}}{\\sum_{j\\in R}e^{Xb}}\\) est donc bien une probabilité, et la vraisemblance partielle est donc bien un produit de probabilités. Pour un individu ayant connu l’évènement, la contribution à la vraisemblance partielle est la probabilité que l’individu observe l’évènement en \\(t_i\\) sachant qu’un évènement (et un seul) s’est produit.\n\nSi \\(\\delta_i = 0\\): \\(PL_i = 1\\)\nSi \\(\\delta_i = 1\\): \\(PL_i =\\frac{e^{X_i^{'}b}}{\\sum_{j\\in R_i}e^{X_{j}^{'}b}}\\)\n\nCondition nécessaire: pas d’évènement simultané: en présence d’évènements mesurés simultanément, l’estimation de la vraisemblance doit faire l’objet d’une correction.\nCorrection de la vraisemblance avec des évènements simultanés:\n\nLa méthode dite exacte: Comme il ne doit pas y avoir d’évènement simultané, on va introduire à la vraisemblance partielle toutes les permutations possibles des évènements observés au même moment. Bien qu’en \\(t_i\\) on observe au même moment l’évènement pour 2 observations (A,B) une métrique temporelle plus précise permettrait de savoir si A s’est produit avant B ou B s’est produit avant A (2 permutations). Comme le nombre de permutations est calculé à l’aide d’une factorielle 2, avec 3 évènements mesurés simultanément, on obtient 6 permutations (\\(3\\times2\\times1\\)). Problème: le nombre de permutations pour chaque \\(t_i\\) peut devenir très vite particulièrement élevé. Par exemple pour 10 évènements simultanés, le nombre de permutations est égal à 3,628,800. Le temps de calcul devient extrêmement long, et ce type de correction totalement inopérant.\nLa méthode dite de Breslow: il s’agit d’une approximation de la méthode exacte permettant de ne pas avoir à intégrer chaque permutation. Cette approximation est utilisée par défaut par les logiciels Sas et Stata.\nLa méthode dite d’Efron: elle corrige l’approximation de Breslow, et est jugée plus proche de la méthode exacte. C’est la méthode utilisée par défaut avec le logiciel R, et elle est disponible avec les autres applications.\n\n\n\n8.1.2 Estimation des paramètres\nOn utilise la méthode habituelle, à savoir la maximisation de la log-vraisemblance (ici partielle).\n\nConditions de premier ordre: calcul des équations de score à partir des dérivées partielles. Solution: \\(\\frac{\\partial log(PL)}{\\partial{b_k}}=0\\). On ne peut pas obtenir de solution numérique directe.\n\nRemarque: les équations de score sont utilisées pour tester la validité de l’hypothèse de constance des rapports de risque pour calculer les résidus de Schoenfeld (voir plus loin).\n\nConditions de second ordre: calcul des dérivées secondes qui permettent d’obtenir la matrice d’information de Fisher et la matrice des variances-covariances des paramètres.\nComme il n’y a pas de solution numérique directe, on utilise un algorithme d’optimisation (ex: Newton-Raphson) à partir des équations de score et de la matrice d’information de Fisher.\n\nEléments de calcul\nEn logarithme (sans évènement simultané), la vraisemblance partielle s’ecrit:\n\\[pl(b)=\\sum_{i=1}^n\\delta_i\\left(log(e^{X_{i}^{'}b})-log\\sum_{j\\in R_i}e^{X_{j}^{'}b}\\right)\\]\n\\[pl(b)=\\sum_{i=1}^n\\delta_i\\left(X_{i}^{'}b-log\\sum_{j\\in R_i}e^{X_{j}^{'}b}\\right)\\]\nCalcul de l’équation de score pour une covariable \\(X_k\\):\n\\[\\frac{\\partial pl(b)}{\\partial{b_k}}=\\sum_{i=1}^n\\delta_i\\left(X_{ik}-\\sum_{j\\in R_i}X_{jk}\\frac{e^{X_{j}^{'}b}}{\\sum_{j\\in R_i}e^{X_{j}^{'}b}}\\right)\\]\nComme \\(\\frac{e^{X_{j}b}}{\\sum_{j\\in R}e^{X_{j}b}}\\) est une probabilité, et \\(\\sum_{j\\in R}X_{ik}\\times p_i\\) est l’espérance (la moyenne) \\(E(X_k)\\) d’avoir la caractéristique \\(X_k\\) lorsqu’un évènement a été observé. Au final:\n\\[\\frac{\\partial lp(b)}{\\partial{b_k}}= \\sum_{i=1}^n\\delta_i\\left(X_{ik} - E(X_{j\\in R_i,k}) \\right)\\]\nCette expression va permettre d’analyser le respect ou non de l’hypothèse de risques proportionnels via les résidus de Schoenfeld.\n\n\n8.1.3 Lecture des résultats\nComme il s’agit d’un modèle à risque proportionnel, les rapports de risques sont constants pendant toute la période d’observation. Il s’agit d’une propriété de l’estimation.\nCovariable binaire (indicatrice) \\(X=(0,1)\\): \\(RR=\\frac{h(t\\ |\\ X=1)}{h(t\\ |\\ X=0)}=e^b\\).\nA chaque moment de la durée \\(t\\), le risque d’observer l’évènement est \\(e^b\\) fois plus important/plus faible pour \\(X=1\\) que pour \\(X=0\\).\nCovariable quantitative (fixe dans le temps)\n\\(RR=\\frac{h(t\\ |\\ X=a+c)}{h(t\\ |\\ X=a)}=e^{c \\times{b}}\\). On prendra pour illustrer une variable type âge au début de l’exposition au risque (a) et un delta de comparaison avec un âge inférieur c.\nSi \\(c=1\\) (résultat de l’estimation): A un âge donnée, le risque de connaitre l’évènement est \\(e^b\\) fois inférieur/supérieur à celui d’une personne qui a un an de moins.\nExemple pour les insuffisances cardiaques\n\nCorrection de la vraisemblance: méthode d’Efron\nNombre d’observations: 103\nNombre de décès: 75\nLog-Vraisemblance: -289.30639\n\n\nCox: log Hazard Ratio (Risks Ratio)\n\n\nVariables\nlogRR\nStd.Err\nz\n\\(P&gt;|z|\\)\n95% IC\n\n\n\n\nyear\n-0.119\n0.0673\n-1.78\n0.076\n-0.2516;+0.0124\n\n\nage\n+0.0296\n0.0135\n2.19\n0.029\n+0.0031;+0.0561\n\n\nsurgery\n-0.9873\n0.4363\n-2.26\n0.024\n-1.8424;-0.1323\n\n\n\n\nCox: Hazard Ratio (Risks Ratio)\n\n\nVariables\nRR\nStd.Err\nz\n\\(P&gt;|z|\\)\n95%CI\n\n\n\n\nyear\n0.8872\n0.0597\n-1.78\n0.076\n0.7775; 1.0124\n\n\nage\n1.0300\n0.0139\n2.19\n0.029\n1.0031; 1.0577\n\n\nsurgery\n0.3726\n0.1625\n-2.26\n0.024\n0.1584; 0.8761\n\n\n\nOn retrouve les des tests non paramétriques pour l’opération, à savoir qu’un pontage réduit les risques journaliers de décès pendant la période d’observation (augmente la durée de survie).\nDe la même manière, plus on entre à un âge élevé dans la liste d’attente plus le risque de décès augmente. La variable year, qui traduit des progrès en médecine, renvoie à une réduction plutôt modérée du risque journalier de décès durant l’attente d’une greffe.\nR-Stata-Sas-Python\n\nR\n\n\nLe modèle est estimé avec la fonction coxph de la librairie survival. Hors options, la syntaxe est identiques aux fonctions survfit et survdif.\n\n8.1.3.1 Stata\nLe modèle est estimé avec la commande stcox.\n\n\n8.1.3.2 SAS\nLe modèle est estimé avec la proc phreg.\n\n\n8.1.3.3 Python\nAvec la librairie lifelines, le modèle est estimé avec la fonction CoxPHFitter. Avec la librairie statmodels, il est estimé avec la fonction smf.phreg."
  },
  {
    "objectID": "07-cox.html#analyse-de-la-constance-des-rapports-de-risque",
    "href": "07-cox.html#analyse-de-la-constance-des-rapports-de-risque",
    "title": "8  Le modèle de Cox",
    "section": "8.2 Analyse de la constance des rapports de risque",
    "text": "8.2 Analyse de la constance des rapports de risque\n\nLes rapports de risque (RR) estimés par le modèle sont contraints à être constant sur toute la période d’observation. C’est une hypothèse forte.\nLe respect de cette hypothèse doit être analysé, en particulier pour le modèle de Cox où la baseline du risque est habituellement estimée à l’aide de ces rapports (par exemple la méthode dite de Breslow, non traitée). En post-estimation, les valeurs estimées du risque pourront présenter des valeurs aberrantes si on dévie trop de constance, en particulier en obtenant des négatives des taux de risque.\nAnalyser cette hypothèse revient à introduire une interaction entre les rapports et la durée ou plutôt précisément une fonction de la durée).\nPlusieurs méthodes disponibles, on traitera celles basées sur les résidus de Schoenfeld, et l’introduction directe d’une intéraction entre une fonction la durée et les covariables du modèle. Cette dernière fait également office de méthode de correction lorsque la violation de l’hypothèse est jugée trop importante ou problématique du point de vue des résultats obtenus.\nSi on regarde les courbes de Kaplan-Meier, leurs croisement non tardif impliquera nécessairement un problème sur cette hypothèse.\n\n\n8.2.1 Test de Grambsch-Therneau sur les résidus de Schoenfeld\nCe test a été proposé par P.Grambsch et T.Therneau 3 dans un cadre à durée strictement continue. Il repose originellement sur une régression linéaire estimé avec les moindres carrés généralisés (GLS) correction de l’autocorrélation des erreurs avec des sér) . Dans un premier temps pour des raisons plutôt pratiques (informatique), le test a une version moindres carrés ordinaires (OLS). Jusqu’en 2020, tous les logiciels ne proposaient que le test OLS. T.Therneau avec la V3 de package survival a substitué - assez brutalement - le test GLS au test OLS. Si les résultats sont proches dans le cadre d’une durée continue et que le test GLS peut être considéré comme un test exact, cela devient problématique dans une situation de durée discrète/groupée 4. Le test OLS reste, à mon sens, la méthode à privilégier dans le cas discret.\nIl est également important de souligner que pour P.Grambsch et T.Therneau 5 n’est qu’un moyen parmi d’autres d’analyser une violation de l’hypothèse de proportionnalité. Ce n’est pas the solution (comme tout autre test au passage). Le croisement des courbes de séjours peut-être suffisant pour alerter sur cette violation.\nPrincipe du test: consiste à regarder la corrélation entre les résidus de Schoenfeld obtenus directement avec la fonction de score de la vraisemblanc partielle de Cox et une fonction de la durée.\nPrincipe de calcul des résidus\n\nLes résidus bruts sont directement calculés à partir des équations de scores [voir section estimation].\nIls ne sont calculés que pour les observations qui ont connues l’évènement, au moment où un évènement s’est produit.\nLa somme des résidus pour chaque covariable est égale à 0. Il s’agit de la propriété de l’équation de score à l’équilibre.\nOn utilise généralement les résidus standardisés (remis à l’échelle / scaled) - par leur variance -. C’est la mesure de cette variance qui distingue le test OLS du test GLS.\n\nPour une observation dont l’évènement s’est produit en \\(t_i\\), le résidu brut de Schoenfeld pour la covariable \\(X_k\\), après estimation du modèle, est égal à:\n\\[rs_{ik}=X_{ik}- \\sum_{j\\in R_i}X_{jk}\\frac{e^{X_{j}^{'}b}}{\\sum_{j\\in R_i}e^{X_{j}^{'}b}}= X_{ik} - E(X_{j\\in R_i})\\]\n\nCe résidu est formellement la contribution d’une observation ou d’un moment d’évènement au score. Il se lit comme la différence entre la valeur observée d’une covariable et sa valeur espérée au moment où l’évènement s’est produit.\nSi la constance des rapports de risque varie peu les résidus ne doivent pas suivre une tendance précise localement ou globalement, à la hausse ou à la baisse.\n\nPourquoi?\nPar l’exemple, sans censure à droite et en ne considérant que les résidus bruts: Avec un rapport de risque strictement égal à 1 en début d’exposition, une population soumise au risque \\(R_i=100\\) avec 50 hommes et 50 femmes. Si l’hypothèse PH (strictement) respectée, lorsqu’il reste 90 personnes soumises au risque, on devrait avoir 45 hommes et 45 femmes. Avec \\(R_i=50\\), 25 hommes et 25 femmes,…….avec \\(R_i=10\\), 5 hommes et 5 femmes.\nAu final l’espérance d’avoir la caractéristique \\(X\\) est toujours égal à 0.5 et les résidus bruts prendront toujours la valeur -.5 si \\(X=0\\) et .5 si \\(X=1\\). En faisant une simple régression linéaire entre les résidus, qui alternent ces deux valeurs, et \\(t\\), le coefficient estimé sera en toute logique très proche de 0.\nDe manière encore plus simple, cette proportionnalité avec un risque ratio égal à 1 suggère qu’au cours de la durée d’observation, on observe une succession d’un même nombre d’hommes et de femmes qui connaissent l’évènement. Si tous les hommes ou presque avaient observés l’évènement plutôt en début d’éxposition et si toutes les femmes ou presque avaient observé l’évènement plutôt en fin d’exposition, l’hypothèse de proportionnalité pourraient fortement remise en cause.\nOn trouvera des éléments de calcul du test OLS ici\n\n\n\n\n\n\nAvertissement\n\n\n\n\nTest omnibus: Ne pas l’utiliser bien qu’il figure généralement en bas des output. Il n’a pas d’interprétation directe, et les p-value peuvent présenter des valeurs très faibles alors que ce n’est pas le cas pour les covariables prises une à une. Rester comme c’est souvent le cas à un test à un degré de liberté.\nTransformations de la durée: n’importe quelle fonction de la durée peut être utilisée pour réaliser le test. On retient généralement les fonctions suivantes: \\(g(t)=t\\) (« identity »), \\(g(t)=log(t)\\), \\(g(t)=KM(t)\\) ou \\(g(t)=1- S(t\\)) où \\(S(t\\)) est l’estimateur de Kaplan-Meier. Enfin une transformation appelée « rank » est utilisée seulement pour les durées strictement continue ou suffisamment dispersées . Par exemple \\(t=(0.1,0.5,1,2.6,3)\\) donne une transformation \\(t=(1,2,3,4)\\). A savoir : $g(t)=t rend le test relativement sensible aux évènements tardifs lorsque la population restant soumise est peu nombreuse (outliers).\nPar défaut Stata, Sas, Python: \\(g(t)=t\\)\nPar défaut R: \\(g(t)= 1 - S(t)\\)\n\n\n\nPour des raisons de reproductibilité dans l’espace des logiciels et dans le temps pour les différentes versions du package survival de R, on ne présente ici que la version OLS.\nTest OLS avec \\(g(t)=t\\)\n\nTest OLS Grambsch-Therneau avec \\(g(t)=t\\)\n\n\nVariables\nchi2\ndf\nP&gt;Chi2\n\n\n\n\nyear\n0.80\n1\n0.3720\n\n\nage\n1.61\n1\n0.2043\n\n\nsurgery\n5.54\n1\n0.0186\n\n\n\nIci l’hypothèse de proportionnalité des risques est questionnable pour la variable surgery. Le risque ratio pourrait ne pas constant dans le temps. Ce n’est pas du tout étonnant, le premier décès pour les personnes opérées d’un pontage n’est observé qu’au bout de 165 jours. Au final, un test était-il bien nécessaire pour arriver à ce constat ???????\nTest OLS avec \\(g(t)=1- S(t)\\)\n\nTest Grambsch-Therneau avec \\(g(t)=1- S(t)\\)\n\n\nVariables\nchi2\ndf\nP&gt;Chi2\n\n\n\n\nyear\n1.96\n1\n0.162\n\n\nage\n1.15\n1\n0.284\n\n\nsurgery\n3.96\n1\n0.046\n\n\n\nR-Stata-Sas-Python\n\nR\n\n\nAttention seulement version GLS du test depuis le V3 de survival.\n\nAprès avoir créer un objet à l’estimation du modèle de Cox, on utilise la fonction cox.zph. Cette fonction utilise par défaut \\(g(t)=1-S(t)\\) où \\(S(t)\\) sont les estimateurs de la courbe de Kaplan-Meier. On peut modifier cette fonction. Il est préférable de conserver cette fonction par défaut.\nTest OLS: j’ai récupéré le programme du test antérieur, renommé cox.zphold. On peut le charger simplement, et il est facilement exécutable. Pour le charger: source(\"https://raw.githubusercontent.com/mthevenin/analyse_duree/main/cox.zphold/cox.zphold.R\")\n\n\n8.2.1.1 Stata\nLe test (OLS) est obtenu avec la commande estat phtest, d. Par défaut Stata utilise \\(g(t)=t\\). On peut modifier cette fonction.\n\n\n8.2.1.2 SAS\nLe test (OLS) est disponible depuis quelques années avec l’argument zph sur la ligne proc lifetest. Par défaut SAS utilise \\(g(t)=t\\). On peut modifier cette fonction.\n\n\n8.2.1.3 Python\nLe test (OLS) est donné avec la fonction proportional_hazard_test de la librairie lifelines. La fonction utilise par défaut \\(g(t)=t\\), mais on peut afficher les résultats pour toutes les transformations de \\(t\\) disponibles avec l’option time_transform=’all’.\n\n\n\n\n\n\n8.2.2 Intéraction avec la durée\nPetit retour sur l’estimation du modèle\nPour estimer le modèle de Cox, les données sont dans un premier temps splitées aux moment où au moins un évènement a été observé.\nSur l’application, avec 2 individus avec la covariable age (rappel: il s’agit de l’âge en \\(t_0\\):\n\nBase spittées sur les intervals d’évènement\n\n\nid\nage\ndied\n\\(t_0\\)\n\\(t\\)\n\n\n\n\n2\n51\n0\n0\n1\n\n\n2\n51\n0\n1\n2\n\n\n2\n51\n0\n2\n3\n\n\n2\n51\n0\n3\n5\n\n\n2\n51\n1\n5\n6\n\n\n3\n54\n0\n0\n1\n\n\n3\n54\n0\n1\n2\n\n\n3\n54\n0\n2\n3\n\n\n3\n54\n0\n3\n5\n\n\n3\n54\n0\n5\n6\n\n\n3\n54\n0\n6\n8\n\n\n3\n54\n0\n8\n9\n\n\n3\n54\n0\n9\n12\n\n\n3\n54\n1\n12\n16\n\n\n\nLes bornes des intervalles \\([t_0;t]\\) présentent des valeurs seulement lorsqu’un évènement s’est produit (principe de la vraisemblance partielle). Il n’y a donc pas de valeurs pour \\(t\\) et \\(t_0\\) en \\(t=4\\) pour \\(id=(2,3)\\)et \\(t=7,10,11,13,14,15\\) pour \\(id=3\\).\nLes deux individus observent l’évènement en \\(t=6\\) pour \\(id=2\\), et en \\(t=16\\) pour \\(id=3\\). Avant ce moment la valeur de la variable évènement/censure (ici \\(d\\)) prend toujours la valeur 0, et prend la valeur 1 le jour du décès.\nSur cette base splitée aux moments d’évènement (n=3573), on pourra vérifier facilement que les résultats obtenus par le modèle de Cox sont identiques à ceux obtenus précédemment.\nIntroduction d’une intéraction avec une fonction de la durée\nOn a une variable de durée (on prendra \\(g(t)=t\\)) qui sera croisée avec la variable surgery.\nLe modèle s’écrit:\n\\[h(t | X,t) = h_0(t)e^{b_1age + b_2year + b_3 surgery + b_4 (surgery\\times t)}\\]\nLe modèle avec cette intéraction donne les résultats suivants:\n\nModèle de Cox avec une intéraction entre une fonction de la durée et la variable *surgery\n\n\n\n\n\n\n\n\n\n\nVariable\n\\(e^b\\)\nStd.err\nz\nP&gt;|z|\n95% IC\n\n\n\n\nyear\n0.884\n0.059\n-1.84\n0.066\n0.776 ; 1.008\n\n\nage\n1.029\n0.014\n+2.15\n0.032\n1.003 ; 1.057\n\n\n\\(surgery(t_{0+})\\)\n0.173\n0.117\n-2.60\n0.009\n0.046 ; 0.649\n\n\n\\(surgery\\times t\\)\n1.002\n0.001\n+2.02\n0.043\n1.000 ; 1.004\n\n\n\nOn retrouve donc un résultat proche de celui obtenu à partir du test OLS sur les résidus de Schoenfeld pour la variable surgery. Et c’est normal. Avec \\(g(t)=t\\), il a le mérite de pouvoir être interprété directement. Ce qui ne veut pas dire qu’il s’agit de la meilleure solution.\nDonc, malgré une hypothèse plutôt forte sur la forme fonctionnelle de l’intéraction, et dans les faits surement pas pertinente, on peut dire que chaque jour le rapport des risques entre personnes opérées et personnes non opérées augmente de +0.2%. Pour plus précis, étant à l’origine &lt;1, l’écart se modère. L’effet de l’opération sur la survie des individus s’estompe donc avec le temps.\nA noter\n\nLe modèle n’est plus un modèle à risque proportionnel. La variable surgery n’est plus une variable fixe mais une variable tronquée dynamique qui prend la valeur de \\(t\\) pour les personnes qui ont été opérées d’un pontage avant leur entrée dans le registre de greffe.\n\nSi \\(surgery = 0\\)\n\n\n\nid\nsurgery\ndied\n\\(t_0\\)\n\\(t\\)\nsurgery*t\n\n\n\n\n2\n0\n0\n0\n1\n0\n\n\n2\n0\n0\n1\n2\n0\n\n\n2\n0\n0\n2\n3\n0\n\n\n2\n0\n0\n3\n5\n0\n\n\n2\n0\n1\n5\n6\n0\n\n\n\nSi \\(surgery = 1\\) (jusqu’à \\(t=6\\) car aucun décès précoce pour ce groupe)\n\n\n\nid\nsurgery\ndied\n\\(t_0\\)\n\\(t\\)\nsurgery*t\n\n\n\n\n40\n1\n0\n0\n1\n1\n\n\n40\n1\n0\n1\n2\n2\n\n\n40\n1\n0\n2\n3\n3\n\n\n40\n1\n0\n3\n5\n5\n\n\n40\n1\n1\n5\n6\n6\n\n\n\nExemple pour une variable quantitative (age)\n\n\n\nid\nage\ndied\n\\(t_0\\)\n\\(t\\)\nage*t\n\n\n\n\n2\n51\n0\n0\n1\n51\n\n\n2\n51\n0\n1\n2\n102\n\n\n2\n51\n0\n2\n3\n153\n\n\n2\n51\n0\n3\n5\n255\n\n\n2\n51\n1\n5\n6\n306\n\n\n\n\nL’altération des rapports de risque dépend de la forme fonctionnelle de l’intéraction choisie. Ici la variation dans la durée du rapport des risque est constante, ce qui est une hypothèse assez forte. On a, en quelques sorte, réintroduit une hypothèse de proportionnalité, ici sur le degré d’altération des écarts de risques dans le temps, qui devient lui même strictement constant.\n\n\n\n8.2.3 Que faire ?\nNe rien faire\nOn interprète le risque ratio comme un ratio moyen pendant la durée d’observation (P.Allison). Difficilement soutenable pour l’analyse des effets cliniques, elle peut être envisagée dans d’autres domaines. Attention au nombre de variables qui ne respectent pas l’hypothèse, l’estimation de la baseline du risque pourrait être sensiblement affectée si l’analyse a des visée prédictives. Il convient tout de même lors de l’interprétation, de préciser les variables qui seront analysées sous cette forme très « moyenne » sur la période d’observation.\nOn peut également adapter cette stratégie du « ne rien faire » selon sens de l’altération des rapports de risque. Si aux cours du temps des écarts de risque, s’accentuent à la hausse comme à la baisse, on peut conserver cet estimateur moyen. Mais si cette non proportionnalité conduit à un changement du sens des rapport de risque je suis moins convaincu de la pertinence de cette stratégie. Encore une fois, et il faut le rappeler, l’estimation des courbes de survie doit permette d’anticiper ce dernier cas de figure.\nIl faut également tenir compte de l’intérêt portée par les variables qui présentent un problème par rapport à l’hypothèse. Il n’est peut-être pas nécessaire de complexifier le modèle pour des variables introduites comme simples contrôles.\nMais plus problématique [important]… On sait qu’une des causes du non respect de l’hypothèse peut provenir d’effets de sélection liées à des variables omises ou non observables. En analyse de durée ce problème prend le nom de frailty (fragilité) lorsque cette non homogénéité n’est pas observable. Des estimations, plus complexes, sont possibles dans ce cas, et sont en mesure malgré leur interprétation plutôt difficile de régler le problème. Il convient donc de bien spécifier le modèle au niveau des variables de contrôle observables et disponibles.\nModèle de Cox stratifié\nUtiliser la méthode dite de « Cox stratifiée » (non traitée). Utile si l’objectif est de présenter des fonctions de survie prédites ajustées, et si une seule covariable (binaire) présente un problème. Les HR ne seront pas estimés pour la variable qui ne respecte pas l’hypothèse.\nIntéraction\nIntroduire une interaction avec la durée. Cela peut permettre en plus d’enrichir le modèle au niveau de l’interprétation. Valable si peu de covariables présentent des problèmes de stabilité des rapports de risque, dans l’idéal une seule variable. Attention tout de même à la forme de la fonction, dans l’exemple on a contraint l’effet d’interaction à être strictement linéaire, ce qui est une hypothèse plutôt forte…. on introduit de nouveau une contrainte de proportionnalité dans le modèle.\nModèles alternatifs\nUtiliser un modèle alternatif: modèles paramétriques à risques proportionnels si la distribution du risque s’ajuste bien, le modèle paramétrique « flexible » de Parmar-Royston ou un modèle à temps discret. Pour la dernière solution, on peut également corriger la non proportionnalité avec l’introduction d’une intéraction. Si on ne le fait pas, les risques prédits, par définition des probabilités conditionnelles, resteront toujours dans les bornes contrairement au modèle de Cox.\nUtiliser un modèle non paramétrique additif dit d’Aalen ou une de ses variantes (non traité). Mais ces modèles, dont les résultats sont présentés par des graphiques, se commentent assez difficilement.\nForêt aléatoire\nAutre méthode : les forêts aléatoires. L.Breiman a dès le départ proposé une estimation des modèles de survie par cette méthode. Par définition, pas sensible à l’hypothèse PH. Mais cela reste des méthodes à finalité prédictive, moins riche en interprétation."
  },
  {
    "objectID": "07-cox.html#footnotes",
    "href": "07-cox.html#footnotes",
    "title": "8  Le modèle de Cox",
    "section": "",
    "text": "Se reporter à la définition des grandeurs dans la section Théorie↩︎\n\\(n! = (n)\\times(n-1)\\times(n-2)\\times....\\times3\\times2\\times1\\)↩︎\nIl s’agit bien de la personne qui maintient le package survival dans R↩︎\nPour les personnes utilisant R, je donne un moyen pour récupérer et exécuter le test OLS sous R↩︎\nSe reporter à leur ouvrage Modeling Survival Data: Extending the Cox Model (2001)↩︎"
  },
  {
    "objectID": "02-donnees.html#données-prospectives-et-rétrospectives",
    "href": "02-donnees.html#données-prospectives-et-rétrospectives",
    "title": "3  Les Données",
    "section": "3.1 Données prospectives et rétrospectives",
    "text": "3.1 Données prospectives et rétrospectives\n\n3.1.1 Les données prospectives\n\nIndividus suivis à des dates successives. On parle souvent de données de stock mises à jour à intervalle de temps plus ou moins réguliers.\nInstrument de mesure identique à chaque vague (si possible).\nAvantages: qualité des données et techniquement l’absence de biais de mémoire1.\nInconvénients:\nDélais pour les exploiter dans une analyse,\nMêmes hypothèses entre deux passages pas forcément respectées\nAttrition, censure ou troncature à gauche liés aux âges d’inclusion.\n\nA noter l’exploitation croissante des données administratives qui peuvent regorger d’informations biographiques. Déjà disponibles, le problème du coût de collecte peut-être est contourné2. Ce type de données comprend par exemple les informations issues des fichiers des Ressources Humaines des entreprises, qui ont été exploitées à l’Ined dans le cadre du projet « worklife » (https://worklife.site.ined.fr/). Une des sources de plus en plus utilisée en France est maintenant l’EDP3. Elles engendrent en revanche des questionnements techniques liés à l’inférence (on ne travaille directement pas sur des échantillons), et à une présence potentiellement massive de problèmes de censures à gauche ou par intervalles, ou de troncature à gauche4.\nUn exemple de mauvaise pratique avec ce type de données sera développé plus loin dans la section relative aux censures et aux troncatures.\n\n\n3.1.2 Les données rétrospectives\n\nIndividus interrogés une seule fois.\nRecueil de biographies thématiques depuis une origine jusqu’au moment de l’enquête.\nRecueil d’informations complémentaires à la date de l’enquête (âge, sexe, csp au moment de l’enquête et/ou csp représentative).\nAvantages: Information longitudinale immédiatement disponible.\nInconvénients: questionnaire long, informations datées qui font appel à la mémoire de l’enquêté.e. A de rares exceptions (enfant, mariage), il est difficile d’aller chercher des datations trop fines avec une retrospectivité assez longue.\n\nLes deux types de recueil peuvent être mixés avec des enquêtes à passages répétés comprenant des informations retrospectives entre 2 vagues. Par exemple la cohorte Elfe de l’Ined-Inserm ou la Millenium-Cohort-Study en Grande Bretagne5."
  },
  {
    "objectID": "02-donnees.html#grille-ageven",
    "href": "02-donnees.html#grille-ageven",
    "title": "3  Les Données",
    "section": "3.2 Grille AGEVEN",
    "text": "3.2 Grille AGEVEN\nPour recueillir des informations biographiques retrospectives, on utilise généralement la méthode des grilles AGEVEN.\nIl s’agit d’une grille âge-évènement, de type chronologique, avec des repères temporels en ligne (âge, année). En colonne, sont complétés de manière progressive et relative, les évènements relatifs à des domaines, par exemple la biographie professionnelle, familiale, résidentielle…\n\n\n\n\n\n\nRéférences\n\n\n\n\nAntoine P., X. Bry and P.D. Diouf, 1987 “La fiche Ageven : un outil pour la collecte des données rétrospectives”, Statistiques Canada 13(2).\nVivier G, “Comment collecter des biographies ? De la fiche Ageven aux grilles biographiques, Principes de collecte et Innovations récentes”, Acte des colloques de l’AIDELF, 2006.\nGRAB, 1999, “Biographies d’enquêtes : bilan de 14 collectes biographiques”, Paris, INED.\n\nExemple grille Ageven page 121: &lt;http://retro.erudit.org/livre/aidelf/2006/001404co.pdf&lt;"
  },
  {
    "objectID": "02-donnees.html#enregistrement-des-données",
    "href": "02-donnees.html#enregistrement-des-données",
    "title": "3  Les Données",
    "section": "3.3 Enregistrement des données",
    "text": "3.3 Enregistrement des données\nLa question du format des fichiers biographiques mis à disposition n’est pas neutre, en particulier au niveau des manipulations pour créer le fichier d’analyse, opération qui pourra s’avérer particulièrement chronophage et complexe si plusieurs modules doivent être appariés. On distingue trois formats d’enregistrement.\n\n3.3.1 Large [format individu]\nUne ligne par individu, qui renseigne sur une même ligne tous les évènements liés à un domaine : les datations et les caractéristiques des évènements.\nExemple: domaine : unions - échelle temporelle: année - fin de l’observation en 1986:\n\n\n\nid\ndebut1\nfin1\ncause1\ndébut2\nfin2\ncause2\n\n\n\n\nA\n1979\n1982\ndécès conjoint\n1985\n.\n.\n\n\nB\n1983\n1984\nSéparation\n.\n.\n.\n\n\n\nInconvénients: peut générer beaucoup de vecteurs colonnes avec de nombreuses valeurs manquantes. Le nombre de colonnes va dépendre du nombre maximum d’évènements. Si ce nombre concerne un seul individu, on va multiplier le nombre de colonnes pour un niveau d’information très limité. Situation classique, le nombre d’enfants, où les naissances de rang élevé deviennent de plus en plus rares.\n\n\n3.3.2 Semi-long [format individu-évènements]\nC’est le format le plus courant de mise à disposition des enquêtes biographiques. Si les transitions sont de type continu, par exemple le lieu de résidence (on habite toujours quelque part), la date de fin de la séquence correspond à la date de début de la séquence suivante. Les dates de fin ne sont pas forcément renseignées sur une ligne pour des trajectoires continues, l’information peut être donnée sur la ligne suivante avec la date de début.\nPour la séquence qui se déroule au moment de l’enquête, la date de fin est souvent une valeur manquante, une fin de séquence pouvant se produire juste avant l’enquête au cours d’une même année. Il est également possible d’avoir une information qui ne s’est pas encore produite au moment de l’enquête, mais qui aura lieu peu de temps après (personne enceinte, donc une naissance probable la même année).\nExemple précédent (trajectoires discontinues):\n\n\n\nid\ndebut\nfin\ncause\nNumero séquence\n\n\n\n\nA\n1979\n1982\ndécès conjoint\n1\n\n\nA\n1985\n.\n.\n2\n\n\nB\n1983\n1984\nSéparation\n1\n\n\n\n\n\n3.3.3 Long [format individu-périodes]\nTypique des recueils prospectifs. Ils engendrent des lignes sans information supplémentaire par rapport à la ligne précédente.\nExemple précédent:\n\n\n\nid\nAnnée\ncause\nNumero séquence\n\n\n\n\nA\n1979\n.\n1\n\n\nA\n1980\n.\n1\n\n\nA\n1981\n.\n1\n\n\nA\n1982\nDécès conjoint\n1\n\n\nA\n1985\n.\n2\n\n\nA\n1986\n.\n2\n\n\nB\n1983\n.\n1\n\n\nB\n1984\nSéparation\n1\n\n\n\nIci les trajectoires ne sont pas continues. Une forme continue présenterait toute la trajectoire, avec l’ajout d’un statut du type être en couple ou non. Pour ID=A, en 1983 et 1984, deux lignes « pas couple » (cohabitant ou non) pourraient être insérées avec au total 3 séquences.\n Remarque : pour certaines analyses (par exemple analyse en temps discret), on doit transformer passer d’un format large ou semi-long à un format long, sur les durées observées ou sur des intervalles de durées construits."
  },
  {
    "objectID": "02-donnees.html#exemples-de-mise-à-disposition",
    "href": "02-donnees.html#exemples-de-mise-à-disposition",
    "title": "3  Les Données",
    "section": "3.4 Exemples de mise à disposition",
    "text": "3.4 Exemples de mise à disposition\nDeux enquêtes biographiques de type rétrospectives produite par l’Ined, avec un fichier qui fournit des informations générales sur les individus (une ligne par individu), et une série de modules biographiques en format individus-évènements.\n\n3.4.1 Enquête biographie et entourage (Ined)\nhttps://grab.site.ined.fr/fr/enquetes/france/biographie_entourage/\n\n\n\nBiographie et entourage: base caractéristiques individuelle\n\n\n\n\n\nBiographie et entourage: base biographique logements\n\n\n\n\n3.4.2 Enquête MAFE (Ined)\n\n\n\nMAFE: base caractéristiques individuelles\n\n\n\n\n\nMAFE: base biographique logement"
  },
  {
    "objectID": "02-donnees.html#footnotes",
    "href": "02-donnees.html#footnotes",
    "title": "3  Les Données",
    "section": "",
    "text": "Cet avantage peut se trouver contrebalancé par des phénomènes de censure par intervalles, donc de trous tout aussi problématique que ceux liés à la mémoire↩︎\nJe ne suis par forcément à l’aise avec cet argument souvent avancé. La maintenance et l’alimentation de ce type de données est également coûteuse, elle ne se fait pas par magie. Malheureusement cet argument est sûrement une des raisons qui explique les problèmes de financement des enquêtes retrospective↩︎\nEchantillon Démographique Permanent. Un bon exemple de données administrative dont le coût de production est loin d’être négligeable↩︎\nSe reporter par exemple à la présentation du très rigoureux guide de l’utilisateur de l’EDP: Les informations disponibles : des informations à géométrie variable et à trous↩︎\nPour avoir exploité ces données, cette cohorte souffre contrairement à Elfe de changement assez récurrent d’un passage à un autre (parait-il lié à ses changements de direction, chacune souhaitant laisser sa “patte”). En revanche elle a le mérite de s’être investi sur la récupération de l’attrition au cours du temps. Cela rend les données difficile à exploiter dans un cadre longitudinal, mais cela à le mérite de donner une certaine stabilisation de l’échantillon de départ↩︎"
  }
]