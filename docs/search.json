[
  {
    "objectID": "annexes/residusch.html",
    "href": "annexes/residusch.html",
    "title": "Tests GT simplifié sur les résidus de Schoenfeld",
    "section": "",
    "text": "Important\n\n\n\nAttention il ne s’agit pas du test actuellement implémenté dans la nouvelle version de survival (v3) qui, malheureusement, lui a substitué la version dite “exacte”. Le programme de la fonction du test simplifié est néanmoins facilement récupérable et exécutable. lien.\nJe continue de préconiser l’utilisation de cette version non “exact” du test, reproductible avec les autres applications statistiques (Stata,Sas,Python).\n\n\n\nLe test dit “simplifié”, qui n’apparait pas dans le texte original de P.Gramsch et T.Thernau lien, répond à un soucis d’instabilité des variances des résidus de Schoenfeld en fin de durée d’observation lorsque peu d’observation restent soumises au risque. Cet argument est soulevé dans leur ouvrage de 2022 lien avant d’en présenter sa version.\nIl est simplifié car on applique à tous les résidus bruts la variance du paramètre (\\(b\\)) estimés par le modèle de Cox.\nLe test devient alors un simple test de corrélation entre les résidus et une fonction de la durée (centrée). Dans l’esprit, il peut être également approché par une regression linéaire par les moindre carrés ordinaires entre les résidus et une fonction de la durée (voir page 134 de l’ouvrage de Grambsch et Therneau).\n\nSoit les données suivantes, avec t la variable de durées, Y la variable de censure et X la seule et unique covariable.\n\nPas d’évènement simultané (donc pas de correction de la vraisemblance)\nCovariable de type indicatrice\n\n\n\n\n\\(t_i\\)\n\\(Y_i\\)\n\\(X_i\\)\n\n\n\n\n1\n1\n1\n\n\n2\n0\n0\n\n\n3\n0\n0\n\n\n4\n1\n1\n\n\n5\n1\n1\n\n\n6\n1\n0\n\n\n7\n0\n1\n\n\n\n\ntest = data.frame(time=  c(1,2,3,4,5,6,7),\n                    Y=c(1,0,0,1,1,1,0),\n                    X=     c(1,0,0,1,1,0,1))\n\nEstimation du modèle de Cox:\n\nlibrary(survival)\nfit = coxph(formula = Surv(time, Y) ~ X, data=test)\nfit\n\nCall:\ncoxph(formula = Surv(time, Y) ~ X, data = test)\n\n    coef exp(coef) se(coef)    z     p\nX 0.6217    1.8622   1.1723 0.53 0.596\n\nLikelihood ratio test=0.31  on 1 df, p=0.5797\nn= 7, number of events= 4 \n\n\nCalcul des résidus brut (si et seulement si \\(Y=1\\)) dans le cas d’une seule covariable avec \\(b\\) égal à 0.62:\n\\[rs_{i}=X_{i}- \\sum_{j\\in R_i}X_{i}\\frac{e^{0.62\\times X}}{\\sum_{j\\in R_i}e^{0.62\\times X}}= X_{i} - E(X_{j\\in R_i})\\] Il y a ici 4 résidus à calculer, pour \\(t=(1,4,5,6)\\)\nRésidus pour \\(t=1\\)\n\n\\(a_1= \\sum_{j\\in R_i}e^{0.62\\times X} = e^{0.62} + 1 + 1 + e^{0.62} + 1 + e^{0.62}= 10.43\\)\n\\(b_1= \\sum_{j\\in R_i}X_{i}\\frac{e^{0.62\\times X}}{\\sum_{j\\in R_i}e^{0.62\\times X}} = 4\\times\\frac{e^{0.62}}{10.43} = 0.71\\)\n\\(r_1 = 1 - 0.71 = 0.29\\)\n\nRésidus pour \\(t=4\\)\n\n\\(a_4 = e^{0.62} + e^{0.62} + 1 + e^{0.62} = 6.58\\)\n\\(b_4 = 4\\times\\frac{e^{0.62}}{6.58} = 0.84\\)\n\\(r_4 = 1 - 0.84 = 0.15\\)\n\nRésidus pour \\(t=5\\)\n\n\\(a_5 = e^{0.62} + e^{0.62} + 1 = 4.71\\)\n\\(b_5 = 2\\times\\frac{e^{0.62}}{4.71} = 0.78\\)\n\\(r_5 = 1 - 0.78 = 0.21\\)\n\nRésidus pour \\(t=6\\)\n\n\\(a_6 = e^{0.62} + 1 = 2.86\\)\n\\(b_6 = \\frac{e^{0.62}}{2.86} = 0.65\\)\n\\(r_6 = 0 - 0.65 = -0.65\\)\n\nLes résidus “standardisés”, ou plutôt scaled residuals (je cale sur une traduction correcte en français) sont égaux à:\n\\[sr_i = b + nd \\times Var(b) \\times r_i\\] Avec \\(nd= \\sum Y_i\\)\n\n\\(\\sum Y_i = 4\\)\n\\(Var(b) = (1.1723)^2=1.37\\)\n\\(sr_1 = 0.62 + 4\\times 1.37 \\times 0.29 = 2.20\\)\n\\(sr_4 = 0.62 + 4\\times 1.37 \\times 0.15 = 1.47\\)\n\\(sr_5 = 0.62 + 4\\times 1.37 \\times 0.21 = 1.78\\)\n\\(sr_6 = 0.62 + 4\\times 1.37 \\times (-0.65) = -2.95\\)\n\nAvec \\(g(t_i)\\) une fonction de la durée (\\(g(t_i)=t_i\\), \\(g(t_i)=1-KM(t_i)\\)…) et \\(\\overline{g(t)}\\) sa valeur moyenne, la statistique du test score simplifié pour une covariable est égale à :\n\\[\\frac{[\\sum_i(g(t_i) - \\overline{g(t_i)}\\times sr_i)]^2}{nd \\times Var(b) \\times (\\sum_i(g(t_i) - \\overline{g(t_i)})^2}\\] Et suis un \\(\\chi^2\\) à 1 degré de liberté.\nAvec \\(\\overline{g(t_i)}=t_i\\), le calcul de la statistique de test est:\n\n\\(\\overline{g(t_i)}= \\frac{28}{7}=4\\)\n\\(\\frac{[(1-4)\\times 2.20] + [(4-4)\\times 1.47 + (5-4)\\times 1.78 + (6-4)\\times (-2.95)]^2 }{4\\times 1.37 \\times [(1-4)^2 + (4-4)^2 + (5-4)^2 + (6-4)^2] } = \\frac{114.9}{76.72} = 1.49\\)\n\n\n#source(\"D:/D/Marc/SMS/FORMATIONS/analyse_duree/cox.zphold/cox.zphold.R\")\n\nsource(\"https://raw.githubusercontent.com/mthevenin/analyse_duree/master/cox.zphold/cox.zphold.R\")\n\n\ncox.zphold(fit, transform=\"identity\")\n\n     rho chisq     p\nX -0.688  1.49 0.222"
  },
  {
    "objectID": "annexes.html",
    "href": "annexes.html",
    "title": "Annexes",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nNov 9, 2022\n\n\nTests GT simplifié sur les résidus de Schoenfeld\n\n\nMarc Thevenin\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "comp_fsurvie.html",
    "href": "comp_fsurvie.html",
    "title": "Tests de comparaison",
    "section": "",
    "text": "Les tests d’égalités des fonctions de survie entre différentes valeurs d’une covariable sont calculés à partir de la méthode de Kaplan Meier.\n L’utilisation du test correspond à la nécessité de déterminer si une même distribution gouverne les évènements observés dans les différentes strates.\nAttention: pas de test possible sur des variables continues. Il faut donc prévoir des regroupements pour les transformer en variable ordinale.\nDeux méthodes sont utilisées:"
  },
  {
    "objectID": "comp_fsurvie.html#r-stata-sas-python",
    "href": "comp_fsurvie.html#r-stata-sas-python",
    "title": "Tests de comparaison",
    "section": "R-Stata-Sas-Python",
    "text": "R-Stata-Sas-Python\n\nRSasPython\n\n\nOn utilise la fonction survdiff de la librairie survival. Le résultat du test de Peto-Peto est affiché par défaut (rho=1). Si on souhaite utiliser le test non pondéré, on ajoute l’option rho=0. Pour obtenir le résultat d’un test multiple corrigé (plus d’un degré de liberté), on peut utiliser la fonction pairwise_survdiff de la librairie survminer. Cette fonction permet d’obtenir des tests 2 à 2 si une variable a plus de deux groupes.\nJe conseille de rester sur l’option Peto-Peto et dans le cas d’une variable à plus de deux modalités, d’utiliser la fonction de survminer\n\nStata\nOn utilise la commande sts test avec le nom de la version du test: peto, wilcoxon . Sans préciser le nom de la variante, le test non pondéré est exécuté.\n\n\n\nLe test non pondéré et la version Wilcoxon sont données avec l’option strata de la proc lifetest. Attention : ne jamais utiliser la version LR Test qui est biaisée. Pour obtenir d’autres versions du test du log-rank, on ajoute /test=all à l’option strata.\n\n\nAvec la librairie lifelines, on utilise la fonction logrank_test. Quatre variantes sont disponibles (Wilcoxon, Tarone-Ware, Peto-Peto et Fleming-Harrigton). On peut également utiliser la fonction duration.survdiff de statmodels (non pondéré, Wilcoxon - appelé ici Breslow- et Tarone-Ware)."
  },
  {
    "objectID": "comp_fsurvie.html#application",
    "href": "comp_fsurvie.html#application",
    "title": "Tests de comparaison",
    "section": "Application",
    "text": "Application\nOn compare ici l’effet du pontage sur le risque de décéder depuis l’inscription dans le registre de greffe.\n\n\nLog-rank test for equality of survivor functions\n\n        |   Events         Events\nsurgery |  observed       expected\n--------+-------------------------\n0       |        69          60.34\n1       |         6          14.66\n--------+-------------------------\nTotal   |        75          75.00\n\n              chi2(1) =       6.59\n              Pr>chi2 =     0.0103\n\n\nWilcoxon (Breslow) test for equality of survivor functions\n\n        |   Events         Events        Sum of\nsurgery |  observed       expected        ranks\n--------+--------------------------------------\n0       |        69          60.34          623\n1       |         6          14.66         -623\n--------+--------------------------------------\nTotal   |        75          75.00            0\n\n              chi2(1) =       8.99\n              Pr>chi2 =     0.0027\n\n\nTarone-Ware test for equality of survivor functions\n\n        |   Events         Events        Sum of\nsurgery |  observed       expected        ranks\n--------+--------------------------------------\n0       |        69          60.34    73.111827\n1       |         6          14.66   -73.111827\n--------+--------------------------------------\nTotal   |        75          75.00            0\n\n              chi2(1) =       8.46\n              Pr>chi2 =     0.0036\n\n\nPeto-Peto test for equality of survivor functions\n\n        |   Events         Events        Sum of\nsurgery |  observed       expected        ranks\n--------+--------------------------------------\n0       |        69          60.34    6.0529913\n1       |         6          14.66   -6.0529913\n--------+--------------------------------------\nTotal   |        75          75.00            0\n\n              chi2(1) =       8.66\n              Pr>chi2 =     0.0033\n\nLes résultats font apparaître que l’opération permet d’allonger la durée de survie des personnes."
  },
  {
    "objectID": "comp_fsurvie.html#r-stata-sas-python-1",
    "href": "comp_fsurvie.html#r-stata-sas-python-1",
    "title": "Tests de comparaison",
    "section": "R-Stata-Sas-Python",
    "text": "R-Stata-Sas-Python\nAttention, selon les logiciels la durée max par défaut n’est pas la même. Pour R et Sas, il s’agit du moment où le dernier évènement à été observé sur l’ensemble de l’échantillon, alors que pour Stata si on compare le moment du dernier évènement observé entre les deux groupes, c’est le plus court qui fait office de durée maximale. Cela affectera légèrement la valeur des Rmst estimées par défaut.\nPour l’exemple, la durée maximale utilisée par R/Sas est de 1407 jours alors que pour Stata elle est de 995 jours.\n\nRStataSASPython\n\n\nLibrairie SurvRm2. Programmée par les mêmes personnes que la commande Stata, la fonction proposée n’est pas très souple.\n\n\nCommande externe strmst2. La plus ancienne fonction proposée par les logiciels. Au final plus limitée que la solution Sas. J’ai programmé une commande, diffrmst, qui représente graphiquement les estimations des Rmst pour chaque temps d’évènement, leurs différences et les p-value issues des comparaisons.\n\n\nDisponible depuis la version 15.1 de SAS/Stat (fin 2018). Les estimations et le résultat du test de comparaison sont récupérables très simplement dans une proc lifetest, avec en option **plots=(rmst)** . Bien que sortie tardivement par rapport Stata et R, les résultats sont particulièrement complets.\n\n\nEstimation un peu pénible. A partir de l’estimateur KM obtenu avec la fonction KaplanMeierFitter de lifelines, on peut obtenir les RMST avec la fonction restricted_mean_survival_time. On peut tracer les fonctions, en revanche le test de comparaison n’est pas implémenté."
  },
  {
    "objectID": "comp_fsurvie.html#application-1",
    "href": "comp_fsurvie.html#application-1",
    "title": "Tests de comparaison",
    "section": "Application",
    "text": "Application\n::: caption-warning Les résultats qui suivent ont été obtenu avec Stata. La valeur par défaut de \\(t_max\\) est légèrement différente, mais les conclusions sont identiques.\n\nRestricted Mean Survival Time (RMST) by arm\n-----------------------------------------------------------\n   Group |  Estimate    Std. Err.      [95% Conf. Interval]\n---------+-------------------------------------------------\n   arm 1 |   734.758     133.478      473.145      996.370\n   arm 0 |   310.169      43.158      225.581      394.757\n-----------------------------------------------------------\n\nBetween-group contrast (arm 1 versus arm 0) \n------------------------------------------------------------------------\n           Contrast  |  Estimate       [95% Conf. Interval]     P>|z|\n---------------------+--------------------------------------------------\nRMST (arm 1 - arm 0) |   424.589      149.641      699.537      0.002\nRMST (arm 1 / arm 0) |     2.369        1.513        3.710      0.000\n------------------------------------------------------------------------\n\nIci \\(t^*\\) est égal à 995 jours, soit la durée qui correspond au dernier décès observé lorsqu’une personne a été opérée pour un pontage (surgery=1).\nSur un horizon de 995 jours, ces individus peuvent espérer vivre 735 jours en moyenne, contre 310 jours pour les autres. La durée moyenne de survie est donc deux fois plus importante pour les personnes opérées d’un pontage (rapport des Rmst = 2.3 ), soit une différence de 424 jours.\nRmst et différences de Rmst à tous les points d’évènement jusqu’à \\(t_max\\)\n\n  +--------------------------------------------------------------------------+\n  | _time     _rmst1     _rmst0      _diff          _l         _u         _p |\n  |--------------------------------------------------------------------------|\n  |     1          1          1          0           0          0          . |\n  |     2          2   1.989011    .010989     .010989    .010989          . |   \n  |     3          3   2.945055   .0549451   -.0196757   .1295658   .1489731 |\n  |     5          5   4.791209   .2087912    .0256584   .3919241   .0254456 |\n  |   5.1        5.1   4.882418   .2175824    .0240373   .4111275   .0275679 |\n  |--------------------------------------------------------------------------|\n  |     6          6   5.693407   .3065934    .0643487   .5488381   .0131162 |\n  |     8          8   7.451648   .5483516    .1860869   .9106163   .0030096 |\n  |     9          9    8.31978   .6802198    .2523926   1.108047   .0018318 |\n  |    11         11   10.03407    .965934    .4072903   1.524578   .0007017 |\n  |    12         12   10.89121   1.108791    .4836155   1.733967   .0005087 |\n  |--------------------------------------------------------------------------|\n  |    16         16   14.27525   1.724747    .8259398   2.623554   .0001692 |\n  |    17         17   15.08787   1.912131    .9277063   2.896555   .0001407 |\n  |    18         18   15.88935   2.110646     1.05301   3.168283   .0000918 |\n  |    21         21   18.26041   2.739589    1.458002   4.021176   .0000279 |\n  |    28         28   23.63703   4.362966    2.526501   6.199431   3.22e-06 |\n  |--------------------------------------------------------------------------|\n  |    30         30   25.15095   4.849051    2.842812    6.85529   2.17e-06 |\n  |    31         31   25.89677   5.103226    3.014868   7.191583   1.67e-06 |\n  |    32         32    26.6426     5.3574    3.186886   7.527915   1.31e-06 |\n  |    35         35   28.84618   6.153824    3.736433   8.571216   6.06e-07 |\n  |    36         36    29.5694     6.4306    3.929635   8.931564   4.67e-07 |\n  |--------------------------------------------------------------------------|\n  |    37         37   30.28132   6.718675    4.135427   9.301923   3.44e-07 |\n  |    39         39   31.68257   7.317427    4.569757    10.0651   1.79e-07 |\n  |    40         40   32.37189   7.628103    4.797789   10.45842   1.28e-07 |\n  |    43         43   34.37207   8.627934    5.552385   11.70349   3.83e-08 |\n  |    45         45   35.68291    9.31709     6.07508    12.5591   1.77e-08 |\n  |--------------------------------------------------------------------------|\n  |    50         50   38.90352   11.09648    7.431942   14.76102   2.94e-09 |\n  |    51         51   39.53634   11.46366    7.711818    15.2155   2.12e-09 |\n  |    53         53   40.77938   12.22061    8.298259   16.14297   1.02e-09 |\n  |    58         58   43.83049   14.16951     9.81571   18.52331   1.79e-10 |\n  |    61         61   45.62725   15.37275    10.75502   19.99047   6.81e-11 |\n  |--------------------------------------------------------------------------|\n  |    66         66   48.56535   17.43465    12.37503   22.49426   1.44e-11 |\n  |    68         68   49.71799   18.28201    13.04371    23.5203   7.90e-12 |\n  |    69         69   50.27171   18.72829    13.40325   24.05333   5.45e-12 |\n  |    72         72   51.89897   20.10103    14.51838   25.68369   1.70e-12 |\n  |    77         77   54.49805   22.50194    16.49285   28.51104   2.14e-13 |\n  |--------------------------------------------------------------------------|\n  |    78         78   55.00657   22.99343    16.89797   29.08889   1.43e-13 |\n  |    80         80   56.00101   23.99899    17.73478   30.26321   5.97e-14 |\n  |    81         81   56.48692   24.51307    18.16526   30.86089   3.77e-14 |\n  |    85         85   58.38539   26.61461    19.93458   33.29464   5.77e-15 |\n  |    90         90   60.70197   29.29803     22.1984   36.39766   6.66e-16 |\n  |--------------------------------------------------------------------------|\n  |    96         96   63.41406   32.58594    24.97681   40.19506          0 |\n  |   100        100   65.17693   34.82308    26.87198   42.77418          0 |\n  |   102        102   66.03575   35.96425    27.84368   44.08482          0 |\n  |   109        109   68.96255   40.03745    31.32724   48.74766          0 |\n  |   110        110   69.38067   40.61933    31.82339   49.41528          0 |\n  |--------------------------------------------------------------------------|\n  |   131        131   77.91717   53.08283    42.46146    63.7042          0 |\n  |   149        149   85.23417   63.76583    51.49939   76.03227          0 |\n  |   153        153   86.81235   66.18765    53.54893   78.82638          0 |\n  |   165        165    91.4034    73.5966    59.87845   87.31474          0 |\n  |   180   178.6364   97.14223   81.49413    51.34782   111.6404   1.17e-07 |\n  |--------------------------------------------------------------------------|\n  |   186   184.0909   99.43776   84.65315    53.34505   115.9613   1.16e-07 |\n  |   188   185.7273   100.2029   85.52434    53.56977   117.4789   1.56e-07 |\n  |   207   201.2727   107.2376    94.0351    58.18815   129.8821   2.73e-07 |\n  |   219   211.0909   111.5325   99.55843    61.16676   137.9501   3.72e-07 |\n  |   263   247.0909   126.7373   120.3536    72.25138   168.4559   9.40e-07 |\n  |--------------------------------------------------------------------------|\n  |   265   248.7273   127.4037   121.3235    72.75741   169.8897   9.77e-07 |\n  |   285   265.0909   134.0682   131.0227    77.89536   184.1501   1.34e-06 |\n  |   308   283.9091   141.1427   142.7664    84.36629   201.1664   1.66e-06 |\n  |   334   305.1818   148.8068    156.375    91.96277   220.7872   1.95e-06 |\n  |   340   310.0909   150.4986   159.5923    93.78695   225.3977   2.00e-06 |\n  |--------------------------------------------------------------------------|\n  |   342   311.7273   151.0369   160.6904    94.42397   226.9568   2.01e-06 |\n  |   370   332.0909   158.5728   173.5181    93.67896   253.3572   .0000205 |\n  |   397   351.7273   165.8396   185.8876    98.91358   272.8617    .000028 |\n  |   427   373.5454   173.9138   199.6316    104.6545   294.6087   .0000379 |\n  |   445   386.6364   178.7584    207.878    108.0686   307.6874   .0000446 |\n  |--------------------------------------------------------------------------|\n  |   482   413.5454   188.7166   224.8289    115.0297   334.6281   .0000599 |\n  |   515   437.5454   197.5982   239.9472    121.1866   358.7078    .000075 |\n  |   545   459.3636   205.6725   253.6912    126.7507   380.6316   .0000897 |\n  |   583        487   215.8998   271.1002    133.7623    408.438   .0001093 |\n  |   596   494.8788   219.3987   275.4801    134.5264   416.4339   .0001279 |\n  |--------------------------------------------------------------------------|\n  |   620   509.4243    225.858   283.5662    136.4692   430.6632   .0001579 |\n  |   670   539.7273   239.3151   300.4122    140.0713   460.7531   .0002405 |\n  |   675   542.7576   240.6608   302.0968    140.4026   463.7909   .0002504 |\n  |   733   577.9091   254.9701    322.939    145.3689    500.509   .0003645 |\n  |   841   643.3636   279.1928   364.1708    155.9437   572.3979   .0006085 |\n  |--------------------------------------------------------------------------|\n  |   852   650.0303   281.6599   368.3704    156.9483   579.7925    .000638 |\n  |   915   688.2121   294.2198   393.9923    164.2457   623.7389   .0007762 |\n  |   941   703.9697   299.4033   404.5664    167.1596   641.9732   .0008378 |\n  |   979        727   306.9791   420.0209    171.3309   668.7109   .0009321 |\n  |   995   734.7576   310.1689   424.5887    149.6407   699.5366   .0024726 |\n  +--------------------------------------------------------------------------+\n\nLe graphique suivant, donne les valeurs des Rmst et les écarts de la variable surgery en faisant varier \\(t_max\\) sur chaque durée où un décès a été observé. Il a été réalisé avec Stata, la durée maximale utilisée est de 995 jours"
  },
  {
    "objectID": "concurrent.html",
    "href": "concurrent.html",
    "title": "Risques concurrents",
    "section": "",
    "text": "Le problème des événements multiples dans les analyses de survie a été posée dans les années 1970 avec la notion de “risques concurrents” (competing risks) : il s’agit d’événements survenant au cours de la période d’observations et qui “empêchent” l’occurence de l’événement d’intérêt."
  },
  {
    "objectID": "concurrent.html#estimation-non-paramétrique",
    "href": "concurrent.html#estimation-non-paramétrique",
    "title": "Risques concurrents",
    "section": "Estimation non paramétrique",
    "text": "Estimation non paramétrique\n\nUtiliser l’estimateur de Nelson Aalen: il s’agit du risque instantané cumulé. Comme il ne s’agit pas d’une probabilité, il a été longtemps utilisé comme mesure de l’incidence en présence de risques concurrents dans une logique dite “cause spécifique”:\n\n\\[H_k (t_i)=\\sum_{t_i\\leq t}\\left(\\frac{e_{i,k}}{n_i}\\right) \\]\n\nDe nos jours, l’estimateur le plus utilisé est la fonction dite d’incidence cumulée - CIF- (Kalbfleisch-Prentice, Marubini-Valscchi):\n\nIl repose sur une probabilité tout en supportant la non indépendance des risques.\nSon interprétation est identique à la fonction de répartition \\(F(t)=1-S(t)\\). Cette fonction est donc croissante.\nIl est possible de tester les différences entres CIF: test de Gray (R, SAS) ou test de Pepe-Mori (Stata).\n\n\nLa fonction d’incidence cumulée (CIF)\n\n\nSi \\(h_k(t_i)\\) est le risque “cause-spécific en \\(t_i\\) et \\(S(t_i-1)\\) l’estimateur de Kaplan-Meier en \\(t_i-1\\) lorsque tous les risques sont regroupés en un évènement unique, l’incidence cumulée pour le risque \\(k\\) en \\(t_i\\) est égale à:\n\n\\[IC_k(t_i)= \\sum_{t_i\\leq t}S(t_i-1)h_k(t_i)\\]\n\nLes valeurs prises par cette fonction pour la cause \\(k\\) ne dépendent donc pas seulement des individus ayant observé l’évènement à partir de cette seule cause, mais aussi du nombre de personnes qui n’ont pas encore observés l’évènement à partir des autres causes identifiées. Cette dernière information est donnée par \\(S(t_i-1)\\).\nL’incidence cumulée peut ainsi s’interpréter comme la proportion d’individus qui sont sortis du risque jusqu’en \\(t_i\\) en raison de la cause \\(k\\).\n\n\n\n failure:  compet == 1\n competing failures:  compet == 2\n\n    Time       CIF         SE     [95% Conf. Int.]\n--------------------------------------------------\n       1    0.0097     0.0097     0.0009    0.0477\n       2    0.0194     0.0136     0.0038    0.0619\n       3    0.0485     0.0212     0.0181    0.1022\n       5    0.0680     0.0248     0.0300    0.1273\n       6    0.0874     0.0278     0.0429    0.1515\n       8    0.0971     0.0292     0.0497    0.1634\n       9    0.1068     0.0304     0.0566    0.1751\n      12    0.1166     0.0316     0.0638    0.1868\n      16    0.1264     0.0328     0.0711    0.1984\n      18    0.1362     0.0338     0.0785    0.2099\n      21    0.1559     0.0358     0.0937    0.2325\n      30    0.1657     0.0367     0.1014    0.2437\n      32    0.1756     0.0376     0.1093    0.2550\n      35    0.1856     0.0384     0.1173    0.2662\n      37    0.1955     0.0392     0.1253    0.2773\n      39    0.2055     0.0400     0.1335    0.2884\n      40    0.2156     0.0407     0.1418    0.2996\n      45    0.2256     0.0414     0.1502    0.3107\n      50    0.2357     0.0421     0.1586    0.3217\n      53    0.2458     0.0427     0.1671    0.3327\n      58    0.2559     0.0433     0.1757    0.3436\n      61    0.2660     0.0439     0.1843    0.3544\n      66    0.2761     0.0445     0.1930    0.3652\n      68    0.2861     0.0450     0.2018    0.3759\n      69    0.2962     0.0454     0.2106    0.3866\n      72    0.3063     0.0459     0.2195    0.3973\n      77    0.3164     0.0463     0.2284    0.4079\n      78    0.3265     0.0467     0.2374    0.4184\n      81    0.3365     0.0471     0.2464    0.4289\n      85    0.3466     0.0474     0.2554    0.4393\n      90    0.3567     0.0478     0.2645    0.4497\n      96    0.3668     0.0481     0.2737    0.4601\n     100    0.3769     0.0484     0.2829    0.4704\n     102    0.3870     0.0486     0.2921    0.4807\n     110    0.3972     0.0489     0.3016    0.4911\n     149    0.4078     0.0491     0.3112    0.5019\n     153    0.4183     0.0494     0.3209    0.5125\n     186    0.4291     0.0496     0.3309    0.5235\n     188    0.4399     0.0498     0.3409    0.5343\n     207    0.4506     0.0500     0.3509    0.5451\n     263    0.4614     0.0502     0.3610    0.5559\n     285    0.4836     0.0505     0.3818    0.5780\n     308    0.4947     0.0506     0.3923    0.5890\n     340    0.5058     0.0507     0.4028    0.5999\n     583    0.5211     0.0513     0.4162    0.6158\n     733    0.5391     0.0524     0.4313    0.6351\n     852    0.5584     0.0535     0.4475    0.6555\n     995    0.5811     0.0550     0.4657    0.6801\n    1032    0.6039     0.0561     0.4850    0.7036\n    1386    0.6343     0.0584     0.5084    0.7362\n\n            failure:  compet == 2\n competing failures:  compet == 1\n\n    Time       CIF         SE     [95% Conf. Int.]\n--------------------------------------------------\n       2    0.0194     0.0136     0.0038    0.0619\n      16    0.0391     0.0191     0.0128    0.0897\n      17    0.0489     0.0213     0.0182    0.1029\n      28    0.0587     0.0232     0.0240    0.1157\n      36    0.0686     0.0250     0.0302    0.1286\n      40    0.0787     0.0267     0.0368    0.1413\n      43    0.0888     0.0283     0.0436    0.1539\n      51    0.0989     0.0297     0.0506    0.1663\n      68    0.1090     0.0310     0.0578    0.1785\n      72    0.1190     0.0323     0.0651    0.1905\n      80    0.1291     0.0334     0.0726    0.2024\n     165    0.1396     0.0346     0.0804    0.2149\n     219    0.1504     0.0358     0.0886    0.2276\n     334    0.1615     0.0370     0.0970    0.2406\n     342    0.1730     0.0383     0.1058    0.2540\n     675    0.1910     0.0414     0.1177    0.2777\n     979    0.2138     0.0457     0.1321    0.3086\n\nEn présence du risque concurrent, et traité comme tel, 50% des personnes sont décédées directement de la malformation cardiaque au bout de 308 jours (200 jours avec une estimation de type « cause specific »).\n\nOn peut vérifier que la somme des estimateurs permet d’obtenir la survie toutes causes confondues. Il n’y a pas de surprise à cela, dans l’estimateur Marubini-Valscchi la survie d’ensemble intervient comme un facteur de pondération de la mesure d’intensité dite « cause-specific »."
  },
  {
    "objectID": "concurrent.html#r-stata-sas-python",
    "href": "concurrent.html#r-stata-sas-python",
    "title": "Risques concurrents",
    "section": "R-Stata-Sas-Python",
    "text": "R-Stata-Sas-Python\n\n\n\n\n\n\nL’estimation avec des risques de type « cause-specific » demande juste de recoder la variable évènement/censure, en glissant les risques concurrents en censure à droite.\nPour l’estimation des CIF (risque de sous répartition):\n\nR: la librairie cmprsk permet d’estimer simplement les incidences cumulées avec la fonction cuminc.\nSas: maintenant directement estimable avec proc lifetest. Il suffit d’indiquer le ou les risques d’intérêt dans l’instruction indiquant la variable de durée et de censure avec l’option failcode=valeur.\nStata: Estimation avec la commande externe stcompet. La commande génère des variables qui demande des manipulations supplémentaires pour afficher les résultats sous forme de tableau par exemple. On peut utiliser et préférer la commande externe stcomlist.\nPython: le wrapper de R (cmprsk) ne fonctionne plus à ce jour à défaut de mise à jour."
  },
  {
    "objectID": "concurrent.html#compararaison-des-cif",
    "href": "concurrent.html#compararaison-des-cif",
    "title": "Risques concurrents",
    "section": "Compararaison des CIF",
    "text": "Compararaison des CIF\n\nTest d’homogénéité de Gray: est basé sur une autre mesure du risque en évènement concurrent. Il s’agit du « subdistribution risks (« risque de sous-répartition », A.Latouche). Son interprétation n’est pas aisée car les personnes ayant observé un risque concurrent sont remises dans le Risk Set. Mais il est directement lié à l’estimation des CIF. Disponible avec SAS et R. Il est également sensible l’hypothèse de proportionnalité et à la distribution des censures à droites entre les groupes comparés. A ma connaissance il n’y a pas de variantes pondérées.\nTest de Pepe & Mori: teste directement deux courbes d’incidences et seulement 2. Il y a une version du test qui repose sur une autre fonction directement tirée des incidences, la CPF (Conditional Probability Function), qui n’est pas traite pas afin de ne pas rajouter un concept de supplémentaire.\n\n\nTest de Gray pour la variable surgery\n\nTests:\n         stat      pv    df\ncause1 5.7834605 0.0161  1\ncause2 0.1293076 0.7191  1\n\n\nTest de Pepe-Mori pour la variable surgery\n\nMain event failure:  compet == 1\nChi2(1) = 6.2028  -  p =  0.01275\n\nCompeting event failure:  compet == 2\nChi2(1) = 1.8796  -  p =  0.17038"
  },
  {
    "objectID": "concurrent.html#r-stata-sas-python-1",
    "href": "concurrent.html#r-stata-sas-python-1",
    "title": "Risques concurrents",
    "section": "R-Stata-Sas-Python",
    "text": "R-Stata-Sas-Python\n\n\n\n\n\n\n\nSas: le test de Gray est estimé si on ajoute l’option strata=nom_variable à la proc lifetest sous risque concurrent (voir encadré précédent). Le test de Pepe-Mori est disponible via une macro externe (%compcif: non testée) :\nStata: Le test de Gray n’est pas disponible, il faut passer par une exécution de la fonction cuminc de la librairie R cmprsk directement dans stata (voir la commande rsource). Pour faire plus simple, on peut estimer le modèle de Fine-Gray avec une seule variable (discrète). Le résultat est comparable à celui du test (voir plus bas). Le test de Pepe-Mori est disponible via la commande externe stpepemori.\nR: On ajoute une variable à la fonction cuminc de la librairie cmprsk. Pas de test de Pepe-Mori sur les fonctions d’incidence à ma connaissance.\nPython: ne pas essayer d’utiliser la librairie cmprsk qui n’est pas mis à jour et ne fonctionne plus."
  },
  {
    "objectID": "concurrent.html#modèles-semi-paramétriques",
    "href": "concurrent.html#modèles-semi-paramétriques",
    "title": "Risques concurrents",
    "section": "Modèles Semi paramétriques",
    "text": "Modèles Semi paramétriques\nCette présentation sera plutôt brève. Dans le domaine des sciences sociales, je préconise plutôt l’utilisation d’un modèle multinomial à temps discret de type logistique. Le modèle de Cox en présence de risques concurrent n’est valable que dans une logique de risques « cause-specific », le modèle de Fine et Gray bien que directement relié à l’estimation des incidences cumulées, repose sur une définition du risque (de sous répartition) dont l’interprétation n’est pas naturelle. Il est également soumis à l’hypothèse de proportionnalité des risques. \nModélisation des risques « cause-specific » : Cox\nModèle de Cox «standard» pour chaque évènement, les évènements concurrents sont traités comme des censures à droite. Aucune interprétation sur les fonctions d’incidence ne peut-être faite.\n\nModèle de Fine-Gray: subdistribution hazard regression\nModèle de type semi-paramétrique avec une redéfinition du risque lié à l’estimation des fonctions d’incidence (voir test de Gray). La différence avec le Cox classique réside dans le calcul du risk-set : les évènements concurrents ne sont pas considérés comme des censures, on laisse les individus leur « survivre » jusqu’à la durée maximale observée dans l’échantillon. L’interprétation n’est donc pas très intuitive (Fine et Gray le soulignent). Ce modèle est relativement contreversé. Il ne sera donc pas exécuté pour l’application\n\nPour les questions liées à l’interprétation de ces deux types de modèles, se reporter à: https://onlinelibrary.wiley.com/doi/epdf/10.1002/sim.7501"
  },
  {
    "objectID": "concurrent.html#r-stata-sas-python-2",
    "href": "concurrent.html#r-stata-sas-python-2",
    "title": "Risques concurrents",
    "section": "R-Stata-Sas-Python",
    "text": "R-Stata-Sas-Python\n\n\n\n\n\n\n\nR: on utilise la fonction crr du package cmprsk.\nSas: même principe que pour l’estimation non paramétrique, on ajoute l’option eventcode=valeur à l’instruction model de la proc phreg.\nStata: on utilise la commande interne stcrreg.\nPython : ne pas essayer d’utiliser la librairie cmprsk qui n’est pas mis à jour et ne fonctionne donc plus."
  },
  {
    "objectID": "concurrent.html#modèle-à-temps-discret",
    "href": "concurrent.html#modèle-à-temps-discret",
    "title": "Risques concurrents",
    "section": "Modèle à temps discret",
    "text": "Modèle à temps discret\n\nIl s’agit d’une extension du modèle à temps discret à évènement unique (toutes causes regroupées) avec ici le modèle logistique multinomial.\nS’il ne permet pas une interprétation sur les fonctions d’incidences, les risques concurrents ne sont pas traitées comme des censures à droite et sont estimés simultanément à la cause d’intérêt.\nLe modèle multinomial repose sur une hypothèse dite « d’indépendance » des alternatives non pertinentes » (IIA). Cela peut donc paraitre contradictoire d’utiliser ce modèle pour des évènements qui sont supposés non indépendants. Néanmoins la dépendance entre risques concurrents n’est pas non plus stricte. L’hypothèse d’IIA est souvent illustrée par l’exemple des couleurs des bus dans le choix du mode de transport (G.Debreu). On est loin ici d’un tel niveau de dépendance.\nEn terme de lecture, le modèle logistique multinomial les estimateurs peuvent directement s’interpréter comme des rapports de risque (ou relative risk ratio).\nEn sciences sociales, il me semble que ce type de modèle soit à privilégier.\nOn peut également envisager un modèle de type probit multinomial, mais on peut rencontrer des problèmes d’estimations (repose sur la loi normale multivariée). Prévoir un regroupement des causes concurrentes, et dans tous les cas de figure ne pas dépasser trois causes. Niveau lecture, il conviendra d’utiliser une méthode de standardisation, de type « effets marginaux ».\n\nPour l’exemple, j’ai utilisé comme plus haut pour les modèles à temps discret à évènement unique, le mois comme métrique temporelle.\n\nMultinomial logistic regression                 Number of obs     =      1,127\n                                                LR chi2(10)       =      86.25\n                                                Prob > chi2       =     0.0000\nLog likelihood = -275.00542                     Pseudo R2         =     0.1356\n\n------------------------------------------------------------------------------\n           e |        RRR   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n0            |  (base outcome)\n-------------+----------------------------------------------------------------\n1            |\n           t |     0.8159     0.0338    -4.91   0.000       0.7522      0.8850\n          t2 |     1.0032     0.0009     3.53   0.000       1.0014      1.0049\n         age |     1.0449     0.0183     2.51   0.012       1.0097      1.0813\n        year |     0.8795     0.0718    -1.57   0.116       0.7494      1.0321\n     surgery |     0.3175     0.1711    -2.13   0.033       0.1104      0.9129\n-------------+----------------------------------------------------------------\n2            |\n           t |     0.8168     0.0565    -2.93   0.003       0.7134      0.9353\n          t2 |     1.0030     0.0015     1.94   0.052       1.0000      1.0060\n         age |     1.0111     0.0248     0.45   0.654       0.9635      1.0610\n        year |     0.8158     0.1127    -1.47   0.141       0.6223      1.0695\n     surgery |     0.5412     0.4221    -0.79   0.431       0.1173      2.4959\n------------------------------------------------------------------------------\nRemarque : les constantes ne sont pas reportées, \nles valeurs de la référence n’ayant pas grand  sens (année et âge à 0)\n\n[ajouter présentation sous forme d’AME / seulement possible avec Stata]"
  },
  {
    "objectID": "cox.html",
    "href": "cox.html",
    "title": "Modèle de Cox",
    "section": "",
    "text": "On peut ignorer la partie sur l’estimation du modèle. On retiendra tout de même qu’il est déconseillé de tester la méthode dite exacte pour la correction de la vraisemblance, qui ne peut matériellement fonctionner qu’avec un nombre très limité d’évènements observés simultanément, ce qui est plutôt rare avec des données à durées discrètes ou groupées, classiques dans les sciences sociales.\n\n\nOn se situe dans une situation où la durée est mesurée sur une échelle strictement continue. Il ne peut donc y avoir qu’un seul évènement observé en \\(t_i\\) (idem pour les censures).\nPour une observation quelconque en \\(t_i\\) qui, pour uniquement un seul individu correspond à un évènement ou une censure, la vraisemblance peut s’écrire:\n\\[L_i=f(t_i)^{\\delta_i}S(t_i)^{1-\\delta_i}\\]\n\nVraisemblance partielle de Cox\n\n\\(f(t_i)\\) est la valeur de la fonction de densité en \\(t_i\\)\n\\(S(t_i)\\) est la valeur de la fonction de survie en \\(t_i\\)\n\\(\\delta_i=1\\) si l’évènement est observé: \\(L_i=f(t_i)\\)\n\\(\\delta_i=0\\) si l’observation est censurée: \\(L_i=S(t_i)\\)\n\n Comme \\(f(t_i)=h(t_i)\\times S(t_i)\\), on obtient: \\(L_i=[h(t_i)S(t_i)]^{\\delta_i}S(t_i)^{1-\\delta_i} = h(t_i)^{\\delta_i}S(t_i)\\).\nPour \\(i=1,2,.....,n\\), la vraisemblance totale s’ecrit donc: \\(L_i=\\prod_{i=1}^{n}h(t_i)^{\\delta_i}S(t_i)\\).\n On peut réécrire cette vraisemblance en la multipliant et en la divisant par: \\(\\sum_{j\\in R_i}h(t_i)\\), où \\(j\\in R_i\\) est l’ensemble des observation soumises au risque en \\(t_i\\).\n\n\\[L=\\prod_{i=1}^{n}\\left[h(t_i)\\frac{\\sum_{j\\in R}h(t_i)}{\\sum_{j\\in R}h(t_i)}\\right]^{\\delta_i}S(t_i)= \\prod_{i=1}^{n}\\left[\\frac{h(t_i)}{\\sum_{j\\in R_i}h(t_i)}\\right]^{\\delta_i}\\sum_{j\\in R_i}h(t_i)^{\\delta_i}S(t_i)\\]\n La vraisemblance partielle retient seulement le premier terme de la vraisemblance, soit:\n\\[PL=\\prod_{i=1}^{n}\\left[\\frac{h(t_i)}{\\sum_{j\\in R}h(t_i)}\\right]^{\\delta_i}\\]\nUne fois remplacée la valeur de \\(h(t_i)\\) par son expression en tant que modèle à risques proportionnels, la vraisemblance partielle ne dépendra plus de la durée. Mais elle va dépendre de l’ordre d’arrivée des évènements, c’est à dire leur rang.  Remarque: pour les observations censurées(\\(\\delta_i=0\\)), \\(PL=1\\). Toutefois, ces censures à droite entrent dans l’expression \\(\\sum_{j\\in R}h(t_i)\\) tant qu’elles sont soumises au risque.\n En remplaçant donc \\(h(t_i)\\) par l’expression \\(h_0(t)e^{X_i^{'}b}\\):\n\\[PL=\\prod_{i=1}^{n}\\left[\\frac{h_0(t)e^{X_{i}^{'}b}}{\\sum_{j\\in R_i}h_0(t)e^{X_{j}^{'}b}}\\right]^{\\delta_i} =\\prod_{i=1}^{n}\\left[\\frac{e^{X_i^{'}b}}{\\sum_{j\\in R_i}e^{X_{j}^{'}b}}\\right]^{\\delta_i}\\]\nL’expression \\(\\frac{e^{Xb}}{\\sum_{j\\in R}e^{Xb}}\\) est une probabilité, la vraisemblance partielle est donc bien un produit de probabilités. Il s’agit de la probabilité qu’un individu observe l’évènement en \\(t_i\\) sachant qu’un évènement (et un seul) s’est produit.\n\nCondition nécessaire: pas d’évènement simultané:\nOn rappelle que le temps est mesuré de manière strictement continue, il ne doit pas y avoir d’évènement simultané. Sinon, l’estimation de la vraisemblance doit être corrigée. \nCorrection de la vraisemblance avec des évènements simultanés:\n\nLa méthode dite exacte: Comme en réalité il n’y a pas d’évènement simultané, on va intégrer à la vraisemblance toutes les permutations possibles des évènements observés simultanément: si en \\(t_i\\) on observe au « même moment » l’évènement pour A et B, une échelle temporelle plus précise nous permettrait de savoir si A s’est produit avant B ou B s’est produit avant A. Comme le nombre de permutations est calculé par une factorielle, avec 3 évènements mesurés simultanément, il y a 6 permutations possibles (\\(3\\times2\\times1\\)).\nProblème: le nombre de permutations pour chaque \\(t_i\\) peut devenir très vite particulièrement élevé. Par exemple pour 10 évènements simultanés, le nombre de permutations est égal à 3.628.800. Le temps de calcul devient particulièrement long, et ce type de correction totalement inopérant.\nLa méthode dite de Breslow: il s’agit d’une approximation de la méthode exacte permettant de ne pas avoir à intégrer chaque permutation. Cette approximation est utilisée par défaut par les logiciels Sas et Stata.\nLa méthode dite d’Efron: elle corrige l’approximation de Breslow, et est jugée plus proche de la méthode exacte. C’est la méthode utilisée par défaut avec le logiciel R, et elle est disponible avec les autres applications.\n\n\n\n\nOn utilise la méthode habituelle, à savoir la maximisation de la log-vraisemblance (ici partielle).\n\nConditions de premier ordre: calcul des équations de score à partir des dérivées partielles. Solution: \\(\\frac{\\partial log(PL)}{\\partial{b_k}}=0\\). On ne peut pas obtenir de solution numérique directe.\nRemarque: les équations de score sont utilisées pour tester la validité de l’hypothèse de constance des rapports de risque pour calculer les résidus de Schoenfeld (voir plus bas).\nConditions de second ordre: calcul des dérivées secondes qui permettent d’obtenir la matrice d’information de Fisher et la matrice des variances-covariances des paramètres.\nComme il n’y a pas de solution numérique directe, on utilise un algorithme d’optimisation (ex: Newton-Raphson) à partir des équations de score et de la matrice d’information de Fisher.\n\nEléments de calcul\n En logarithme (sans évènement simultané), la vraisemblance partielle s’ecrit:\n\\[pl(b)=\\sum_{i=1}^n\\delta_i\\left(log(e^{X_{i}^{'}b})-log\\sum_{j\\in R_i}e^{X_{j}^{'}b}\\right)\\]\n\\[pl(b)=\\sum_{i=1}^n\\delta_i\\left(X_{i}^{'}b-log\\sum_{j\\in R_i}e^{X_{j}^{'}b}\\right)\\]\nCalcul de l’équation de score pour une covariable \\(X_k\\):\n\\[\\frac{\\partial pl(b)}{\\partial{b_k}}=\\sum_{i=1}^n\\delta_i\\left(X_{ik}-\\sum_{j\\in R_i}X_{jk}\\frac{e^{X_{j}^{'}b}}{\\sum_{j\\in R_i}e^{X_{j}^{'}b}}\\right)\\]\nComme \\(\\frac{e^{X_{j}b}}{\\sum_{j\\in R}e^{X_{j}b}}\\) est une probabilité, et \\(\\sum_{j\\in R}X_{ik}\\times p_i\\) est l’espérance (la moyenne) \\(E(X_k)\\) d’avoir la caractéristique \\(X_k\\) lorsqu’un évènement a été observé. Au final:\n\\[\\frac{\\partial lp(b)}{\\partial{b_k}}= \\sum_{i=1}^n\\delta_i\\left(X_{ik} - E(X_{j\\in R_i,k}) \\right)\\]\nCette expression va permettre de tester le respect ou non de l’hypothèse de risques proportionnels.\n\n\n\nComme il s’agit d’un modèle à risque proportionnel, les rapports de risques sont constants pendant toute la période d’observation. Il s’agit d’une propriété de l’estimation.\n\nCovariable binaire (indicatrice)\n\n\\(X=(0,1)\\): \\(RR=\\frac{h(t\\ |\\ X=1)}{h(t\\ |\\ X=0)}=e^b\\).\nA chaque moment de la durée \\(t\\), le risque d’observer l’évènement est \\(e^b\\) fois plus important/plus faible pour \\(X=1\\) que pour \\(X=0\\).\n\n\nCovariable quantitative (mais fixe dans le temps)\n\n\\(RR=\\frac{h(t\\ |\\ X=a+c)}{h(t\\ |\\ X=a)}=e^{c \\times{b}}\\). On prendra pour illustrer une variable type âge au début de l’exposition au risque (a)* et un delta de comparaison avec un âge inférieur (c).\nSi \\(c=1\\) (résultat de l’estimation): A un âge donnée en début d’exposition, le risque de connaitre l’évènement est \\(e^b\\) fois inférieur/supérieur à celui d’une personne qui a un an de moins .\n\n\nExemple pour les insuffisances cardiaques\n Estimateurs: \\(b\\)\n\nCox regression -- Efron method for ties\n\nNo. of subjects =          103                  Number of obs    =         103\nNo. of failures =           75\nTime at risk    =        31938\n                                                LR chi2(3)       =       17.63\nLog likelihood  =   -289.30639                  Prob > chi2      =      0.0005\n\n------------------------------------------------------------------------------\n          _t |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        year |    -0.1196     0.0673    -1.78   0.076      -0.2516      0.0124\n         age |     0.0296     0.0135     2.19   0.029       0.0031      0.0561\n     surgery |    -0.9873     0.4363    -2.26   0.024      -1.8424     -0.1323\n------------------------------------------------------------------------------\n\nRapports des risques: \\(e^b\\)\n\nCox regression -- Efron method for ties\n\nNo. of subjects =          103                  Number of obs    =         103\nNo. of failures =           75\nTime at risk    =        31938\n                                                LR chi2(3)       =       17.63\nLog likelihood  =   -289.30639                  Prob > chi2      =      0.0005\n\n------------------------------------------------------------------------------\n          _t | Haz. Ratio   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        year |     0.8872     0.0597    -1.78   0.076       0.7775      1.0124\n         age |     1.0300     0.0139     2.19   0.029       1.0031      1.0577\n     surgery |     0.3726     0.1625    -2.26   0.024       0.1584      0.8761\n------------------------------------------------------------------------------\n\nOn retrouve les résultats des tests non paramétriques pour l’opération, à savoir qu’un pontage réduit les risques journaliers de décès pendant la période d’observation (augmente la durée de survie).\nDe la même manière, plus on entre à un âge élevé dans la liste d’attente plus le risque de décès augmente. La variable year, qui traduit des progrès en médecine, exprime une réduction relativement modérée du risque journalier de décès durant l’attente de la greffe du coeur.\n\n\n\n\nRStataSASPython\n\n\nLe modèle est estimé avec la fonction coxph de la librairie survival. Hors options, la syntaxe est identiques aux fonctions survfit et survdif.\n\n\nLe modèle est estimé avec la commande stcox.\n\n\nLe modèle est estimé avec la proc phreg.\n\n\nAvec la librairie lifelines, le modèle est estimé avec la fonction CoxPHFitter. Avec la librairie statmodels, il est estimé avec la fonction smf.phreg."
  },
  {
    "objectID": "cox.html#tests-sur-les-résidus-de-schoenfeld",
    "href": "cox.html#tests-sur-les-résidus-de-schoenfeld",
    "title": "Modèle de Cox",
    "section": "Tests sur les résidus de Schoenfeld",
    "text": "Tests sur les résidus de Schoenfeld\n\n\n\n\n\n\nImportant\n\n\n\nRésumé rapide: le test consiste à regarder la corrélation entre des résidus obtenus directement avec la fonction de score de la vraisemblanc partielle de Cox.\n\nTest simplifié: simple corrélation obtenu par une régression linéaire par les moindres carrés ordinaires. Pour chaque covariable, on utilise la variance de son estimateur obtenu par le modèle.\nTest exact: corrélation obtenu par une régression linéaire par les mondres carrés généralisés. Pour chaque covariable, on calcule la variance du résidu après avoir calculé la matrice d’information de fisher (seulement les termes sur la diagonale). Les moindres carrés généralisés corrigeront la présence d’autocorrélations des résidus, possibles lorsqu’on utilise des données temporelles.\n\n\n\n\nLes résidus « bruts » sont directement calculés à partir des équations de scores (voir section estimation).\nCe résidu n’est calculé que pour les observations qui ont connues l’évènement.\nIl est calculé au moment où l’évènement s’est produit.\nLa somme des résidus pour chaque covariable est égale à 0 (propriété de l’équation de score à l’équilibre).\nOn utilise généralement les résidus de Schoenfeld « standardisés » ou plutôt “remis à l’échelle” - par leur variance - pour tenir compte du fait que la population soumise au risque diminue au cours du temps.\nPour une observation dont l’évènement s’est produit en \\(t_i\\), le résidu brut de Schoenfeld pour la covariable \\(X_k\\), après estimation du modèle, est égal à:\n\n\\[rs_{ik}=X_{ik}- \\sum_{j\\in R_i}X_{jk}\\frac{e^{X_{j}^{'}b}}{\\sum_{j\\in R_i}e^{X_{j}^{'}b}}= X_{ik} - E(X_{j\\in R_i})\\]\n\nCe résidu est formellement la contribution d’une observation au score. Il se lit comme la différence entre la valeur observée d’une covariable et sa valeur espérée au moment où un évènement se produit.\nSi l’hypothèse de constance des risk ratios est respectée, les résidus ne doivent pas suivre une tendance précise, , en particulier à la hausse ou à la baisse.\nIntuitivement sans censure à droite et en ne considérant que les résidus bruts: on a un RR strictement égal à 1 en début d’exposition \\(R_i=100\\) avec 50 hommes et 50 femmes. Si l’hypothèse PH (strictement) respectée, lorsqu’il reste 90 personnes soumises au risque, on devrait avoir 45 hommes et 45 femmes. Avec \\(R_i=50\\), 25 hommes et 25 femmes,…….avec \\(R_i=10\\), 5 hommes et 5 femmes. Au final \\(X_k\\) est toujours égal à 0.5 et les résidus bruts prendront toujours la valeur -.5 si \\(X=0\\) et .5 si \\(X=1\\). En faisant une simple régression linéaire entre les résidus, qui alternent ces deux valeurs, et \\(t\\), le coefficient estimé sera non significativement différent de 0.\nOn peut tester l’hypothèse sur les résidus par une régression entre ces résidus pour chaque covariable et la durée (ou fonction dérivée de la durée, par exemple \\(t\\) ou \\(log(t)\\))). La solution la plus utilisée est le test de Grambsch-Therneau sous sa forme simplifiée (R jusqu’à v3 de survival, Sas, Stata, Python) ou exacte (R depuis la V3).\n\nDes éléments de calculs du test “simplifié” sont donnés dans la section “annexes”\n\n\n\n\n\n\nRésultats identiques entre SAS - STATA - SURVIVAL V2 (R) - Lifelines/statmodels (Python)\n\n\n\nAvec \\(g(t)=t\\):\n\n      Test of proportional-hazards assumption\n\n      Time:  Time\n      ----------------------------------------------------------------\n                  |       rho            chi2       df       Prob>chi2\n      ------------+---------------------------------------------------\n      year        |      0.10162         0.80        1         0.3720\n      age         |      0.12937         1.61        1         0.2043\n      surgery     |      0.29664         5.54        1         0.0186\n      ------------+---------------------------------------------------\n      global test |                      8.76        3         0.0327\n      ----------------------------------------------------------------\n\n\n\nIci l’hypothèse de proportionalité des risques est questionnable pour la variable surgery. Le risque ratio pourrait ne pas constant dans le temps. Ce n’est pas étonnant, sur les données le premier décès pour les personnes opérées d’un pontage n’est observé qu’au bout de 165 jours.\nRemarques / à savoir\n\nTest multiple: de nouveau il convient de se méfier du résultat du test multiple. Le risque de premier espèce peut-être assez faible alors que les tests pour chaque covariables prises une à une présentent des valeurs élevées (>.1 par exemple).\nLe résultat de ce test est considéré par certain.e.s comme un indicateur de l’ampleur du biais qui affecte la baseline du risque.\nLe résultat de ce test multiple est considéré par certain.e.s comme un indicateur de l’ampleur du biais qui affecte la baseline du risque.\nTransformations de la durée: n’importe quelle fonction de la durée peut être utilisée pour réaliser le test. On retient généralement les fonctions suivantes: \\(g(t)=t\\) (« identity »), \\(g(t)=log(t)\\), \\(g(t)=KM(t)\\) ou \\(g(t)=1- KM(t\\)) où \\(KM(t\\)) est l’estimateur de Kaplan-Meier. Enfin une transformation appelée « rank », est utilisée seulement pour les durées strictement continue ou suffisamment dispersées . Par exemple \\(t=(0.1,0.5,1,2.6,3)\\) donne une transformation \\(t=(1,2,3,4)\\). A savoir : la fonction « identity » rend le test relativement sensible aux évènements très tardifs et rares (outliers).\n\n\nRStataSASPython\n\n\nAttention version exacte du test depuis le v3 de survival.\n\nAprès avoir créer un objet à l’estimation du modèle de Cox, on utilise la fonction cox.zph. Cette fonction utilise par défaut \\(g(t)=1-KM(t)\\) où \\(KM(t)\\) sont les estimateurs de la courbe de Kaplan-Meier. On peut modifier cette fonction. Il est préférable de conserver cette fonction par défaut.\nTest antérieur: j’ai récupéré le programme du test antérieur, renommé cox.zphold. On peut le télécharger, et il est facilement exécutable.\n\n\n\nLe test est obtenu avec la commande estat phtest, d. Par défaut Stata utilise \\(g(t)=t\\). On peut modifier cette fonction.\n\n\nLe test est disponible depuis quelques années avec l’argument zph sur la ligne proc lifetest. Par défaut SAS utilise \\(g(t)=t\\). On peut modifier cette fonction.\n\n\nOn utilise la fonction proportional_hazard_test de la librairie lifelines. La fonction utilise par défaut \\(g(t)=t\\), mais on peut afficher les résultats pour toutes les transformations de \\(t\\) disponibles avec l’option time_transform=’all’."
  },
  {
    "objectID": "cox.html#intéraction-avec-la-durée",
    "href": "cox.html#intéraction-avec-la-durée",
    "title": "Modèle de Cox",
    "section": "Intéraction avec la durée",
    "text": "Intéraction avec la durée\n\nPetit retour sur l’estimation du modèle.\nPour estimer le modèle de Cox, les données sont dans un premier temps splitées aux temps d’évènement. A l’exception de Sas, les autres logiciels disposent d’une fonction qui effectue cette opération (R: survsplit - Stata: stsplit).\n\n      +------------------------------+\n      | id   surgery    d    t    t0 |\n      |------------------------------|\n  24. |  2         0    0    1     0 |\n  25. |  2         0    0    2     1 |\n  26. |  2         0    0    3     2 |\n  27. |  2         0    0    5     3 |\n  28. |  2         0    1    6     5 |\n      |------------------------------|\n  29. |  3         0    0    1     0 |\n  30. |  3         0    0    2     1 |\n  31. |  3         0    0    3     2 |\n  32. |  3         0    0    5     3 |\n  33. |  3         0    0    6     5 |\n      |------------------------------|\n  34. |  3         0    0    8     6 |\n  35. |  3         0    0    9     8 |\n  36. |  3         0    0   12     9 |\n  37. |  3         0    1   16    12 |\n      +------------------------------+\n\nLes bornes des intervalles \\([t_0 ;t]\\) ont des valeurs seulement lorsqu’un évènement s’est produit (principe de la vraisemblance partielle). Il n’y a donc pas de valeurs pour \\(t\\) et \\(t_0\\) en \\(t=4\\) (\\(id=2,3\\)), \\(t=7,10,11,13,14,15\\) (\\(id=3\\)).\nLes deux individus observent l’évènement en \\(t=6\\) pour \\(id=2\\), et en \\(t=16\\) pour \\(id=3\\). Avant ce moment la valeur de la variable évènement/censure (ici \\(d\\)) prend toujours la valeur 0, et prend la valeur 1 le jour du décès.\nOn vérifie que les paramètres estimés sont identiques\n\nCox regression with Efron method for ties\n\nNo. of subjects =    103                                Number of obs =  3,573\nNo. of failures =     75\nTime at risk    = 31,938\n                                                        LR chi2(3)    =  17.63\nLog likelihood = -289.30639                             Prob > chi2   = 0.0005\n\n------------------------------------------------------------------------------\n          _t | Haz. ratio   Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\n        year |      0.887      0.060    -1.78   0.076        0.778       1.012\n         age |      1.030      0.014     2.19   0.029        1.003       1.058\n     surgery |      0.373      0.163    -2.26   0.024        0.158       0.876\n------------------------------------------------------------------------------\n\nIntroduction d’une intéraction avec une fonction de la durée\nOn a une variable de durée (on prendra \\(g(t)=t\\)) qui sera croisée avec la variable surgery. Le modèle va s’écrire:\n\\[h(t | X,t) = h_0(t)e^{b_1age + b_2year + b_3 surgery + b_4 (surgery\\times t)}\\]\nExemple\n\nCox regression with Efron method for ties\n\nNo. of subjects =    103                                Number of obs =    103\nNo. of failures =     75\nTime at risk    = 31,938\n                                                        LR chi2(4)    =  21.58\nLog likelihood = -287.32903                             Prob > chi2   = 0.0002\n\n------------------------------------------------------------------------------\n          _t | Haz. ratio   Std. err.      z    P>|z|     [95% conf. interval]\n-------------+----------------------------------------------------------------\nmain         |\n        year |      0.884      0.059    -1.84   0.066        0.776       1.008\n         age |      1.029      0.014     2.15   0.032        1.003       1.057\nsurgery(t0+) |      0.173      0.117    -2.60   0.009        0.046       0.649\n-------------+----------------------------------------------------------------\nRapport de HR|\n   surgery*t |      1.002      0.001     2.02   0.043        1.000       1.004\n------------------------------------------------------------------------------\n\nOn retrouve donc un résultat proche de celui obtenu à partir du test simplifié sur les résidus de Schoenfeld pour la variable surgery. Avec \\(g(t)=t\\), il a le mérite de pouvoir être interprété directement. Ce qui ne veut pas dire qu’il s’agit de la meilleure solution.\nDonc, malgré une hypothèse plutôt forte sur la forme fonctionnelle de l’intéraction, et dans les fait surement pas pertinente, on peut dire que chaque jour le HR entre personnes opérées et personnes non opérées augmente de 0.2%. L’effet de l’opération sur la survie des individus s’estompe avec le temps.\nImportant:\n\nLe modèle n’est plus un modèle à risque proportionnel. La variable surgery n’est plus une variable fixe mais une variable tronquée dynamique qui prend la valeur de \\(t\\) pour les personnes qui ont été opérées d’un pontage avant leur entrée dans le registre de greffe.\nL’altération des rapports de risque dépend de la forme fonctionnelle de l’intéraction choisie. Ici la modification dans le temps du rapport des risque est constante, ce qui est une hypothèse assez forte. On a, en quelques sorte, réintroduit une hypothèse de proportionnalité, ici sur le degré d’altération des écarts de risques dans le temps, qui devient lui même constant.\nAttention: les calculs ont été donnés à titre d’exemple. Le calculs des intervalles de confiance des rapports de risque s’avère nécessaires si l’on souhaite voir s’ils s’inversent vraiment."
  },
  {
    "objectID": "cox.html#que-faire",
    "href": "cox.html#que-faire",
    "title": "Modèle de Cox",
    "section": "Que faire",
    "text": "Que faire\nNe rien faire\nOn interprète le “hazard ratio” comme un ratio moyen sur une durée d’observation (P.Allison). Difficilement soutenable pour l’analyse d’effets cliniques, on peut l’envisager dans d’autres domaines. Attention au nombre de variables qui ne respecte pas l’hypothèse, l’estimation de la baseline du risque pourrait être sensiblement affectée. Il convient tout de même lors de l’interprétation, de préciser celles qui seront analysées sous un rapport « moyen».\nOn peut également adapter cette stratégie du « ne rien faire » selon sens de l’altération des rapports de risque. Si aux cours du temps les écarts de risque, déjà important en début d’observation s’accentuent, à la hausse comme à la baisse, on peut conserver cet estimateur moyen. Mais s’ils suivent un processus de modération: \\(RR>1\\) qui baisse ou \\(RR<1\\) qui augmente au cours du temps, je suis moins convaincu de la pertinence de cette option.\nIl faut également tenir compte de l’intérêt porté par les variables qui présentent un problème par rapport à l’hypothèse. Il n’est peut-être pas nécessaire de complexifier le modèle pour des variables introduites comme simples contrôles.\nMais plus problématique [très important]… On sait qu’une des causes du non respect de l’hypothèse peut provenir d’effets de sélection liées à des variables omises ou non observables. En analyse de durée ce problème prend le nom de frailty (fragilité) lorsque cette non homogénéité n’est pas observable. Des estimations, plus complexes, sont possibles dans ce cas, et sont en mesure malgré leur interprétation plutôt difficile de régler le problème. Si l’hypothèse est sensible aux biais d’omission, il convient donc de bien spécifier le modèle au niveau des variables de contrôle observables et disponibles.\nLe test de Grambsch-Therneau peut-être également appréhendé comme un test qui donne un élément d’information sur de possibles biais d’omission (endogénéité)dans le modèle.\n\nModèle de Cox stratifié\nUtiliser la méthode dite de « Cox stratifiée » (non traitée). Utile si l’objectif est de présenter des fonctions de survie prédites ajustées, et si une seule covariable (binaire) présente un problème. Les RR ne seront pas estimés pour la variable qui ne respecte pas l’hypothèse.\n[Je pense faire un courté entrée dessuis dans la section annexe en 2023].\nIntéraction\nIntroduire une interaction avec la durée, ce qui a été fait juste haut dessus. Cela peut permettre d’enrichir le modèle au niveau de l’interprétation. Valable si peu de covariables présentent des problèmes de stabilité des rapports de risque, dans l’idéal une seule variable. Attention tout de même à la forme de la fonction, dans l’exemple on a contraint l’effet d’interaction à être strictement linéaire, ce qui est une hypothèse plutôt forte…. on introduit de nouveau une contrainte de proportionalité dans le modèle.\nModèles alternatifs\nUtiliser un modèle alternatif: modèles paramétriques à risques proportionnels si la distribution du risque s’ajuste bien, le modèle paramétrique « flexible » de Parmar-Royston (ajout prévu pour 2023) ou un modèle à temps discret. Pour la dernière solution, on peut également corriger la non proportionalité avec l’introduction d’une intéraction. Si on ne le fait pas, les risques prédits, par définition sous forme de probabilités conditionnelles, resteront toujours dans les bornes contrairement au modèle de Cox.\nUtiliser un modèle non paramétrique additif dit d’Aalen ou une de ses variantes (non traité). Mais ces modèles, dont les résultats sont présentés par des graphiques, se commentent assez difficilement.\nForêt aléatoire\nAutre méthode : les forêts aléatoires [à présenter un jour tout de même]. L.Breiman a dès le départ proposé une estimation des modèles de survie par cette méthode. Par définition, pas sensible à l’hypothèse PH. Mais cela reste des méthodes à finalité prédictive, moins riche en interprétation."
  },
  {
    "objectID": "cox.zphold/Readme.html",
    "href": "cox.zphold/Readme.html",
    "title": "Analyse des durées",
    "section": "",
    "text": "Programme du test de la V1-V2 de survival. Visiblement le test “exact” est (très) sensible aux évènements simultanés/groupés, classiques en sciences sociales. Il me semble préférable d’exécuter ce test “simplifié” qui est par ailleurs la solution adoptée par les autres logiciels: Sas - Stata - Python (“lifelines” ou “statmodels).\nIci je n’ai rien fait de plus que renommer la fonction de la version antérieure\nMarche à suivre:\n\nTélécharger le programme cox.zphold.R\nCharger la fonction cox.zphold() avec: source(\"//...path.../cox.zphold.R\")\nExécuter un modèle de Cox avec la fonction fit=coxph(...)\nExécuter le test avec cox.zphold(fit)\nPackage survival: https://github.com/therneau/survival\n\nLien dépôt: https://github.com/therneau/survival\nDescription et licence: https://github.com/therneau/survival/blob/master/DESCRIPTION"
  },
  {
    "objectID": "discret.html",
    "href": "discret.html",
    "title": "Modèle temps discret",
    "section": "",
    "text": "On va principalement traiter du modèle logistique à temps discret.\nAvec un lien logistique, le modèle à temps discret, avec seulement des covariables fixes, peut s’écrire:\n\\[log\\left[\\frac{P(Y=1\\ |\\ t_p,X_k)}{1-P(Y=1\\ |\\ t_p,X_k)}\\right]= a_0 + \\sum_{p}a_pf(t_p)+\\sum_{k}b_kX_k\\]"
  },
  {
    "objectID": "discret.html#ajustement-avec-une-durée-en-continu",
    "href": "discret.html#ajustement-avec-une-durée-en-continu",
    "title": "Modèle temps discret",
    "section": "Ajustement avec une durée en continu",
    "text": "Ajustement avec une durée en continu\nLe modèle étant paramétrique, on doit trouver une fonction qui ajuste le mieux les données. Toutes transformations de la variable est possible: \\(f(t)=a\\times t\\), \\(f(t)=a\\times ln(t)\\)……formes quadratiques. Les ajustements sous forme de splines (cubiques) tendent à se développer ces dernières années.\nPour sélectionner cette fonction, on peut tester différents modèles sans covariable additionnelle, et sélectionner la forme qui minimise un critère d’information de type AIC ou BIC (vraisemblance pénalisée).\n Exemple:\nOn va tester les paramétrisations suivante:s une forme linéraire stricte \\(f(t)=a\\times t\\) et des effets quadratiques d’ordres 2 et 3: \\(f(t)=a_1\\times t + a_2\\times t^{2}\\) et \\(f(t)=a_1\\times t + a_2\\times t^{2} + a_3\\times t^{3}\\).\nExemple Estimation des probabilités de décéder selon différents ajustements de la durée (modèle logistique à temps discret) \nCritères AIC\n\n\n\n\\(f(t)\\)\nAIC\n\n\n\n\n\\(a\\times t\\)\n504\n\n\n\\(a_1\\times t + a_2\\times t^{2}\\)\n492\n\n\n\\(a_1\\times t + a_2\\times t^{2} + a_3\\times t^{3}\\)\n486\n\n\n\nOn peut utiliser la troisième forme à savoir \\(a_1\\times t + a_2\\times t^{2} + a_3\\times t^{3}\\).\n\nEstimation du modèle avec toutes les covariables\n\nLogistic regression                             Number of obs     =      1,127\n                                                LR chi2(6)        =      90.69\n                                                Prob > chi2       =     0.0000\nLog likelihood = -230.33671                     Pseudo R2         =     0.1645\n\n------------------------------------------------------------------------------\n           e |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n           t |  -.3720566   .0823946    -4.52   0.000    -.5335471   -.2105661\n          t2 |   .0142379    .005023     2.83   0.005     .0043929    .0240828\n          t3 |  -.0001659   .0000785    -2.11   0.035    -.0003198    -.000012\n        year |  -.1326693   .0737755    -1.80   0.072    -.2772666     .011928\n         age |   .0333413   .0146876     2.27   0.023     .0045541    .0621285\n     surgery |  -1.010918    .448598    -2.25   0.024    -1.890154   -.1316821\n------------------------------------------------------------------------------\nRemarque : la constante n’est pas reportée, les valeurs de la référence n’ayant pas grand  sens (année et âge à 0)\n\nMaintenant si on estime le modèle avec la méthode de Cox (avec des durées mesurées sur une échelle de 30 jours) :\n\nCox regression -- Efron method for ties\n\nNo. of subjects =          103                  Number of obs    =         103\nNo. of failures =           75\nTime at risk    =         1127\n                                                LR chi2(3)       =       17.97\nLog likelihood  =   -289.81242                  Prob > chi2      =      0.0004\n\n------------------------------------------------------------------------------\n          _t |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        year |  -.1304397   .0674344    -1.93   0.053    -.2626087    .0017293\n         age |   .0288141   .0134981     2.13   0.033     .0023583    .0552698\n     surgery |  -.9695805   .4361069    -2.22   0.026    -1.824334   -.1148266\n------------------------------------------------------------------------------\n\nOn remarque que les coefficients estimés sont particulièrement proches."
  },
  {
    "objectID": "discret.html#ajustement-discret",
    "href": "discret.html#ajustement-discret",
    "title": "Modèle temps discret",
    "section": "Ajustement discret",
    "text": "Ajustement discret\n\nIl s’agit d’introduire la variable de durée dans le modèle comme une variable catégorielle (factor).\nPas conseillé si on a beaucoup de points d’observation, ce qui est le cas ici.\nA l’inverse, si peu de points d’observation la paramétrisation avec une durée continue n’est pas conseillé, , avec en plus un nombre très (trop) élevé de degrés de liberté.\nLa correction de la non proportionnalité peut être plus compliquée à réaliser (non traité).\n\nOn va supposer que l’on ne dispose que de 4 intervalles d’observation. Pour l’exemple, on va créer ces points à partir des quartiles de la durée, et conserver pour chaque personne une seule observation par intervalle.\n\nt=1: Entre le début de l’exposition et 4 mois.\nt=2: Entre 5 mois et 11 mois .\nt=3: Entre 12 mois et 23 mois.\nt=4: 24 mois et plus.\n\nOn va estimer le risque globalement sur l’intervalle. La base sera plus courte que la précédente (197 observations pour 103 individus).\n\n         4 |\n quantiles |           e\n      of t |         0          1 |     Total\n-----------+----------------------+----------\n         1 |        50         53 |       103 \n         2 |        35         11 |        46 \n         3 |        27          5 |        32 \n         4 |        10          6 |        16 \n-----------+----------------------+----------\n     Total |       122         75 |       197 \n\n\nLogistic regression                             Number of obs     =        197\n                                                LR chi2(6)        =      39.30\n                                                Prob > chi2       =     0.0000\nLog likelihood = -111.23965                     Pseudo R2         =     0.1501\n\n------------------------------------------------------------------------------\n           e |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         ct4 |\n          2  |  -1.033368   .4188719    -2.47   0.014    -1.854342   -.2123944\n          3  |  -1.615245    .544858    -2.96   0.003    -2.683147   -.5473433\n          4  |  -.4789305   .5992969    -0.80   0.424    -1.653531    .6956698\n             |\n        year |  -.2032436   .0931956    -2.18   0.029    -.3859036   -.0205835\n         age |   .0468518   .0184958     2.53   0.011     .0106006     .083103\n     surgery |  -1.110163   .5025594    -2.21   0.027    -2.095161   -.1251644\n------------------------------------------------------------------------------\nRemarque : la constante n’est pas reportée, les valeurs de la référence\n           n’ayant pas grand  sens (année et âge à 0)\n\nAu niveau de l’interpretation, avec 37% d’évènements sur l’ensemble des observations, il n’est plus possible d’interpréter le modèle en terme de risque (probabilité). La lecture en termes d’Odds Ratio s’impose.\nProbabilités estimées à partir d’un modèle avec la durée seulement.\nRisques sur la longueur de l’intervalle.\n\n\n\nDurées\np\n\n\n\n\n0 à 4 mois\n0.51\n\n\n4 à 11 mois\n0.24\n\n\n11 à 23 mois\n0.16\n\n\n23 à 61 mois\n0.37"
  },
  {
    "objectID": "Donnees.html",
    "href": "Donnees.html",
    "title": "Les Données",
    "section": "",
    "text": "On distingue deux types de données : les données prospectives et rétrospectives:"
  },
  {
    "objectID": "Donnees.html#les-données-prospectives",
    "href": "Donnees.html#les-données-prospectives",
    "title": "Les Données",
    "section": "Les données prospectives",
    "text": "Les données prospectives\n\nIndividus suivis à des dates successives.\nInstrument de mesure identique à chaque vague (si possible).\nAvantages: qualité des données (moins de biais de mémoire).\n\nInconvénients: coût important, délais pour les exploiter dans une analyse, mêmes hypothèses entre deux passages pas forcément respectées, problèmes d’attrition, problèmes liés aux âges d’inclusion.\n\nA noter l’exploitation croissante des données administratives qui peuvent regorger d’informations biographiques. Déjà disponibles, le problème du coût de collecte est contourné. Ce type de données comprend par exemple les informations issues des fichiers des Ressources Humaines des entreprises, qui sont par exemples actuellement exploitées à l’Ined, par exemple dans le cadre du projet « worklife » (https://worklife.site.ined.fr/). Elles engendrent en revanche des questionnements techniques liésà l’inférence ((on ne travaille directement pas sur des échantillons)."
  },
  {
    "objectID": "Donnees.html#les-données-rétrospectives",
    "href": "Donnees.html#les-données-rétrospectives",
    "title": "Les Données",
    "section": "Les données rétrospectives",
    "text": "Les données rétrospectives\n\nIndividus interrogés une seule fois.\nRecueil de biographies thématiques depuis une origine jusqu’au moment de l’enquête.\nRecueil d’informations complémentaires à la date de l’enquête (âge, sexe, csp au moment de l’enquête et/ou csp représentative).\nAvantages: Information longitudinale immédiatement disponible, “faible” coût.\nInconvénients Questionnaire long, informations datées qui font appel à la mémoire de l’enquêté.e. A de rares exceptions (enfant, mariage), il est difficile d’aller chercher des datations trop fines avec une retrospectivité assez longue.\n\nLes deux types de recueil peuvent être mixés avec des enquêtes à passages répétés comprenant des informations retrospectives entre 2 vagues (Exemple: la cohorte Elfe de l’Ined-Inserm ou la Millenium-Cohort-Study en Grande Bretagne)."
  },
  {
    "objectID": "Donnees.html#large-format-individu",
    "href": "Donnees.html#large-format-individu",
    "title": "Les Données",
    "section": "Large [format individu]",
    "text": "Large [format individu]\nUne ligne par individu, qui renseigne sur une même ligne tous les évènements liés à un domaine : les datations et les caractéristiques des évènements.\nExemple: domaine : unions - échelle temporelle: année - fin de l’observation en 1986:\n\n\n\nid\ndebut1\nfin1\ncause1\ndébut2\nfin2\ncause2\n\n\n\n\nA\n1979\n1982\ndécès conjoint\n1985\n.\n.\n\n\nB\n1983\n1984\nSéparation\n.\n.\n.\n\n\n\nInconvénients: peut générer beaucoup de vecteurs colonnes avec de nombreuses valeurs manquantes. Le nombre de colonnes va dépendre du nombre maximum d’évènements. Si ce nombre concerne un seul individu, on va multiplier le nombre de colonnes pour un niveau d’information très limité. Situation classique, le nombre d’enfants, où les naissances de rang élevé deviennent de plus en plus rares."
  },
  {
    "objectID": "Donnees.html#semi-long-format-individu-évènements",
    "href": "Donnees.html#semi-long-format-individu-évènements",
    "title": "Les Données",
    "section": "Semi-long [format individu-évènements]",
    "text": "Semi-long [format individu-évènements]\nC’est le format le plus courant de mise à disposition des enquêtes biographiques. Si l’évènement est de type continu, par exemple le lieu de résidence, la date de fin de la séquence correspond à la date de début de la séquence suivante. Les dates de fin ne sont pas forcément renseignées sur une ligne pour des trajectoires continues, l’information peut être donnée sur la ligne suivante avec la date de début.\nPour la séquence en cours au moment de l’enquête, la date de fin est souvent une valeur manquante, une fin de séquence pouvant se produire juste avant l’enquête au cours d’une même année. Il est également possible d’avoir une information qui ne s’est pas encore produite au moment de l’enquête, mais qui aura lieu peu de temps après (personne enceinte, donc une naissance probable la même année).\nExemple précédent (trajectoires discontinues):\n\n\n\nid\ndebut\nfin\ncause\nNumero séquence\n\n\n\n\nA\n1979\n1982\ndécès conjoint\n1\n\n\nA\n1985\n.\n.\n2\n\n\nB\n1983\n1984\nSéparation\n1"
  },
  {
    "objectID": "Donnees.html#long-format-individu-périodes",
    "href": "Donnees.html#long-format-individu-périodes",
    "title": "Les Données",
    "section": "Long [format individu-périodes]",
    "text": "Long [format individu-périodes]\nTypique des recueils prospectifs. Ils engendrent des lignes sans information supplémentaire par rapport à la ligne précédente.\nExemple précédent:\n\n\n\nid\nAnnée\ncause\nNumero séquence\n\n\n\n\nA\n1979\n.\n1\n\n\nA\n1980\n.\n1\n\n\nA\n1981\n.\n1\n\n\nA\n1982\nDécès conjoint\n1\n\n\nA\n1985\n.\n2\n\n\nA\n1986\n.\n2\n\n\nB\n1983\n.\n1\n\n\nB\n1984\nSéparation\n1\n\n\n\nIci les trajectoires ne sont pas continues. Une forme continue présenterait toute la trajectoire, avec l’ajout d’un statut du type être en couple ou non. Pour ID=A, en 1983 et 1984, deux lignes « pas couple » (cohabitant ou non) pourraient être insérées avec au total 3 séquences.\n Remarque : pour certaines analyses (par exemple analyse en temps discret), on doit transformer passer d’un format large ou semi-long à un format long, sur les durées observées ou sur des intervalles de durées construits."
  },
  {
    "objectID": "Donnees.html#enquête-biographie-et-entourage-ined",
    "href": "Donnees.html#enquête-biographie-et-entourage-ined",
    "title": "Les Données",
    "section": "Enquête biographie et entourage (Ined)",
    "text": "Enquête biographie et entourage (Ined)\nhttps://grab.site.ined.fr/fr/enquetes/france/biographie_entourage/\nBase sur les caractéristiques individuelles\n\nModule biographique sur le logement et les lieux de résidence"
  },
  {
    "objectID": "Donnees.html#enquête-mafe-ined",
    "href": "Donnees.html#enquête-mafe-ined",
    "title": "Les Données",
    "section": "Enquête MAFE (Ined)",
    "text": "Enquête MAFE (Ined)\nhttps://mafeproject.site.ined.fr/\n\n\n\n\n\n\nNote\n\n\n\nLa manipulation des données biographiques est un sujet très important avec ce type d’enquêtes, tout particulièrement avec MAFE . A l’initiative de Cris Beauchemin, des tutoriels de manipulation avec Stata, SAS et R ont été rédigés, en mettant à disposition un jeu de données réduit issu de l’enquête. Celui avec R doit être mis à jour dans quelques mois, il est out of date sur certaines parties (2015). La problématique choisie est la reconstitution des états annuels de transnationalité des unions au Sénégal, donc avec des situations de polygamie….c’est franchement un peu l’enfer.\nMalgré tout, on peut se faire un aperçu des divers problèmes rencontrés lorsqu’on manipule ce genre de données.\nnotes 8-9-10\n\n\nBase sur les caractéristiques individuelles\n\nModule biographique sur les lieux de résidence"
  },
  {
    "objectID": "dynamiques.html",
    "href": "dynamiques.html",
    "title": "Variables dynamiques",
    "section": "",
    "text": "Cette section sera principalement traitée par l’exemple, et on ne s’intéressera qu’aux variables de type discrète, avec un seul changement d’état."
  },
  {
    "objectID": "dynamiques.html#cox-solutions-logiciels",
    "href": "dynamiques.html#cox-solutions-logiciels",
    "title": "Variables dynamiques",
    "section": "Cox: solutions logiciels",
    "text": "Cox: solutions logiciels\n\nSasR - Stata, Python\n\n\nLa base n’est pas modifiée et la création de la TVC est faite “en aveugle” dans la procédure phreg, après l’instruction model. Ce n’est pas super.\n\n\nLa base doit être transformée en format long aux temps d’évènement (survsplit avec R, stsplit avec Stata) avant la création de la variable dynamique.\n\n\n\n\nModèle à temps discret\nMême principe pour la construction de la variable dynamique. Pour rappel l’échelle temporelle est le mois, et on a créé en amont une variable qui transforme les jour d’attente en mois (mwait).\n\n+---------------------------------------------+\n  | id   year   age   surgery   tvc   mwait   t |\n  |---------------------------------------------|\n  | 13     68    54         0     0       2   1 |\n  | 13     68    54         0     1       2   2 |\n  | 13     68    54         0     1       2   3 |\n  +---------------------------------------------+\n\n\nLogistic regression                             Number of obs     =      1,127\n                                                LR chi2(7)        =      90.73\n                                                Prob > chi2       =     0.0000\nLog likelihood = -230.32152                     Pseudo R2         =     0.1645\n\n------------------------------------------------------------------------------\n           e |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n           t |   -.365048   .0915105    -3.99   0.000    -.5444052   -.1856907\n          t2 |   .0139226   .0053256     2.61   0.009     .0034846    .0243606\n          t3 |   -.000162   .0000815    -1.99   0.047    -.0003217   -2.27e-06\n        year |  -.1324928   .0737516    -1.80   0.072    -.2770433    .0120577\n         age |    .033829   .0149503     2.26   0.024      .004527    .0631311\n     surgery |  -1.007795   .4490177    -2.24   0.025    -1.887854   -.1277365\n         tvc |  -.0543011   .3114096    -0.17   0.862    -.6646528    .5560505\n------------------------------------------------------------------------------\nRemarque : la constante n’est pas reportée, les valeurs de la référence\nn’ayant pas grand  sens (année et âge à 0)"
  },
  {
    "objectID": "frailty.html",
    "href": "frailty.html",
    "title": "Fragilité et immunité",
    "section": "",
    "text": "Je suis vraiment très très en retard dans la mise à jour de la formation\nA faire un jour….mais le plus rapidement sera le mieux.\nQuelques remarques tout de même…\n\n\nFragilité (Frailty)\nPour la fragilité (« frailty »), lire la dernière section du document de travail de Simon Quantin, je n’ai pas vu de meilleure présentation du problème que la sienne. Très important, car une des sources de la non proportionnalité des risques se trouvent dans l’omission de variables. Ici on va être confronté une omission sur des traits non observables. Certaines de ces caractéristiques vont « accélérer » dès le début de la période d’exposition la survenue de l’évènement. L’introduction d’un facteur de fragilité se fait par l’introduction d’un effet aléatoire dans le modèle, de nature plus complexe, reposant sur des hypothèse fortes et rendant l’interprétation des modèles plus compliquée (on doit passer par des graphiques, le tableau de régression ne pouvant présenter des résultats qu’en début d’exposition).\n\n\nImmunité (Cure fraction)\nPour l’immunité qui est un cas particulier du précédent, il est étudié depuis les années 1950. On va questionner l’exposition au risque d’une partie des observations. Visuellement on peut commencer à s’interroger lorsque la courbe de séjour ne tend pas vers 0 mais présente une longue asymptote sur une valeur nettement supérieure à 0: \\(\\lim_{t \\to \\infty}S(t)=a\\). Cette problématique peut fortement affecter les modèles de durées avec des évènements récurrents (non présentés). Pour traiter cette question, les modèles peuvent être de type mixte (probabilité d’être immunisé associée à un modèle de durée) ou non mixte de type bayesien. Il n’y a pas de méthode unifiée à ce jour, et reste très dépendante du champ d’analyse.\nEn démographie des analyses, méthodologiquement plutôt complexes, ont été réalisées dans le champ de la fécondité."
  },
  {
    "objectID": "fsurvie.html",
    "href": "fsurvie.html",
    "title": "Les fonctions de survie",
    "section": "",
    "text": "Les méthodes non paramétriques portent généralement sur l’analyse des fonctions de survie (séjour) ou sur celle des fonctions de répartitions, plus rarement sur les mesures d’incidence données par le risque cumulé. Deux méthodes d’estimations sont proposées : la méthode dite actuarielle et la méthode dite de Kaplan & Meier. Ces deux méthodes sont adaptées à des mesures différentes de la durée : plutôt discrète pour la technique actuarielle et plutôt continue pour Kaplan-Meier (KM). Cela induit un traitement différent de la censure dans l’estimation. La seconde est de très très loin la plus diffusée, surement en raison des tests de comparaison qu’elle propose."
  },
  {
    "objectID": "fsurvie.html#les-variables-danalyse",
    "href": "fsurvie.html#les-variables-danalyse",
    "title": "Les fonctions de survie",
    "section": "Les variables d’analyse",
    "text": "Les variables d’analyse\nOn a un échantillon aléatoire de \\(n\\) individus avec:\n\nDes indicateurs de fin d’épisode \\(e_1,e_2,....,e_k\\) avec \\(e_i=0\\) si censure à droite et \\(e_i=1\\) si évènement observé pendant la période d’observation.\nDes durées d’exposition au risque \\(t_1,t_2,....,t_k\\) jusqu’à l’évènement ou la censure.\nEn théorie, il ne peut pas y avoir d’évènement en \\(t=0\\)."
  },
  {
    "objectID": "fsurvie.html#calcul-de-la-fonction-de-survie",
    "href": "fsurvie.html#calcul-de-la-fonction-de-survie",
    "title": "Les fonctions de survie",
    "section": "Calcul de la fonction de survie",
    "text": "Calcul de la fonction de survie\nRappel: La fonction de survie donne la probabilité que l’évènement survienne après \\(t_i\\), soit \\(S(t_i)=P(T>t_i)\\).\nPour survivre en \\(t_i\\), il faut avoir survécu en \\(t_{i-1}\\), \\(t_{i-2}\\), …., \\(t_{1}\\).\nLa fonction de survie rapporte donc des probabilités conditionnelles: survivre en \\(t_i\\) conditionnellement au fait d’y avoir survécu avant. Il s’agit donc d’un produit de probabilités:\n\nSoit \\(d_i=\\sum e_i\\) le nombre d’évènements observé en \\(t_i\\) et \\(r_i\\) la population encore soumise au risque en \\(i\\). On peut mesurer l’intensité de l’évènement en \\(t_i\\) en calculant le quotient \\(q(t_i)=\\frac{d_i}{r_i}\\). Si le temps est strictement continu on devrait toujours avoir \\(q(t_i)=\\frac{1}{r_i}\\).\n\\(S(t_i) = (1 - \\frac{d_i}{r_i})\\times{S(t_{i-1})} = S(t_i) = (1 - q(t_i))\\times{S(t_{i-1})}\\). En remplaçant \\(S(t_{i-1})\\) par sa valeur: \\(S(t_i) = (1 - \\frac{d_i}{r_i})\\times(1 - \\frac{d_{i-1}}{r_{i-1}})\\times{S(t_{i-2})}\\).\n En remplaçant toutes les expressions de la survie jusqu’en \\(t_0\\) (\\(S(0)=1\\)):\n\n\\[S(t_i)=\\displaystyle \\prod_{t_i\\leq{k}} (1-q(t_i))\\]\n\n\n\n\n\n\n\nNote\n\n\n\nApplication pour la suite de la formation\n\nOn va analyser le risque de décéder (la survie) de personnes souffrant d’une insuffisance cardiaque. Le début de l’exposition est leur inscription dans un registre d’attente pour une greffe du coeur.\nLes covariables sont dans un premier temps toutes fixes: l’année (year) et l’âge (age) à l’entrée dans le registre, et le fait d’avoir été opéré pour un pontage aorto-coronarien avant l’inscription (surgery). Le début de l’exposition au risque est l’entrée dans le registre, la durée est mesurée en jour (stime). La variable évènement est le décès (died).\nL’introduction d’une dimension dynamique, la greffe, est donnée par les informations présentent dans les les variable transplant et wait.\n[Mettre liens git avec la base en .csv, .dta, .sas7bdat]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nyear\nage\ndied\nstime\nsurgery\ntransplant\nwait\nmois\ncompet\n\n\n\n\n15\n68\n53\n1\n1\n0\n0\n0\n1\n1\n\n\n43\n70\n43\n1\n2\n0\n0\n0\n1\n1\n\n\n61\n71\n52\n1\n2\n0\n0\n0\n1\n1\n\n\n75\n72\n52\n1\n2\n0\n0\n0\n1\n1\n\n\n102\n74\n40\n0\n11\n0\n0\n0\n1\n0\n\n\n74\n72\n29\n1\n17\n0\n1\n5\n1\n2"
  },
  {
    "objectID": "fsurvie.html#estimation",
    "href": "fsurvie.html#estimation",
    "title": "Les fonctions de survie",
    "section": "Estimation",
    "text": "Estimation\nEchelle temporelle\nLa durée est divisée en \\(J\\) intervalles, en choisissant \\(J\\) points: \\(t_0<t_1<...<t_J\\) avec \\(t_{J+1}=\\infty\\).\nCalcul du Risk set\n\nA \\(t_{min}=0\\), \\(n_0=n\\) individus soumis au risque: \\(r_0=n_0\\).\nLe nombre d’exposé.e.s au risque sur un intervalle est calculé en soustrayant la moitié des cas censurés sur la longueur de l’intervalle: \\(r_i=n_i- 0.5\\times{c_i}\\), avec \\(n_i\\) le nombre de personnes soumises au risque au début de l’intervalle et \\(c_i\\) le nombre d’observations censurées sur la longueur de l’intervalle. On suppose donc que les observations censurées \\(c_i\\) sont sorties de l’observation uniformément sur l’intervalle. Les cas censurés le sont en moyenne au millieu de l’intervalle. \n\nCalcul de \\(S(t_i)\\)\nOn applique la méthode de la section précédente avec:\n\\[q(t_i)=\\frac{d_i}{n_i - 0.5\\times c_i}\\]\nCalcul de la durée médiane (ou autre quantiles)\nRappel: en raison de la présence de censures à droite, le dernier intervalle étant ouvert jusqu’à la dernière sortie d’observation, il n’est pas conseillé de calculer des durées moyennes. On préfère utiliser la médiane ou tout autre quantile lorsqu’ils sont calculables.\nDéfinition: il s’agit de la durée telle que \\(S(t_i)=0.5\\).\nCalcul: Comme on applique une méthode continue et monotone à l’intérieur d’intervalles, on ne peut pas calculer directement un point de coupure qui correspond à 50% de survivants. On doit donc trouver ce point par interpolation linéaire dans l’intervalle \\([t_i;t_{i+1}[\\) avec \\(S(t_{i+1})\\leq0.5\\) et \\(S(t_{i})>0.5\\)."
  },
  {
    "objectID": "fsurvie.html#r-stata-sas-python",
    "href": "fsurvie.html#r-stata-sas-python",
    "title": "Les fonctions de survie",
    "section": "R-Stata-Sas-Python",
    "text": "R-Stata-Sas-Python\n\nRStataSasPython\n\n\nLes fonctions de survie avec la méthode dite actuarielle sont estimables avec le package discSurv. Avec le temps, il s’est étoffé, on peut maintenant paramatrer des intervalles, mais les quantiles de la durées ne sont toujours pas estimables, ce qui est fort dommage.\nDans les résulats qui suivent, ces quantiles ont été estimés avec Stata en retenant la définition des bornes de Sas.\n\n\nCommande ltable, avec en option la paramétrisation des intervalles de durées. Voir la commande externe qlt (MT) qui calcule les durées médianes (+ autres quartiles) et qui recalcule la fonction de séjour avec une définition des intervalles de durées identique à celle de SAS.\n\n\nSous une proc lifetest avec en option method=lifetable. On peut paramétrer les intervalles d’estimation avec l’option width.\n\n\nA l’heure actuelle, aucune fonction à ma connaissance\n\n\n\n\nStata: commande ltable. Voir la commande externe qlt (MT) qui calcule les durées médianes (+ autres quartiles) et qui cale la définition des intervalles avec celle de SAS.\nR: une fonction programmée par un utilisateur (package discSurv => fonction lifeTable), mais pas convaincante car pas d’estimation sur les quantiles, et estimation avec des intervalles toujours fixés à \\(dt=1\\). D’un intérêt très limité, voire nul.\n\nPython: à l’heure actuelle, aucune fonction à ma connaissance."
  },
  {
    "objectID": "fsurvie.html#application",
    "href": "fsurvie.html#application",
    "title": "Les fonctions de survie",
    "section": "Application",
    "text": "Application\n\n                 Beg.                                 Std.\n   Interval     Total   Deaths   Lost    Survival    Error     [95% Conf. Int.]\n-------------------------------------------------------------------------------\n    0    10       103       13      0     0.8738    0.0327     0.7926    0.9247\n   10    20        90        6      1     0.8152    0.0383     0.7257    0.8779\n   20    30        83        3      0     0.7857    0.0405     0.6931    0.8533\n   30    40        80        6      2     0.7261    0.0441     0.6284    0.8020\n   40    50        72        4      0     0.6857    0.0461     0.5857    0.7664\n   50    60        68        4      0     0.6454    0.0476     0.5439    0.7299\n   60    70        64        5      0     0.5950    0.0489     0.4926    0.6834\n   70    80        59        4      0     0.5546    0.0496     0.4523    0.6454\n   80    90        55        3      0     0.5244    0.0499     0.4225    0.6165\n   90   100        52        2      0     0.5042    0.0499     0.4029    0.5971\n  100   110        50        2      1     0.4838    0.0500     0.3831    0.5773\n  110   120        47        1      0     0.4735    0.0499     0.3732    0.5673\n  130   140        46        0      1     0.4735    0.0499     0.3732    0.5673\n  140   150        45        1      0     0.4630    0.0499     0.3631    0.5570\n  150   160        44        1      0     0.4525    0.0499     0.3530    0.5467\n  160   170        43        1      0     0.4420    0.0498     0.3429    0.5364\n  180   190        42        2      1     0.4207    0.0496     0.3227    0.5154\n  200   210        39        1      0     0.4099    0.0495     0.3125    0.5047\n  210   220        38        1      0     0.3991    0.0494     0.3024    0.4939\n  260   270        37        1      1     0.3882    0.0492     0.2921    0.4830\n  280   290        35        2      0     0.3660    0.0489     0.2714    0.4608\n  300   310        33        1      0     0.3549    0.0486     0.2612    0.4496\n  330   340        32        1      0     0.3438    0.0483     0.2510    0.4383\n  340   350        31        2      1     0.3213    0.0477     0.2305    0.4153\n  370   380        28        0      1     0.3213    0.0477     0.2305    0.4153\n  390   400        27        0      1     0.3213    0.0477     0.2305    0.4153\n  420   430        26        0      1     0.3213    0.0477     0.2305    0.4153\n  440   450        25        0      1     0.3213    0.0477     0.2305    0.4153\n  480   490        24        0      1     0.3213    0.0477     0.2305    0.4153\n  510   520        23        0      1     0.3213    0.0477     0.2305    0.4153\n  540   550        22        0      1     0.3213    0.0477     0.2305    0.4153\n  580   590        21        1      0     0.3060    0.0478     0.2156    0.4008\n  590   600        20        0      1     0.3060    0.0478     0.2156    0.4008\n  620   630        19        0      1     0.3060    0.0478     0.2156    0.4008\n  670   680        18        1      1     0.2885    0.0482     0.1983    0.3847\n  730   740        16        1      0     0.2705    0.0484     0.1808    0.3680\n  840   850        15        0      1     0.2705    0.0484     0.1808    0.3680\n  850   860        14        1      0     0.2511    0.0487     0.1622    0.3501\n  910   920        13        0      1     0.2511    0.0487     0.1622    0.3501\n  940   950        12        0      1     0.2511    0.0487     0.1622    0.3501\n  970   980        11        1      0     0.2283    0.0493     0.1398    0.3299\n  990  1000        10        1      0     0.2055    0.0494     0.1187    0.3088\n 1030  1040         9        1      0     0.1826    0.0489     0.0988    0.2869\n 1140  1150         8        0      1     0.1826    0.0489     0.0988    0.2869\n 1320  1330         7        0      1     0.1826    0.0489     0.0988    0.2869\n 1380  1390         6        1      0     0.1522    0.0493     0.0715    0.2609\n 1400  1410         5        0      2     0.1522    0.0493     0.0715    0.2609\n 1570  1580         3        0      1     0.1522    0.0493     0.0715    0.2609\n 1580  1590         2        0      1     0.1522    0.0493     0.0715    0.2609\n 1790  1800         1        0      1     0.1522    0.0493     0.0715    0.2609\n-------------------------------------------------------------------------------\n\nDurée pour differents quantiles de la fonction de survie\nDéfinition des bornes Sas-lifetest\nS(t)=0.90: t=    7.923\nS(t)=0.75: t=   35.989\nS(t)=0.50: t=  102.068\nS(t)=0.25: t=  913.968\nS(t)=0.10: t=        .\n\n\n102 jours après leur inscription dans le registre d’attente pour une greffe, 50% des malades sont toujours en vie. Au bout de 914 jours, 75% des personnes sont décédées."
  },
  {
    "objectID": "fsurvie.html#estimation-1",
    "href": "fsurvie.html#estimation-1",
    "title": "Les fonctions de survie",
    "section": "Estimation",
    "text": "Estimation\nDéfinition du Risk Set (\\(r_i\\))\nS’il y a à la fois des évènements et des censures à une durée \\(t_i\\), les observations censurées sont considérées comme exposées au risque à ce moment, comme si elles étaient censurées très rapidement après. C’est la principale caractéristique de cette méthode, appelé également l’estimateur « product-limit »\n\\[r_i=r_{i-1}-d_{i-1}-c_{i-1}\\]\n\nCalcul de \\(q_i\\)\n On applique la méthode de la section précédente avec:\n\\[q_i=\\frac{d_i}{r_{i-1}-d_{i-1}-c_{i-1}}\\] Remarque: la variance de l’estimateur est obtenu par la méthode dite de “Greenwood”“. Il n’y a pas d’intérêt particulier pour ce cours de la décrire.\nRécupération de la médiane\nIl n’y a pas de méthode pour calculer directement la durée médiane (ou tout autre quantile).\nOn va prendre la valeur de la durée qui se situe juste “en dessous” de 50% de survivant.e.s. Elle est donc définie tel que \\(S(t)\\leq0.5\\). donc pas de formule savante pour obtenir ce résultat, c’est une convention. Attention, il n’est pas impossible que le % de survivant.e.s soit bien en deçà de 50% pour l’obtention cette durée médiane."
  },
  {
    "objectID": "fsurvie.html#r-stata-sas-python-1",
    "href": "fsurvie.html#r-stata-sas-python-1",
    "title": "Les fonctions de survie",
    "section": "R-Stata-Sas-Python",
    "text": "R-Stata-Sas-Python\n\nRStataSASPython\n\n\nLes estimateurs sont obtenus avec fonction survfit de la librairie survival. On peut obtenir des rendus graphiques supérieurs avec la librairie survminer (fonction ggsurvplot)\n\n\nAprès avoir appelé les variables de durée et de censure en mode survival avec stset), le tableau des estimateurs est obtenu la commande sts list et le graphique avec sts graph.\n\n\nL’estimation de Kaplan-Meier est affichée par défaut par la proc lifetest.\nWarning : le tableau affiché par SAS est particulièrement pénible à lire voire illisible, en particulier lorsque le nombre de censures est élevé, une ligne étant ajoutée pour chaque observation censurée. Je conseille de ne pas afficher cette partie de l’output (se reporter à la section SAS du chapitre programmation). On récupère pour le reste de l’output les valeurs de la durée pour S(t) =(.75,.5,.25) ainsi que le graphique, ce qui est suffisant.\n\n\nLes resultats sont donnés dans la librairie lifeline par des fonctions dont le nom est interminable. Je conseille plutôt l’utilisation de la librairie statmodels (se reporter à la section dédiée à Python)."
  },
  {
    "objectID": "fsurvie.html#application-1",
    "href": "fsurvie.html#application-1",
    "title": "Les fonctions de survie",
    "section": "Application",
    "text": "Application\nOn reprend l’exemple précédent.\n\nTime    Total   Fail   Lost           Function     Error     [95% Conf. Int.]\n-------------------------------------------------------------------------------\n     1      103      1      0             0.9903    0.0097     0.9331    0.9986\n     2      102      3      0             0.9612    0.0190     0.8998    0.9852\n     3       99      3      0             0.9320    0.0248     0.8627    0.9670\n     5       96      2      0             0.9126    0.0278     0.8388    0.9535\n     6       94      2      0             0.8932    0.0304     0.8155    0.9394\n     8       92      1      0             0.8835    0.0316     0.8040    0.9321\n     9       91      1      0             0.8738    0.0327     0.7926    0.9247\n    11       90      0      1             0.8738    0.0327     0.7926    0.9247\n    12       89      1      0             0.8640    0.0338     0.7811    0.9171\n    16       88      3      0             0.8345    0.0367     0.7474    0.8937\n    17       85      1      0             0.8247    0.0375     0.7363    0.8857\n    18       84      1      0             0.8149    0.0383     0.7253    0.8777\n    21       83      2      0             0.7952    0.0399     0.7034    0.8614\n    28       81      1      0             0.7854    0.0406     0.6926    0.8531\n    30       80      1      0             0.7756    0.0412     0.6819    0.8448\n    31       79      0      1             0.7756    0.0412     0.6819    0.8448\n    32       78      1      0             0.7657    0.0419     0.6710    0.8363\n    35       77      1      0             0.7557    0.0425     0.6603    0.8278\n    36       76      1      0             0.7458    0.0431     0.6495    0.8192\n    37       75      1      0             0.7358    0.0436     0.6388    0.8106\n    39       74      1      1             0.7259    0.0442     0.6282    0.8019\n    40       72      2      0             0.7057    0.0452     0.6068    0.7842\n    43       70      1      0             0.6956    0.0457     0.5961    0.7752\n    45       69      1      0             0.6856    0.0461     0.5855    0.7662\n    50       68      1      0             0.6755    0.0465     0.5750    0.7572\n    51       67      1      0             0.6654    0.0469     0.5645    0.7481\n    53       66      1      0             0.6553    0.0472     0.5541    0.7390\n    58       65      1      0             0.6452    0.0476     0.5437    0.7298\n    61       64      1      0             0.6352    0.0479     0.5333    0.7206\n    66       63      1      0             0.6251    0.0482     0.5230    0.7113\n    68       62      2      0             0.6049    0.0487     0.5026    0.6926\n    69       60      1      0             0.5948    0.0489     0.4924    0.6832\n    72       59      2      0             0.5747    0.0493     0.4722    0.6643\n    77       57      1      0             0.5646    0.0494     0.4621    0.6548\n    78       56      1      0             0.5545    0.0496     0.4521    0.6453\n    80       55      1      0             0.5444    0.0497     0.4422    0.6357\n    81       54      1      0             0.5343    0.0498     0.4323    0.6261\n    85       53      1      0             0.5243    0.0499     0.4224    0.6164\n    90       52      1      0             0.5142    0.0499     0.4125    0.6067\n    96       51      1      0             0.5041    0.0499     0.4027    0.5969\n   100       50      1      0             0.4940    0.0499     0.3930    0.5872\n   102       49      1      0             0.4839    0.0499     0.3833    0.5773\n   109       48      0      1             0.4839    0.0499     0.3833    0.5773\n   110       47      1      0             0.4736    0.0499     0.3733    0.5673\n   131       46      0      1             0.4736    0.0499     0.3733    0.5673\n   149       45      1      0             0.4631    0.0499     0.3632    0.5571\n   153       44      1      0             0.4526    0.0499     0.3531    0.5468\n   165       43      1      0             0.4421    0.0498     0.3430    0.5364\n   180       42      0      1             0.4421    0.0498     0.3430    0.5364\n   186       41      1      0             0.4313    0.0497     0.3327    0.5258\n   188       40      1      0             0.4205    0.0497     0.3225    0.5152\n   207       39      1      0             0.4097    0.0495     0.3123    0.5045\n   219       38      1      0             0.3989    0.0494     0.3022    0.4938\n   263       37      1      0             0.3881    0.0492     0.2921    0.4830\n   265       36      0      1             0.3881    0.0492     0.2921    0.4830\n   285       35      2      0             0.3660    0.0488     0.2714    0.4608\n   308       33      1      0             0.3549    0.0486     0.2612    0.4496\n   334       32      1      0             0.3438    0.0483     0.2510    0.4383\n   340       31      1      1             0.3327    0.0480     0.2409    0.4270\n   342       29      1      0             0.3212    0.0477     0.2305    0.4153\n   370       28      0      1             0.3212    0.0477     0.2305    0.4153\n   397       27      0      1             0.3212    0.0477     0.2305    0.4153\n   427       26      0      1             0.3212    0.0477     0.2305    0.4153\n   445       25      0      1             0.3212    0.0477     0.2305    0.4153\n   482       24      0      1             0.3212    0.0477     0.2305    0.4153\n   515       23      0      1             0.3212    0.0477     0.2305    0.4153\n   545       22      0      1             0.3212    0.0477     0.2305    0.4153\n   583       21      1      0             0.3059    0.0478     0.2156    0.4008\n   596       20      0      1             0.3059    0.0478     0.2156    0.4008\n   620       19      0      1             0.3059    0.0478     0.2156    0.4008\n   670       18      0      1             0.3059    0.0478     0.2156    0.4008\n   675       17      1      0             0.2879    0.0483     0.1976    0.3844\n   733       16      1      0             0.2699    0.0485     0.1802    0.3676\n   841       15      0      1             0.2699    0.0485     0.1802    0.3676\n   852       14      1      0             0.2507    0.0487     0.1616    0.3497\n   915       13      0      1             0.2507    0.0487     0.1616    0.3497\n   941       12      0      1             0.2507    0.0487     0.1616    0.3497\n   979       11      1      0             0.2279    0.0493     0.1394    0.3295\n   995       10      1      0             0.2051    0.0494     0.1183    0.3085\n  1032        9      1      0             0.1823    0.0489     0.0985    0.2865\n  1141        8      0      1             0.1823    0.0489     0.0985    0.2865\n  1321        7      0      1             0.1823    0.0489     0.0985    0.2865\n  1386        6      1      0             0.1519    0.0493     0.0713    0.2606\n  1400        5      0      1             0.1519    0.0493     0.0713    0.2606\n  1407        4      0      1             0.1519    0.0493     0.0713    0.2606\n  1571        3      0      1             0.1519    0.0493     0.0713    0.2606\n  1586        2      0      1             0.1519    0.0493     0.0713    0.2606\n  1799        1      0      1             0.1519    0.0493     0.0713    0.2606\n-------------------------------------------------------------------------------\n\nDurée médiane: \\(t=100\\) (correspond à \\(S(t)=0.4940\\))."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analyse des durées",
    "section": "",
    "text": "Maj à venir sur les résidus de Schoenfeld [21 Octobre 2022]\n\n\n\nTest simplifié versus test exact\nDepuis la version 3 du pkg survival le test permettant d’étudier la validé de l’hypothèse de constance des risques suite à l’estimation d’un modèle de Cox, est celui proposé dans l’article de P.Grambsch et T.Therneau.\nJusqu’à cette version du pkg, une version simplifiée du test était implémenté. C’est également le cas actuellement dans les autres applications statistiques tels que Stata (estat phtest), Sas (proc phreg), Python (lifelines,statmodels).\nDans son principe le test (du score) n’est pas compliqué puisqu’il consiste à introduire une intéraction entre une fonction de la durée et un rapport de risque.\nDifférence entre le test simplifié et le test exact:\n\nTest simplifié: La variance des résidus est celle de l’estimateur du rapport de risque de la covariable. Le test se réduit à une régression linéaire (ordinaire), la standardisation des résidus se fait en appliquant dans l’équation de score une variance moyenne.\nTest exact: pour chaque résidu on calcule leur variance à partir de l’information de Fisher (dérivée de l’équation de score, ici juste sur les termes de la diagonale) . Le test consiste alors à estimer pour chaque covariable une régression linéaire généralisée. On peut retrouver les détails (très) techniques de la méthode dans l’Article de Grambsch-Therneau.\n\nPour les personnes qui utilisent R, le test “simplifié” a été supprimé du package, ce qui ne me semble pas du tout une bonne idée. Je donne néanmoins un moyen simple de le récupérer et de l’exécuter.\nSur quelques tests (données réelles et simulations réalisées avec le pkg coxed), le choix du test, simplifié ou exact, ne semble pas neutre en particulier lorsqu’on travaille sur des durées discrètes ou groupées… soit classiquement les données utilisées en sciences sociales. Se pose également un problème assez critique de reproductibilité, les résultats variants à ce jour d’une solution logicielle à une autre.\nCommme si le choix de la fonction de la durée, elle même débattue dans la littérature, ne suffisait pas.\nD’ici la fin novembre, une note va être incluse au support pour donner des éléments plus précis sur la computation du test. Cela me permetrra d’ajoutés une section sera ajoutée sous formes d’annexes techniques, que j’ai souhaité réduire au maximum dans le corps du support. La sections dédiées au test et à la programmation feront également l’objet d’une modification. J’indiquerai ici l’avancée de la maj, et laisserai cet avertissement plusieurs mois.\n\n\n\n\n\n\n\n\nBibliographie (courte mais efficace)\n\n\n\n\nCours de Gilbert Colletaz (version 2016): \nDocument de travail Insee de Simon Quantin (2019):  [avec version 2 de la librarie survival]\n\n\n\n\n\n\n\n\n\n\nLiens INED\n\n\n\n\nSite de l’Ined: https://www.ined.fr\n\nSite du Service Méthodes Statistiques: https://sms.site.ined.fr\nSite des Rencontres de Statistique Appliquée: https://statapp.site.ined.fr\nSite du séminaire R à l’Usage des Sciences Sociales (RUSS): https://russ.site.ined.fr\n\n\n\n\n\n\n\n\n\nAutres supports\n\n\n\n\nhttps://github.com/mthevenin\nVisualisation des données avec Stata (2020-2022): \nIntroduction à Stata (2018 - un peu ancienne - MAJ en cours): \nIntroduction à la regression logistique (v1 2022): \n\n\n\n  mailto:marc.thevenin@ined.fr - Service Méthodes Statistiques - Ined -\n  Support réalisé avec Rstudio - Quarto\n Langages utilisés pour la partie programmation: R - Stata - Sas - Python"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Questions\nOn dispose de données dites “longitudinales”, et on cherche à appréhender l’occurence d’un évènement au sein d’une population. Les problématiques se basent sur les questions suivantes:\n\n\nObserve-t-on la survenue de l’évènement pour l’ensemble des individus?\nQuelle est la durée jusqu’à la survenue de l’évènement?\nQuels sont les facteurs qui favorisent la survenue de cet évènement? Facteurs fixes ou facteurs pouvant apparaitre/changer au cours de la période d’observation: variables dynamiques (TVC: Time Varying Covariate)\n\n\n\nTerminologies\n\n\n\n\n\n\n\nFrançais\nAnglais\n\n\n\n\nModèles de durée\nDuration analysis (Econométrie)\n\n\nAnalyse de survie\nSurvival analysis (Epidémiologie, médecine, démographie)\n\n\nAnalyse de fiabilité\nFailure time data analysis (Statistiques industrielles)\n\n\nAnalyse des transitions\nEvent-history analysis (Démographie, Sociologie)\n\n\nDonnées de séjour\nTransition analysis (Sociologie)\n\n\nHistoires de vie\n\n\n\n\n\n\n\nExemples d’analyse\nNuptialité, Mise en couple: cohabiter, décohabiter, se marier, Rompre une union …\n Logement: Changement de statut (locataire <=> propriétaire), mobilité résidentielle …\n Emploi: Trouver un 1er emploi, changer d’emploi, entrée ou sortie du chômage …\n Fécondité: Avoir un premier enfant, avoir un nouvel enfant …\n Mortalité: Décéder après diagnostic, survivre après l’administration un traitement…\n\n\n\nElements nécessaire à l’analyse\n\nUn processus temporel\n\nUne échelle de mesure (minutes, heures, jours, mois, années….)\nUne origine définissant un évènement de départ.\n\nUne définition précise de l’évènement d’étude.\n\nUne durée entre le début et la fin de la période d’observation, si nécessaire la fin de la période d’exposition au risque.\n\nUne population soumise au risque de connaître l’évènement (Risk Set)\nDes variables explicatives ou covariables\n\nFixes: genre, génération, niveau de diplôme, csp,……\nDynamiques (TVC: Time varying covariates): Mesurées à tout moment entre le début et la sortie de l’observation: statut matrimonial, taille du ménage, statut d’activité…\n\n\n\n\n\nBibliographie\nLes éléments bibliographiques qui figurent ci-dessous proviennent du champ des sciences sociales ou de l’économie. Quelle que soit la langue, le nombre de cours ou documents sont très nombreux dans le domaine de la médecine.\n Cours Gilbert Colletaz (Université d’Orléans). Le cours est mis à jour tous les ans. Il n’est plus possible d’accéder directement à la dernière version du document, mais il est néanmoins possible de télécharger la version 2016 ( https://docplayer.fr/69359088-Modeles-de-survie-notes-de-cours-master-2-esa-voies-professionnelle-et-recherche-gilbert-colletaz.html ). Applications avec Sas.\n Document de travail de Simon Quantin (Insee). Egalement un excellent document, qui couvre l’ensemble des techniques de base d’analyse des durées en temps dit continu (https://www.insee.fr/fr/statistiques/3695681 ). Il propose surement la meilleure introduction en langue française à la problématique de la fragilité. Applications avec R. On peut juste regretter que les risques concurrents ne soient pas abordés."
  },
  {
    "objectID": "intro_mod.html",
    "href": "intro_mod.html",
    "title": "Introduction",
    "section": "",
    "text": "Proprortionnalité des risques\n La spécification usuelle est: \\[h(t)=h_0(t)\\times e^{X^{'}b}\\] \n\n\\(h(t)\\) est une fonction de risque.\n\\(h_0(t)\\) est une fonction qui dépend du temps mais pas des caractéristiques individuelles. Il définiera le risque de base (baseline).\n\\(e^{X^{'}b}\\) est une fonction qui ne dépend pas du temps, mais des caractéristiques individuelles \\(X^{'}b=\\sum_{k=1}^{p}b_kX_k\\). La forme exponentielle assurera la positivité du risque.\n\n Le risque de base\n\n\\(h(t)=h_0(t)\\) donc \\(e^{X^{'}b}=1\\)\nObservations pour lesquelles \\(X=0\\)\n\n Risques proportionnels\n Cette hypothèse stipule l’invariance dans le temps du “rapport des risques” (hazard ratio).\nAvec une seule covariable \\(X\\) introduite au modèle, et 2 individus \\(A\\) et \\(B\\): \\(h_A(t)=h_0(t)e^{bX_{A}}\\) et \\(h_B(t)=h_0(t)e^{bX_{B}}\\).\nLe rapport des risques entre \\(A\\) et \\(B\\) est égal à:\n\\[\\frac{h_A(t)}{h_B(t)}= \\frac{e^{bX_A}}{e^{bX_B}}=e^{b(X_A-X_B)}\\]\nAutrement dit, la proportionalité des risques peut traduire l’absence d’une interaction significative entres les rapports de risques estiméspar un modèle à risque proportionnel et la durée (ou une fonction de celle-ci).\nIllustration graphique\n\nOn part d’un modèle à risque constant avec \\(h_0(t)=0.1\\).\nComme \\(h_1(t)=0.2\\) quel que soit \\(t\\), le rapport des risques est toujours égal à \\(\\frac{0.2}{0.1}=2=e^{b}\\). Le coefficient estimé est égal à \\(log(2)=0.69\\).\nPour \\(h_{1b}(t)\\), le rapport de risques augmente avec le temps: \\(t=1\\), \\(h_{1b}(1)=0.15\\) et \\(h_{1b}(1000)=0.25\\) l’hypothèse de proportionalité n’est donc pas respectée.\n\n\nLes modèles\nModèle semi-paramétrique de Cox\nLe modèle estime directement les \\(b\\) indépendement de \\(h_0(t)\\), c’est pour cela qu’il est semi-paramétrique. Les rapports de risque (\\(e^{b}\\)) seront utilisés pour estimer la baseline \\(h_0(t)\\), qui peut s’avérer nécessaire pour calculer des fonctions de survie ajustées. Le respect de l’hypothèse de proportionalité est donc important et donc être testé. \nModèle à temps discret\nDe type paramétrique. Peut être estimé à l’aide d’un modèle logistique, probit ou complémentaire log-log. La première est la plus courante, la dernière a l’avantage d’être directement relié au modèle de Cox (modèle de Cox à temps discret).\n Sa forme diffère de la présentation usuelle d’un modèle à risque proportionnel. Toutefois, il est régi par une hypothèse de proportionalité. Le non respect de l’hypothèse est moins critique car la baseline du « risque » est estimée simultanément. Il est comme son nom l’indique, particulièrement adapté au durées discrètes ou groupées.  Avec une spécification logistique, la plus courante, les Odds vont sous certaines conditions, se confondre avec des probabilités/risques.\n\nLes modèles paramétriques standard\nLes modèles dits de Weibull, exponentiel ou Gompertz ont une spécification sous hypothèse de risque proportionnel. Ils seront traités brièvement dans les compléments. Historiquement, le modèle de Cox est une réponse à une possible difficulté dans l’ajustement du risque par une loi de distribution du risque a priori.\n\nModèle paramétrique de Parmar-Royston (non traité)\n\\(h_0(t)\\), via le risque cumulé \\(H(t)\\), est estimé simultanément avec les risques ratios en utilisant la populaire méthode des splines cubiques. Il est implémenté dans les logiciels standards (R, Stata, Sas). Les rapports de risque sont très proches de ceux estimés par le modèle classique de Cox.\nIl offre donc une alternative particulièrement intéressante à celui-ci, et il s’est maintenant largement diffusé dans l’analyse des effets cliniques."
  },
  {
    "objectID": "parametrique.html",
    "href": "parametrique.html",
    "title": "Modèles paramétriques",
    "section": "",
    "text": "Objectifs: présenter (très/trop rapidement) la logique des modèles de type AFT (Accelerated Failure Time), principalement le modèle de Weibull (et exponentiel). Je n’ai pas forcément de pratique sur les modèles paramétriques, et à terme plutôt intéressé pour explorer de manière approfondie et présenter le modèle de Parmar-Royston."
  },
  {
    "objectID": "parametrique.html#hypothèse-aft-accelerated-failure-time",
    "href": "parametrique.html#hypothèse-aft-accelerated-failure-time",
    "title": "Modèles paramétriques",
    "section": "Hypothèse AFT: Accelerated Failure Time",
    "text": "Hypothèse AFT: Accelerated Failure Time\nL’hypothèse AFT signifie que l’effet des covariables est multiplicatif par rapport au temps de survie. Par opposition, les modèles PH décrivent un effet multiplicatif par rapport au risque.  Selon les caractérisques des individus, le temps ne s’écoulent pas à la même vitesse, ils ne partagent plus la même métrique temporelle. Remarque: on a souvent des explications de type dilation/contraction du temps, par analogie à la théorie de la relativité.\n Exemple simple: la durée de vie d’un être humain et d’un chien.\nOn dit qu’une année de vie d’un être humain équivaut à 7 années de vie d’un chien. C’est typiquement une hypothèse d’AFT.\n\\(S_h(t) = S_c(7\\times t)\\). C’est ce facteur multiplicatif qu’estime un modèle paramétrique de type AFT.\n\\[S(t_i | X_1)=  S(\\phi t_i | X_0)\\]\nRemarque: si un modèle s’estime sous hypothèse PH (ex Weibull): \\(h(t_i | X_1)= -\\rho \\phi h(t_i | X_0)\\)\n\nAvantage: l’interprétation des modèles est directement liée aux fonctions de survie. Pratique après une analyse non paramétrique.\nInconvénient: ne permet pas l’introduction de variables dynamiques.\n\nEtre humain versus chien: la probabilité qu’un être humain survive 80 ans est égale à la probabilité qu’un chien survive 11 ans (80/7). Le temps s’écoulerait donc plus vite pour le chien que pour l’être humain (du point de vue d’un référentiel extérieur). Ce raisonnement peut s’appliquer aux quantile du temps de survie: le temps de survie médian d’un être humain est 7 fois plus élevé que celui d’un chien. En terme d’interprétation des paramètres estimés, si le temps de survie est plus court le risque est plus élevé."
  },
  {
    "objectID": "parametrique.html#principe-de-construction-des-modèles-aft",
    "href": "parametrique.html#principe-de-construction-des-modèles-aft",
    "title": "Modèles paramétriques",
    "section": "Principe de construction des modèles AFT",
    "text": "Principe de construction des modèles AFT\nLe raisonnement mathématique est ici bien plus complexe. On donnera juste quelques pistes en début de raisonnement. On part d’une expression proche du modèle linéaire (à une transformation logarithmique près de la variable dépendante). En imposant la contrainte \\(t_i>0\\), en ne posant qu’une seule covariable \\(X\\) de type binaire, et en se situant de nouveau dans une logique de temps continu (pas d’évènement simultané):\n\\[log(t_i)= \\alpha_0 +  \\alpha_1X_i + bu_i\\]\n\\(b\\) est un paramètre d’échelle identique pour toutes les observations et \\(u_i\\) un terme terme d’erreur qui suit une loi de distribution de densité \\(f(u)\\). La combinaison linéaire définira le paramètre de position. C’est la forme de \\(f(u)\\) qui définie le type de modèle paramétrique.\nOn peut écrire: \\(f(u_i) = f(\\frac{log(t_i)- \\alpha_0 - \\alpha_1X_i}{b})\\).\nRemarque: pour une distibution normale/gaussienne, le paramètre de position est l’espérance et le paramètre d’échelle l’écart-type."
  },
  {
    "objectID": "Python.html",
    "href": "Python.html",
    "title": "PYTHON",
    "section": "",
    "text": "Deux paquets d’analyse: principalementlifelines (km, cox, aft…) et statsmodels (estimation logit en temps discret, kaplan-Meier, Cox).  Le package statsmodels est également ne mesure d’estimer des courbes de séjour de type Kaplan-Meier et des modèles à risque proportionnel de Cox. Le package lifelines couvre la quasi totalité des méthodes standards, à l’exception des les risques concurrents.\nChargement de la base"
  },
  {
    "objectID": "Python.html#non-paramétrique-kaplan-meier",
    "href": "Python.html#non-paramétrique-kaplan-meier",
    "title": "PYTHON",
    "section": "Non Paramétrique: Kaplan Meier",
    "text": "Non Paramétrique: Kaplan Meier\n\nEstimateur KM et durée médiane\n\nT = trans['stime']\nE = trans['died']\n\n\nfrom lifelines import KaplanMeierFitter\nkmf = KaplanMeierFitter()\nkmf.fit(T,E)\nprint(kmf.survival_function_)\na = \"DUREE MEDIANE:\"\nb = kmf.median_survival_time_\nprint(a,b)\n\n\n          KM_estimate\ntimeline             \n0.0          1.000000\n1.0          0.990291\n2.0          0.961165\n3.0          0.932039\n5.0          0.912621\n...               ...\n1400.0       0.151912\n1407.0       0.151912\n1571.0       0.151912\n1586.0       0.151912\n1799.0       0.151912\n\n[89 rows x 1 columns]\nDUREE MEDIANE: 100.0\n\n\nkmf.plot()\n\n\n\n\nComparaison des fonctions de survie\n\nax = plt.subplot(111)\nkmf = KaplanMeierFitter()\nfor name, grouped_df in trans.groupby('surgery'):\n    kmf.fit(grouped_df['stime'], grouped_df['died'], label=name)\n    kmf.plot(ax=ax)\n\n\n\nfrom lifelines.statistics import multivariate_logrank_test\nresults = multivariate_logrank_test(trans['stime'], trans['surgery'], trans['died'])\nresults.print_summary()\n\n\n<lifelines.StatisticalResult: multivariate_logrank_test>\n               t_0 = -1\n null_distribution = chi squared\ndegrees_of_freedom = 1\n         test_name = multivariate_logrank_test\n\n---\n test_statistic    p  -log2(p)\n           6.59 0.01      6.61"
  },
  {
    "objectID": "Python.html#semi-paramétrique-cox",
    "href": "Python.html#semi-paramétrique-cox",
    "title": "PYTHON",
    "section": "Semi paramétrique: Cox",
    "text": "Semi paramétrique: Cox\n\nEstimation\n\nmodel = 'year + age + C(surgery) -1'\nX = pt.dmatrix(model, trans, return_type='dataframe')\ndesign_info = X.design_info\nYX = X.join(trans[['stime','died']])\nYX.drop(['C(surgery)[0]'], axis=1, inplace=True)\nYX.head()\n\n\nfrom lifelines import CoxPHFitter\ncph = CoxPHFitter()\ncph.fit(YX, duration_col='stime', event_col='died')\ncph.print_summary()\ncph.plot()\n\n\n<lifelines.CoxPHFitter: fitted with 103 total observations, 28 right-censored observations>\n             duration col = 'stime'\n                event col = 'died'\n      baseline estimation = breslow\n   number of observations = 103\nnumber of events observed = 75\n   partial log-likelihood = -289.31\n         time fit was run = 2021-04-21 13:24:52 UTC\n\n---\n                coef  exp(coef)   se(coef)   coef lower 95%   coef upper 95%  exp(coef) lower 95%  exp(coef) upper\n>  95%\nC(surgery)[1]  -0.99       0.37       0.44            -1.84            -0.13                 0.16                 \n> 0.88\nyear           -0.12       0.89       0.07            -0.25             0.01                 0.78                 \n> 1.01\nage             0.03       1.03       0.01             0.00             0.06                 1.00                 \n> 1.06\n\n                  z    p   -log2(p)\nC(surgery)[1] -2.26 0.02       5.40\nyear          -1.78 0.08       3.72\nage            2.19 0.03       5.12\n---\nConcordance = 0.65\nPartial AIC = 584.61\nlog-likelihood ratio test = 17.63 on 3 df\n-log2(p) of ll-ratio test = 10.90\n\n\n\n\nTests hypothèse PH\nTest PH: Schoenfeld Méthode 1\n\ncph.check_assumptions(YX,p_value_threshold=0.05)\n\n\nThe ``p_value_threshold`` is set at 0.05. Even under the null hypothesis of no violations, some\ncovariates will be below the threshold by chance. This is compounded when there are many covariates.\nSimilarly, when there are lots of observations, even minor deviances from the proportional hazard\nassumption will be flagged.\n\nWith that in mind, it's best to use a combination of statistical tests and visual tests to determine\nthe most serious violations. Produce visual plots using ``check_assumptions(..., show_plots=True)``\nand looking for non-constant lines. See link [A] below for a full example.\n\n<lifelines.StatisticalResult: proportional_hazard_test>\n null_distribution = chi squared\ndegrees_of_freedom = 1\n         test_name = proportional_hazard_test\n\n---\n                    test_statistic    p  -log2(p)\nC(surgery)[1] km              4.01 0.05      4.47\n              rank            3.74 0.05      4.23\nage           km              1.18 0.28      1.86\n              rank            1.06 0.30      1.72\nyear          km              2.07 0.15      2.73\n              rank            2.08 0.15      2.75\n\n\n1. Variable 'C(surgery)[1]' failed the non-proportional test: p-value is 0.0452.\n\nTest PH: Schoenfeld Méthode 2\n\nfrom lifelines.statistics import  proportional_hazard_test \nzph = proportional_hazard_test(cph, YX, time_transform='all')\nzph.print_summary()\n\n\n<lifelines.StatisticalResult: proportional_hazard_test>\n null_distribution = chi squared\ndegrees_of_freedom = 1\n         test_name = proportional_hazard_test\n\n---\n                        test_statistic    p  -log2(p)\nC(surgery)[1] identity            5.54 0.02      5.75\n              km                  4.01 0.05      4.47\n              log                 3.69 0.05      4.19\n              rank                3.74 0.05      4.23\nage           identity            1.61 0.20      2.29\n              km                  1.18 0.28      1.86\n              log                 0.61 0.44      1.20\n              rank                1.06 0.30      1.72\nyear          identity            0.80 0.37      1.43\n              km                  2.07 0.15      2.73\n              log                 1.34 0.25      2.02\n              rank                2.08 0.15      2.75\n\nTest PH: intéraction\n\nfrom lifelines.utils import to_episodic_format\nfrom lifelines import CoxTimeVaryingFitter\n\nTransformation de la base YX\n\nlong = to_episodic_format(YX, duration_col='stime', event_col='died')\n\nCréation de la variable d’intéraction\n\nlong['surgery_t'] = long['C(surgery)[1]'] * long['stop']\n\nEstimation\n\nctv = CoxTimeVaryingFitter()\nctv.fit(long,\n        id_col='id',\n        event_col='died',\n        start_col='start',\n        stop_col='stop',)\nctv.print_summary(4)\n\n\n<lifelines.CoxTimeVaryingFitter: fitted with 31938 periods, 103 subjects, 75 events>\n         event col = 'died'\nnumber of subjects = 103\n number of periods = 31938\n  number of events = 75\npartial log-likelihood = -287.3290\n  time fit was run = 2021-04-21 13:32:40 UTC\n\n---\n                 coef  exp(coef)   se(coef)   coef lower 95%   coef upper 95%  exp(coef) lower 95%  exp(coef) upper 95%\nC(surgery)[1] -1.7547     0.1730     0.6743          -3.0764          -0.4331               0.0461               0.6485\nage            0.0289     1.0293     0.0134           0.0025           0.0552               1.0025               1.0568\nyear          -0.1231     0.8842     0.0668          -0.2541           0.0079               0.7756               1.0080\nsurgery_t      0.0022     1.0022     0.0011           0.0001           0.0044               1.0001               1.0044\n\n                    z      p   -log2(p)\nC(surgery)[1] -2.6022 0.0093     6.7542\nage            2.1479 0.0317     4.9785\nyear          -1.8415 0.0656     3.9312\nsurgery_t      2.0239 0.0430     4.5402\n---\nPartial AIC = 582.6581\nlog-likelihood ratio test = 21.5846 on 4 df\n-log2(p) of ll-ratio test = 12.0103\n\n\n\nVariable dynamique binaire"
  },
  {
    "objectID": "Python.html#modèle-à-temps-discret",
    "href": "Python.html#modèle-à-temps-discret",
    "title": "PYTHON",
    "section": "Modèle à temps discret",
    "text": "Modèle à temps discret\n\nAjustement continu\nModèle logistique estimé avec le paquet statsmodel. La fonction to_episodic_format de lifelines permet de mettre en forme la base.\nPour la durée, on utilisera ici la variable mois (regroupement de stime par intervalle de 30 jours).\n\nimport statsmodels.formula.api as smf #type R formule => ce qu'on utilisera#\nimport statsmodels.api as sm #type python#\n\nTransformation de la base en format long\n\ntd = pd.read_csv(\"D:/Marc/SMS/FORMATIONS/2019/analyse durees/a distribuer/transplantation.csv\")\ntd.drop(['id'], axis=1, inplace=True)\ntd['dur'] = td['mois']\ntd = to_episodic_format(td, duration_col='mois', event_col='died')\n\nRecherche de l’ajustement\n\n\ntd['t2'] = td['stop']**2\ntd['t3'] = td['stop']**3\nfit1 = smf.glm(formula=  \"died ~ stop\", data=td, family=sm.families.Binomial()).fit()\nfit2 = smf.glm(formula=  \"died ~ stop + t2\", data=td, family=sm.families.Binomial()).fit()\nfit3 = smf.glm(formula=  \"died ~ stop + t2 + t3\", data=td, family=sm.families.Binomial()).fit()\n\nComparaison des AIC\n\nprint(\"AIC pour ajustement t1\")\nprint(fit1.aic)\nprint(\"AIC pour ajustement durée t1 + t2\")\nprint(fit2.aic)\nprint(\"AIC pour ajustement durée t1 + t2 + t3\")\nprint(fit3.aic)\n\n\nAIC pour ajustement t1\n512.1039235968562\n\nAIC pour ajustement durée t1 + t2\n508.1014573009212\n\nAIC pour ajustement durée t1 + t2 + t3\n506.1882809518765\n\nEstimation du modèle\n\ntdfit = smf.glm(formula=  \"died ~ stop + t2 + t3 + year + age + surgery\", data=td, family=sm.families.Binomial()).fit()\ntdfit.summary()\n\n\n<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                   died   No. Observations:                 1164\nModel:                            GLM   Df Residuals:                     1157\nModel Family:                Binomial   Df Model:                            6\nLink Function:                  logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -240.20\nDate:                Wed, 21 Apr 2021   Deviance:                       480.39\nTime:                        15:44:21   Pearson chi2:                 1.30e+03\nNo. Iterations:                     7                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      6.3097      5.201      1.213      0.225      -3.884      16.503\nstop          -0.2807      0.077     -3.635      0.000      -0.432      -0.129\nt2             0.0096      0.005      2.083      0.037       0.001       0.019\nt3            -0.0001   6.97e-05     -1.493      0.135      -0.000    3.26e-05\nyear          -0.1263      0.072     -1.747      0.081      -0.268       0.015\nage            0.0337      0.014      2.330      0.020       0.005       0.062\nsurgery       -1.0050      0.447     -2.250      0.024      -1.880      -0.130\n==============================================================================\n\"\"\"\n\n\n\nAjustement discret\nCréation des intervalles pour l’exemple (quantile de la durée en mois)\n\ntd['ct4'] = pd.qcut(td['stop'],[0, .25, .5, .75, 1.]) \ntd['ct4'].value_counts(normalize=True)*100\ntd.ct4 = pd.Categorical(td.ct4)\ntd['ct4'] = td.ct4.cat.codes\n\n\n(0.999, 4.0]    27.233677\n(11.0, 23.0]    24.484536\n(4.0, 11.0]     24.398625\n(23.0, 61.0]    23.883162\nName: ct4, dtype: float64\n\nPour chaque individu, on conserve une seule observation par intervalle.\n\ntd2 = td \ntd2['t'] = td2['ct4']\ntd2 = td2.sort_values(['id', 'stop'])\ntd2 =  td2.groupby(['id','ct4']).last()\n\nEstimation\n\ntd2fit = smf.glm(formula=  \"died ~ C(t) +  year + age + surgery\", data=td2, family=sm.families.Binomial()).fit()\ntd2fit.summary()\n\n\n<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:                   died   No. Observations:                  200\nModel:                            GLM   Df Residuals:                      200\nModel Family:                Binomial   Df Model:                           -1\nLink Function:                  logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -112.26\nDate:                Wed, 21 Apr 2021   Deviance:                       224.52\nTime:                        15:54:03   Pearson chi2:                     229.\nNo. Iterations:                     5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          z      P>|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     11.8018      6.617      1.784      0.074      -1.167      24.770\nC(t)[T.1]     -0.9078      0.408     -2.227      0.026      -1.707      -0.109\nC(t)[T.2]     -1.8451      0.587     -3.141      0.002      -2.996      -0.694\nC(t)[T.3]     -0.3224      0.578     -0.557      0.577      -1.456       0.811\nyear          -0.1947      0.093     -2.104      0.035      -0.376      -0.013\nage            0.0468      0.018      2.543      0.011       0.011       0.083\nsurgery       -1.1025      0.503     -2.192      0.028      -2.088      -0.117\n==============================================================================\n\"\"\""
  },
  {
    "objectID": "Python.html#modèle-paramétrique-de-type-aft",
    "href": "Python.html#modèle-paramétrique-de-type-aft",
    "title": "PYTHON",
    "section": "Modèle paramétrique de type AFT",
    "text": "Modèle paramétrique de type AFT\nfrom lifelines import WeibullAFTFitter, LogLogisticAFTFitter\nWeibull\n\naftw = WeibullAFTFitter()\naftw.fit(YX, duration_col='stime', event_col='died')\naftw.print_summary()\n\n\n<lifelines.WeibullAFTFitter: fitted with 103 total observations, 28 right-censored observations>\n             duration col = 'stime'\n                event col = 'died'\n   number of observations = 103\nnumber of events observed = 75\n           log-likelihood = -488.17\n         time fit was run = 2021-04-21 13:55:14 UTC\n\n---\n                        coef  exp(coef)   se(coef)   coef lower 95%   coef upper 95%  exp(coef) lower 95%  exp(coef) upper 95%\nlambda_ C(surgery)[1]   1.97       7.17       0.78             0.44             3.50                 1.56                33.05\n        year            0.16       1.18       0.12            -0.08             0.40                 0.93                 1.49\n        age            -0.06       0.94       0.02            -0.11            -0.01                 0.90                 0.99\n        _intercept     -3.02       0.05       8.73           -20.13            14.09                 0.00             1.31e+06\nrho_    _intercept     -0.59       0.56       0.09            -0.77            -0.41                 0.46                 0.67\n\n                          z      p   -log2(p)\nlambda_ C(surgery)[1]  2.53   0.01       6.45\n        year           1.33   0.18       2.44\n        age           -2.49   0.01       6.28\n        _intercept    -0.35   0.73       0.46\nrho_    _intercept    -6.33 <0.005      31.93\n---\nConcordance = 0.65\nAIC = 986.34\nlog-likelihood ratio test = 18.87 on 3 df\n-log2(p) of ll-ratio test = 11.75\n\nLoglogistique\n\naftl = LogLogisticAFTFitter()\naftl.fit(YX, duration_col='stime', event_col='died')\naftl.print_summary()\n\n\n<lifelines.LogLogisticAFTFitter: fitted with 103 total observations, 28 right-censored observations>\n             duration col = 'stime'\n                event col = 'died'\n   number of observations = 103\nnumber of events observed = 75\n           log-likelihood = -482.58\n         time fit was run = 2021-04-21 13:55:58 UTC\n\n---\n                       coef  exp(coef)   se(coef)   coef lower 95%   coef upper 95%  exp(coef) lower 95%  exp(coef) upper 95%\nalpha_ C(surgery)[1]   2.27       9.72       0.69             0.92             3.63                 2.51                37.68\n       year            0.24       1.27       0.12             0.01             0.47                 1.01                 1.60\n       age            -0.04       0.96       0.02            -0.08            -0.00                 0.92                 1.00\n       _intercept    -10.41       0.00       8.34           -26.76             5.94                 0.00               381.76\nbeta_  _intercept     -0.18       0.83       0.10            -0.37             0.01                 0.69                 1.01\n\n                         z      p   -log2(p)\nalpha_ C(surgery)[1]  3.29 <0.005       9.96\n       year           2.05   0.04       4.65\n       age           -2.01   0.04       4.49\n       _intercept    -1.25   0.21       2.24\nbeta_  _intercept    -1.86   0.06       4.00\n---\nConcordance = 0.66\nAIC = 975.16\nlog-likelihood ratio test = 21.69 on 3 df\n-log2(p) of ll-ratio test = 13.69"
  },
  {
    "objectID": "Python.html#kaplan-meier",
    "href": "Python.html#kaplan-meier",
    "title": "PYTHON",
    "section": "Kaplan-Meier",
    "text": "Kaplan-Meier\n\nkm = sm.SurvfuncRight(trans[\"stime\"], trans[\"died\"])\nkm.summary()\n\n\n      Surv prob  Surv prob SE  num at risk  num events\nTime                                                  \n1      0.990291      0.009661          103         1.0\n2      0.961165      0.019037          102         3.0\n3      0.932039      0.024799           99         3.0\n5      0.912621      0.027825           96         2.0\n6      0.893204      0.030432           94         2.0\n...         ...           ...          ...         ...\n852    0.250655      0.048731           14         1.0\n979    0.227868      0.049341           11         1.0\n995    0.205081      0.049390           10         1.0\n1032   0.182295      0.048877            9         1.0\n1386   0.151912      0.049277            6         1.0\n\nLes test du log-rank sont disponibles avec la fonction survdiff (nom idem R). Au niveau graphique, la programmation semble un peu lourde et mériterait d’être simplifiée (donc non traitée).\nComparaison de S(t) à partir des tests du log-rank\n Résultat: (statistique de test, p-value)\nTest non pondéré\n\nsm.duration.survdiff(trans.stime, trans.died, trans.surgery)\n\n# (6.590012323234387, 0.010255246157888975)\n\nBreslow\n\nsm.duration.survdiff(trans.stime, trans.died, trans.surgery, weight_type='gb')\n\n# (8.989753779902495, 0.0027149757927903417)\n\nTarone-Ware\n\nsm.duration.survdiff(trans.stime, trans.died, trans.surgery, weight_type='tw')\n\n# (8.462352726451392, 0.0036257256194570653)"
  },
  {
    "objectID": "Python.html#modèle-de-cox",
    "href": "Python.html#modèle-de-cox",
    "title": "PYTHON",
    "section": "Modèle de Cox",
    "text": "Modèle de Cox\n\nmod = smf.phreg(\"stime ~  year + age + surgery \",trans, status='died', ties=\"efron\")\nrslt = mod.fit()\nprint(rslt.summary())\n\n\n                       Results: PHReg\n=============================================================\nModel:                    PH Reg       Sample size:       103\nDependent variable:       stime        Num. events:       75 \nTies:                     Efron                              \n-------------------------------------------------------------\n         log HR log HR SE   HR      t    P>|t|  [0.025 0.975]\n-------------------------------------------------------------\nyear    -0.1196    0.0673 0.8872 -1.7765 0.0757 0.7775 1.0124\nage      0.0296    0.0135 1.0300  2.1872 0.0287 1.0031 1.0577\nsurgery -0.9873    0.4363 0.3726 -2.2632 0.0236 0.1584 0.8761\n=============================================================\nConfidence intervals are for the hazard ratios"
  },
  {
    "objectID": "R.html",
    "href": "R.html",
    "title": "R",
    "section": "",
    "text": "Programme de cette section: Lien"
  },
  {
    "objectID": "R.html#listes",
    "href": "R.html#listes",
    "title": "R",
    "section": "Listes",
    "text": "Listes\n\n\n\n\n\n\n\nAnalyse\nPackages - fonctions\n\n\n\n\nNon paramétrique\n\ndiscsurv\n\nlifetable\ncontToDisc\n\nsurvival\n\nsurvfit\nsurvdif\n\nsurvRM2\n\nrmst2\n\n\n\n\nModèles à risques proportionnel\n\nsurvival\n\ncoxph\ncox.zph (v3) cox.zphold (récupération v2)\nsurvsplit\n\nbase et tydir\n\nuncount\nglm\n\n\n\n\nModèles paramétriques (ph ou aft)\n\nsurvival\n\nsurvreg\n\nflexsurv\n\nsurvreg\n\n\n\n\nRisques concurents\n\ncmprsk\n\ncuminc\n\nnnet\n\nmultinom\n\n\n\n\nAutres (graphiques - mise en forme)\n\nsurvminer\njtools\nstargazer - gtsummary"
  },
  {
    "objectID": "R.html#installation",
    "href": "R.html#installation",
    "title": "R",
    "section": "Installation",
    "text": "Installation\nLes dernières versions de certains packages peuvent être installées via Github (ex: survminer). Pour les récupérer, passer par le package devtools.\n\n#install.packages(\"survival\")\n#install.packages(\"survminer\")\n#install.packages(\"flexsurv\")\n#install.packages(\"survRM2\")\n#install.packages(\"tidyr\")\n#install.packages(\"gtools\")\n#install.packages(\"jtools\")\n#install.packages(\"RecordLinkage\")\n#install.packages(\"gtsummary\")\n#install.packages(\"cmprsk\")\n#install.packages(\"gtsummary\")\n#install.package(stargazer)\n#install.packages(\"muhaz\")\nlibrary(survival)\nlibrary(survminer)\nlibrary(flexsurv)\nlibrary(survRM2)\nlibrary(gtools)\nlibrary(tidyr)\nlibrary(jtools)\n#library(RecordLinkage)\n#library(gtsummary)\nlibrary(cmprsk)\nlibrary(discSurv)\nlibrary(muhaz)\nlibrary(stargazer)"
  },
  {
    "objectID": "R.html#survival-v2-versus-v3",
    "href": "R.html#survival-v2-versus-v3",
    "title": "R",
    "section": "Survival v2 versus v3",
    "text": "Survival v2 versus v3\n Se reporter à la partie sur les risques proportionnels.\n::: {.cell filename=’ installation survival 2.44-1 (30 mars 2019)’}\nrequire(remotes)\ninstall_version(\"survival\", version = \"2.44-1\", repos = \"http://cran.us.r-project.org\")\n:::"
  },
  {
    "objectID": "R.html#méthode-actuarielle",
    "href": "R.html#méthode-actuarielle",
    "title": "R",
    "section": "Méthode actuarielle",
    "text": "Méthode actuarielle\nLa fonction disponible du paquet discsurv, lifetable, a des fonctionalités plutôt limitées. Si on peut depuis une MAJ récente définir des intervalles de durée, il n’y a toujours pas d’estimateurs les différents quantiles de la courbe de survie.\nPire, la programmation est rendue un peu compliquée pour pas grand chose. On doit s’assurer que la base est bien en format data.frame.\nJe donne les codes pour info, sans plus de commentaires.\n\ntrans = as.data.frame(trans)\n\nFonction lifeTable\nIntervalle par defaut \\(dt=1\\)\n\nlt = lifeTable(dataShort=trans, timeColumn=\"stime\", eventColumn = \"died\")\n\nplot(lt, x = 1:dim(lt$Output)[1], y = lt$Output$S, xlab = \"Intervalles t = journalier\", ylab=\"S(t)\")\n\n\n\n\nS(t) méthode actuarielle avec discSurv (1)\n\n\n\n\nIntervalle \\(dt=30\\)\n\n# On définit le vecteur des intervalles (il n'y avait pas plus simple????)\ndt <- 1:ceiling(max(trans$stime)/30)*30\n\n# Base dis avec une nouvelle variable de durée => timeDisc \ndis <- contToDisc(dataShort=trans, timeColumn=\"stime\", intervalLimits = dt )\n\nlt <- lifeTable(dataShort=dis, timeColumn=\"timeDisc\", eventColumn = \"died\")\n\nplot(lt, x = 1:dim(lt$Output)[1], y = lt$Output$S, xlab = \"Intervalles dt = 30 jours\", ylab=\"S(t)\")\n\n\n\n\nS(t) méthode actuarielle avec discSurv (2)\n\n\n\n\nSur les abcisses, ce sont les valeurs des intervalles qui sont reportés: 10=300 jours. Ce n’est vraiment pas terrible. Pour ce type d’estimateurs, il est préférable d’utiliser Sas ou Stata."
  },
  {
    "objectID": "R.html#méthode-kaplan-meier",
    "href": "R.html#méthode-kaplan-meier",
    "title": "R",
    "section": "Méthode Kaplan-Meier",
    "text": "Méthode Kaplan-Meier\nLe package survival est le principal outil d’analyse des durée. Le package survminer permet d’améliorer la présentation des graphiques. \nEstimation des fonctions de survie\n Fonction survfit\nSyntaxe\n\nfit <- survfit(Surv(time, status) ~ x, data = base)\n\nOn peut renseigner directement les variables permettant de calculer la durée et non la variable de durée elle-même. Cette méthode est utilisée lorsqu’on introduit une variable dynamique dans un modèle semi-paramétrique (coxph).\n\nfit <- survfit(Surv(variable_start, variable_end, status) ~ x, data = nom_base)\n\nSans comparaison de groupes:\n\nfit <- survfit(Surv(stime, died) ~ 1, data = trans)\nfit\n\nCall: survfit(formula = Surv(stime, died) ~ 1, data = trans)\n\n       n events median 0.95LCL 0.95UCL\n[1,] 103     75    100      72     263\n\nsummary(fit)\n\nCall: survfit(formula = Surv(stime, died) ~ 1, data = trans)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1    103       1    0.990 0.00966       0.9715        1.000\n    2    102       3    0.961 0.01904       0.9246        0.999\n    3     99       3    0.932 0.02480       0.8847        0.982\n    5     96       2    0.913 0.02782       0.8597        0.969\n    6     94       2    0.893 0.03043       0.8355        0.955\n    8     92       1    0.883 0.03161       0.8237        0.948\n    9     91       1    0.874 0.03272       0.8119        0.940\n   12     89       1    0.864 0.03379       0.8002        0.933\n   16     88       3    0.835 0.03667       0.7656        0.910\n   17     85       1    0.825 0.03753       0.7543        0.902\n   18     84       1    0.815 0.03835       0.7431        0.894\n   21     83       2    0.795 0.03986       0.7208        0.877\n   28     81       1    0.785 0.04056       0.7098        0.869\n   30     80       1    0.776 0.04122       0.6989        0.861\n   32     78       1    0.766 0.04188       0.6878        0.852\n   35     77       1    0.756 0.04250       0.6769        0.844\n   36     76       1    0.746 0.04308       0.6659        0.835\n   37     75       1    0.736 0.04364       0.6551        0.827\n   39     74       1    0.726 0.04417       0.6443        0.818\n   40     72       2    0.706 0.04519       0.6225        0.800\n   43     70       1    0.696 0.04565       0.6117        0.791\n   45     69       1    0.686 0.04609       0.6009        0.782\n   50     68       1    0.675 0.04650       0.5902        0.773\n   51     67       1    0.665 0.04689       0.5796        0.764\n   53     66       1    0.655 0.04725       0.5690        0.755\n   58     65       1    0.645 0.04759       0.5584        0.746\n   61     64       1    0.635 0.04790       0.5479        0.736\n   66     63       1    0.625 0.04819       0.5374        0.727\n   68     62       2    0.605 0.04870       0.5166        0.708\n   69     60       1    0.595 0.04892       0.5063        0.699\n   72     59       2    0.575 0.04929       0.4857        0.680\n   77     57       1    0.565 0.04945       0.4755        0.670\n   78     56       1    0.554 0.04958       0.4654        0.661\n   80     55       1    0.544 0.04970       0.4552        0.651\n   81     54       1    0.534 0.04979       0.4451        0.641\n   85     53       1    0.524 0.04986       0.4351        0.632\n   90     52       1    0.514 0.04991       0.4251        0.622\n   96     51       1    0.504 0.04994       0.4151        0.612\n  100     50       1    0.494 0.04995       0.4052        0.602\n  102     49       1    0.484 0.04993       0.3953        0.592\n  110     47       1    0.474 0.04992       0.3852        0.582\n  149     45       1    0.463 0.04991       0.3749        0.572\n  153     44       1    0.453 0.04987       0.3647        0.562\n  165     43       1    0.442 0.04981       0.3545        0.551\n  186     41       1    0.431 0.04975       0.3440        0.541\n  188     40       1    0.420 0.04966       0.3336        0.530\n  207     39       1    0.410 0.04954       0.3233        0.519\n  219     38       1    0.399 0.04940       0.3130        0.509\n  263     37       1    0.388 0.04923       0.3027        0.498\n  285     35       2    0.366 0.04885       0.2817        0.475\n  308     33       1    0.355 0.04861       0.2713        0.464\n  334     32       1    0.344 0.04834       0.2610        0.453\n  340     31       1    0.333 0.04804       0.2507        0.442\n  342     29       1    0.321 0.04773       0.2401        0.430\n  583     21       1    0.306 0.04785       0.2252        0.416\n  675     17       1    0.288 0.04830       0.2073        0.400\n  733     16       1    0.270 0.04852       0.1898        0.384\n  852     14       1    0.251 0.04873       0.1712        0.367\n  979     11       1    0.228 0.04934       0.1491        0.348\n  995     10       1    0.205 0.04939       0.1279        0.329\n 1032      9       1    0.182 0.04888       0.1078        0.308\n 1386      6       1    0.152 0.04928       0.0804        0.287\n\nplot(fit)\n\n\n\n\n\n\n\n\nLe premier output fit permet d’obtenir la durée médiane, ici égale à 100 (\\(S(100)=0.494\\)). Le second avec la fonction summary permet d’obtenir une table des estimateurs. La fonction de survie peut être tracée avec la fonction plot (en pointillés les intervalles de confiance).\n On peut obtenir des graphes de meilleur qualité avec la librairie survminer.\nAvec la fonction ggsurvplot\n\nggsurvplot(fit, conf.int = TRUE)\n\n\n\n\n\n\n\n\nOn peut ajouter la population encore soumise au risque à plusieurs points d’observation\n\nggsurvplot(fit, conf.int = TRUE, risk.table = TRUE)\n\n\n\n\n\n\n\n\nComparaison des fonctions de survie\nOn va comparer les deuxfonctions de survie pour la variable surgery, celle pour les personnes non opérées et celle pour les personnes opérées.\n\nfit <- survfit(Surv(stime, died) ~ surgery, data = trans)\nfit\n\nCall: survfit(formula = Surv(stime, died) ~ surgery, data = trans)\n\n           n events median 0.95LCL 0.95UCL\nsurgery=0 91     69     78      61     153\nsurgery=1 12      6    979     583      NA\n\nggsurvplot(fit, conf.int = TRUE, risk.table = TRUE)\n\n\n\n\n\n\n\n\nTests du log-rank\nOn utilise la fonction survdiff, avec comme variante le test de Peto-Peto (rho=1).\nLa syntaxe est quasiment identique à la fonction survdiff.\n\nsurvdiff(Surv(stime, died) ~ surgery, rho=1, data = trans)\n\nCall:\nsurvdiff(formula = Surv(stime, died) ~ surgery, data = trans, \n    rho = 1)\n\n           N Observed Expected (O-E)^2/E (O-E)^2/V\nsurgery=0 91    45.28    39.12     0.968      8.65\nsurgery=1 12     2.03     8.18     4.630      8.65\n\n Chisq= 8.7  on 1 degrees of freedom, p= 0.003 \n\n\nIci la variable est binaire. Si on veux tester deux à deux les niveaux d’une variable catégorielle à plus de deux modalités, on utilise la fonction pairwise_survdiff de survminer (syntaxe identique que survdiff).\nComparaison des RMST\nLa fonction rmst2 du package survRM2 permet de comparer les RMST entre 2 groupes (et pas plus). La strate pour les comparaisons doit être renommée arm. La fonction, issue d’une commande de Stata, n’est pas très souple.\n\ntrans$arm=trans$surgery\na=rmst2(trans$stime, trans$died, trans$arm, tau=NULL)\nprint(a)\n\n\nThe truncation time, tau, was not specified. Thus, the default tau  1407  is used. \n\nRestricted Mean Survival Time (RMST) by arm \n                Est.      se lower .95 upper .95\nRMST (arm=1) 884.576 151.979   586.702  1182.450\nRMST (arm=0) 379.148  58.606   264.283   494.012\n\n\nRestricted Mean Time Lost (RMTL) by arm \n                 Est.      se lower .95 upper .95\nRMTL (arm=1)  522.424 151.979   224.550   820.298\nRMTL (arm=0) 1027.852  58.606   912.988  1142.717\n\n\nBetween-group contrast \n                        Est. lower .95 upper .95     p\nRMST (arm=1)-(arm=0) 505.428   186.175   824.682 0.002\nRMST (arm=1)/(arm=0)   2.333     1.483     3.670 0.000\nRMTL (arm=1)/(arm=0)   0.508     0.284     0.909 0.022\n\nplot(a)"
  },
  {
    "objectID": "R.html#estimation-du-modèle",
    "href": "R.html#estimation-du-modèle",
    "title": "R",
    "section": "Estimation du modèle",
    "text": "Estimation du modèle\nPar défaut, R utilise la correction d’Efron pour les évènements simultanés. Il est préférable de ne pas la modifier.\nSyntaxe:\ncoxph(Surv(time, status) ~ x1 + x2 + ....., data=base, ties=\"nom_correction\"))\n\ncoxfit = coxph(formula = Surv(stime, died) ~ year + age + surgery, data = trans)\nsummary(coxfit)\n\nCall:\ncoxph(formula = Surv(stime, died) ~ year + age + surgery, data = trans)\n\n  n= 103, number of events= 75 \n\n            coef exp(coef) se(coef)      z Pr(>|z|)  \nyear    -0.11963   0.88725  0.06734 -1.776   0.0757 .\nage      0.02958   1.03002  0.01352  2.187   0.0287 *\nsurgery -0.98732   0.37257  0.43626 -2.263   0.0236 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        exp(coef) exp(-coef) lower .95 upper .95\nyear       0.8872     1.1271    0.7775    1.0124\nage        1.0300     0.9709    1.0031    1.0577\nsurgery    0.3726     2.6840    0.1584    0.8761\n\nConcordance= 0.653  (se = 0.032 )\nLikelihood ratio test= 17.63  on 3 df,   p=5e-04\nWald test            = 15.76  on 3 df,   p=0.001\nScore (logrank) test = 16.71  on 3 df,   p=8e-04\n\n\nLa table des résultats reporte le logarithme des Risques Ratios (coef) ainsi que les RR (exp(coef)). Il est intéressant de regarder la valeur de concordance (Harrel’s) qui donne des indications sur la qualité de l’ajustement (proche de l’AUC/ROC).\nOn peut représenter sous forme graphique les résultats avec la fonction ggforest de survminer\n\nggforest(coxfit)\n\nWarning in .get_data(model, data = data): The `data` argument is not provided.\nData will be extracted from model fit."
  },
  {
    "objectID": "R.html#hypothèse-ph",
    "href": "R.html#hypothèse-ph",
    "title": "R",
    "section": "Hypothèse PH",
    "text": "Hypothèse PH\n\nTest Grambsch-Therneau\nRésidus de Schoenfeld\nOn utilise la fonction cox.zph pour effectuer ce test.\nLe test peut utiliser plusieurs fonctions de la durée. Par défaut la fonction utilise \\(1-KM\\), soit le complémentaire de l’estimateur de Kaplan-Meier (option transform=\"km\").\n\n\n\n\n\n\nMise à jour à venir: test simplifié et test exact\n\n\n\nAttention aux résultats entre le test de la v2 et de la v3 de survival. Les deux sont présentés, avec un test reposant sur une régression linéaire entre les résidus de Schoenfeld et une fonction de la durée (ici f(t)=t): OLS dans le cas du test simplifié et GLS dans le cas du test exact. Le passage d’un test à l’autre ne me semble pas neutre du point de vue des données utilisées, en particulier celles qui mesurent la durée. Pour illustrer le problème les résultats du test de Grambsch-Therneau sont donnés pour les deux versions.\nPour l’instant je préconise de rester à la version antérieure du test, en raison de son lien très fort avec une régression linéaire entre les résidus standardisés et une fonction de la durée, mais également en termes de reproductibilité, les autres applications utilisant cette version originale. Je donne la marche à suivre pour exécuter ce test.\nDans une analyse, je conseille de préciser dans une analyse la version du test qui a été exécutée. Le test simplifié est actuellement la seule solution dans Stata, Sas ou Python.\n\n\nRésultat du test avec la v3 de survival\nAvec transform=\"km\"\n\ncox.zph(coxfit)\n\n        chisq df     p\nyear    3.309  1 0.069\nage     0.922  1 0.337\nsurgery 5.494  1 0.019\nGLOBAL  8.581  3 0.035\n\n\nAvec transform=\"identity\" (\\(f(t)=t\\)) [remarque: solution de Stata par défaut].\n\ncox.zph(coxfit, transform=\"identity\")\n\n        chisq df     p\nyear     4.54  1 0.033\nage      1.71  1 0.191\nsurgery  4.92  1 0.027\nGLOBAL   9.47  3 0.024\n\n\nRemarque: avec la v3, quelques options ont été ajoutées tel que terms qui permet pour une variable catégorielle à plus de deux modalités de choisir entre un sous test multiple sur la variable (k modalités => k-1 degré de liberté) et une série de tests à 1 degré de liberté sur chaque modalité (k-1 tests). De mon point de vue préférer la seconde solution avec terms=FALSE.\nRésultat du test avec la v2 de survival\n\n\n\n\n\n\nWarning\n\n\n\n J’ai récupéré la fonction de la version précédente de survival (identique Sas, Stata, Python), renommée cox.zphold. Elle se trouve à cette adresse. :[https://github.com/mthevenin/analyse_duree/tree/main/cox.zphold].\n\nUne fois enregistrée avec comme nom de fichier cox.zphold.R, la charger dans le programme d’analyse avec la fonction source()\nAprès avoir estimé le modèle de Cox, exécuter la fonction cox.zphold()\n\n\n\n\nsource(\"D:/D/Marc/SMS/FORMATIONS/2022/Durée2/a distribuer/cox.zphold.R\")\n\ncoxfit = coxph(formula = Surv(stime, died) ~ year + age + surgery,   data = trans)\n\n\ncox.zphold(coxfit)\n\n          rho chisq      p\nyear    0.159  1.96 0.1620\nage     0.109  1.15 0.2845\nsurgery 0.251  3.96 0.0465\nGLOBAL     NA  7.99 0.0462\n\ncox.zphold(coxfit, transform=\"identity\")\n\n          rho chisq      p\nyear    0.102 0.797 0.3720\nage     0.129 1.612 0.2043\nsurgery 0.297 5.539 0.0186\nGLOBAL     NA 8.756 0.0327\n\n\n\n\n\n\n\n\nRégression linéaire ordinaire sur les résidus de Schoenfeld\n\n\n\nPour information, avec une OLS\n\nresid= resid(coxfit, type=\"scaledsch\")\nvarnames <- names(coxfit$coefficients)\ncoln = c(varnames)\ncolnames(resid) = c(coln)\n\ntimes    = as.numeric(dimnames(resid)[[1]])\n\nresid = data.frame(resid)\nresid = cbind(resid, t=times)\n\nyear    = summary(lm(year~t, data=resid))\nage     = summary(lm(age~t, data=resid))\nsurgery = summary(lm(surgery~t, data=resid))\n\n#####################\n# p-values de l'OLS #\n#####################\n\npaste(\"p-value pour year:\",    year$coefficients[2,4])\n\n[1] \"p-value pour year: 0.38565336851861\"\n\npaste(\"p-value pour age:\",     age$coefficients[2,4])\n\n[1] \"p-value pour age: 0.268640363261224\"\n\npaste(\"p-value pour surgery:\", surgery$coefficients[2,4])\n\n[1] \"p-value pour surgery: 0.00975820981508909\"\n\n\nOn est trés proche des résulats du test simplifié effectué avec la v2 de survival. Et c’est tout à fait normal.\n\n\n\n\nIntroduction d’une intéraction\nLorsque la covariable n’est pas continue, elle doit être transformée en indicatrice. Vérifier que les résultats du modèle sont bien identiques avec le modèle estimé précédemment (ne pas oublier d’omettre le niveau en référence).\nIci la variable surgery est déjà sous forme d’indicatrice (0,1).\nLa variable d’intéraction est tt(nom-variable), la fonction de la durée (ici forme linéaire simple) est indiquée en option de la fonction: tt = function(x, t, ...) x*t.\n\ncoxfit2 = coxph(formula = Surv(stime, died) ~ year + age + surgery + tt(surgery), data = trans, tt = function(x, t, ...) x*t)\nsummary(coxfit2)\n\nCall:\ncoxph(formula = Surv(stime, died) ~ year + age + surgery + tt(surgery), \n    data = trans, tt = function(x, t, ...) x * t)\n\n  n= 103, number of events= 75 \n\n                 coef exp(coef)  se(coef)      z Pr(>|z|)   \nyear        -0.123074  0.884198  0.066835 -1.841  0.06555 . \nage          0.028888  1.029310  0.013449  2.148  0.03172 * \nsurgery     -1.754738  0.172953  0.674391 -2.602  0.00927 **\ntt(surgery)  0.002231  1.002234  0.001102  2.024  0.04299 * \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n            exp(coef) exp(-coef) lower .95 upper .95\nyear           0.8842     1.1310   0.77564    1.0080\nage            1.0293     0.9715   1.00253    1.0568\nsurgery        0.1730     5.7819   0.04612    0.6486\ntt(surgery)    1.0022     0.9978   1.00007    1.0044\n\nConcordance= 0.656  (se = 0.032 )\nLikelihood ratio test= 21.58  on 4 df,   p=2e-04\nWald test            = 16.99  on 4 df,   p=0.002\nScore (logrank) test = 19  on 4 df,   p=8e-04\n\n\nRappel: le paramètre estimé pour tt(surgery) ne reporte pas un rapport de risques, mais un rapport de de deux rapports de risques (c’est bien une double différence sur l’échelle d’estimation - log -)."
  },
  {
    "objectID": "R.html#variable-dynamique-binaire",
    "href": "R.html#variable-dynamique-binaire",
    "title": "R",
    "section": "Variable dynamique (binaire)",
    "text": "Variable dynamique (binaire)\nLa dimension dynamique est le fait d’avoir été opéré pour une greffe du coeur.\n\n\nEtape 1: créer un vecteur donnant les durées aux temps d’évènement.\nEtape 2: appliquer ce vecteurs de points de coupure à la fonction survsplit.\nEtape 3: modifier la variable transplant (ou créer une nouvelle) à l’aide de la variable wait qui prend la valeur 1 à partir du jour de la greffe, 0 avant.\n\nEtape 1 Création de l’objet cut (vecteur)\n\ncut= unique(trans$stime[trans$died == 1])\n\nEtape 2\n\ntvc = survSplit(data = trans, cut = cut, end = \"stime\", start = \"stime0\", event = \"died\")\n\nRemarque: pour estimer le modèle de Cox de départ avec cette base longue.\n\ncoxph(formula = Surv(stime0, stime, died) ~ year + age + surgery, data = tvc)\n\nCall:\ncoxph(formula = Surv(stime0, stime, died) ~ year + age + surgery, \n    data = tvc)\n\n            coef exp(coef) se(coef)      z      p\nyear    -0.11963   0.88725  0.06734 -1.776 0.0757\nage      0.02958   1.03002  0.01352  2.187 0.0287\nsurgery -0.98732   0.37257  0.43626 -2.263 0.0236\n\nLikelihood ratio test=17.63  on 3 df, p=0.0005243\nn= 3573, number of events= 75 \n\n\nEtape 3\n\ntvc$tvc=ifelse(tvc$transplant==1 & tvc$wait<=tvc$stime,1,0)\n\nEstimation du modèle\nEn format long, on doit préciser dans la formule l’intervalle de durée avec les variables stime0 (début) et stime(fin)\n\ntvcfit = coxph(formula = Surv(stime0, stime, died) ~ year + age + surgery + tvc, data = tvc)\nsummary(tvcfit)\n\nCall:\ncoxph(formula = Surv(stime0, stime, died) ~ year + age + surgery + \n    tvc, data = tvc)\n\n  n= 3573, number of events= 75 \n\n            coef exp(coef) se(coef)      z Pr(>|z|)  \nyear    -0.12032   0.88664  0.06734 -1.787   0.0740 .\nage      0.03044   1.03091  0.01390  2.190   0.0285 *\nsurgery -0.98289   0.37423  0.43655 -2.251   0.0244 *\ntvc     -0.08221   0.92108  0.30484 -0.270   0.7874  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n        exp(coef) exp(-coef) lower .95 upper .95\nyear       0.8866      1.128    0.7770    1.0117\nage        1.0309      0.970    1.0032    1.0594\nsurgery    0.3742      2.672    0.1591    0.8805\ntvc        0.9211      1.086    0.5068    1.6741\n\nConcordance= 0.659  (se = 0.032 )\nLikelihood ratio test= 17.7  on 4 df,   p=0.001\nWald test            = 15.79  on 4 df,   p=0.003\nScore (logrank) test = 16.74  on 4 df,   p=0.002\n\n\n\nggforest(tvcfit)\n\nWarning in .get_data(model, data = data): The `data` argument is not provided.\nData will be extracted from model fit."
  },
  {
    "objectID": "R.html#ft-quantitative",
    "href": "R.html#ft-quantitative",
    "title": "R",
    "section": "\\(f(t)\\) quantitative",
    "text": "\\(f(t)\\) quantitative\nAvec un effet quadratique d’ordre 2.\n\ndt$t2=dt$t^2\ndtfit = glm(died ~ t + t2 + year + age + surgery, data=dt, family=\"binomial\")\nsumm(dtfit)\n\nMODEL INFO:\nObservations: 1127\nDependent Variable: died\nType: Generalized linear model\n  Family: binomial \n  Link function: logit \n\nMODEL FIT:\nχ²(5) = 84.70, p = 0.00\nPseudo-R² (Cragg-Uhler) = 0.19\nPseudo-R² (McFadden) = 0.15\nAIC = 478.67, BIC = 508.83 \n\nStandard errors: MLE\n------------------------------------------------\n                     Est.   S.E.   z val.      p\n----------------- ------- ------ -------- ------\n(Intercept)          7.74   5.22     1.48   0.14\nt                   -0.20   0.04    -5.63   0.00\nt2                   0.00   0.00     3.98   0.00\nyear                -0.15   0.07    -2.04   0.04\nage                  0.03   0.01     2.35   0.02\nsurgery             -1.00   0.45    -2.24   0.02\n------------------------------------------------\n\n\n*Remarque sur la présentation de l’output: la fonction summ est intégrée au package jtools. Elle ne fonctionne qu’avec le package Recordlinkage qui doit être installé et chargé."
  },
  {
    "objectID": "R.html#ft-discrète",
    "href": "R.html#ft-discrète",
    "title": "R",
    "section": "\\(f(t)\\) discrète ",
    "text": "\\(f(t)\\) discrète \nOn va créer une variable de type dicrète regroupant la variable t sur ses quartiles (pour l’exemple seulement, tous types de regroupement est envisageable).\nOn va utiliser la fonction quantcut du package gtools.\n\ndt$ct4 <- quantcut(dt$t)\ntable(dt$ct4) \n\n\n  [1,4]  (4,11] (11,23] (23,60] \n    299     275     282     271 \n\n\nOn va générer un compteur et un total d’observations sur la strate regroupant id et ct4.\n\ndt$n = ave(dt$x,dt$id, dt$ct4, FUN=cumsum)\ndt$N = ave(dt$x,dt$id, dt$ct4, FUN=sum)\n\nOn conserve la dernière observation dans la strate.\n\ndt2 = subset(dt, n==N)\n\nEstimation du modèle\n\nfit = glm(died ~ ct4 + year + age + surgery, data=dt2, family=binomial)\nsumm(fit)\n\nMODEL INFO:\nObservations: 197\nDependent Variable: died\nType: Generalized linear model\n  Family: binomial \n  Link function: logit \n\nMODEL FIT:\nχ²(6) = 39.30, p = 0.00\nPseudo-R² (Cragg-Uhler) = 0.25\nPseudo-R² (McFadden) = 0.15\nAIC = 236.48, BIC = 259.46 \n\nStandard errors: MLE\n------------------------------------------------\n                     Est.   S.E.   z val.      p\n----------------- ------- ------ -------- ------\n(Intercept)         12.45   6.65     1.87   0.06\nct4(4,11]           -1.03   0.42    -2.47   0.01\nct4(11,23]          -1.62   0.54    -2.96   0.00\nct4(23,60]          -0.48   0.60    -0.80   0.42\nyear                -0.20   0.09    -2.18   0.03\nage                  0.05   0.02     2.53   0.01\nsurgery             -1.11   0.50    -2.21   0.03\n------------------------------------------------"
  },
  {
    "objectID": "R.html#modèle-de-weibull",
    "href": "R.html#modèle-de-weibull",
    "title": "R",
    "section": "Modèle de Weibull",
    "text": "Modèle de Weibull\nDe type AFT\n\nweibull = survreg(formula = Surv(stime, died) ~ year + age + surgery, data = trans, dist=\"weibull\")\nsummary(weibull)\n\n\nCall:\nsurvreg(formula = Surv(stime, died) ~ year + age + surgery, data = trans, \n    dist = \"weibull\")\n              Value Std. Error     z       p\n(Intercept) -3.0220     8.7284 -0.35   0.729\nyear         0.1620     0.1218  1.33   0.184\nage         -0.0615     0.0247 -2.49   0.013\nsurgery      1.9703     0.7794  2.53   0.011\nLog(scale)   0.5868     0.0927  6.33 2.5e-10\n\nScale= 1.8 \n\nWeibull distribution\nLoglik(model)= -488.2   Loglik(intercept only)= -497.6\n    Chisq= 18.87 on 3 degrees of freedom, p= 0.00029 \nNumber of Newton-Raphson Iterations: 5 \nn= 103 \n\n\nDe type PH\n La paramétrisation PH n’est pas possible avec la fonction survreg.\nIl faut utiliser le package flexsurv, qui permet également d’estimer les modèles paramétriques disponibles avec survival.\n\nlibrary(flexsurv)\n\nPour estimer le modèle de Weibull de type PH, on utilise en option dist=\"weibullPH\".\n\nflexsurvreg(formula = Surv(stime, died) ~ year + age + surgery, data = trans, dist=\"weibullPH\")\n\nCall:\nflexsurvreg(formula = Surv(stime, died) ~ year + age + surgery, \n    data = trans, dist = \"weibullPH\")\n\nEstimates: \n         data mean  est        L95%       U95%       se         exp(est) \nshape           NA   5.56e-01   4.64e-01   6.67e-01   5.16e-02         NA\nscale           NA   5.37e+00   4.27e-04   6.75e+04   2.59e+01         NA\nyear      7.06e+01  -9.01e-02  -2.20e-01   3.97e-02   6.62e-02   9.14e-01\nage       4.46e+01   3.42e-02   7.13e-03   6.13e-02   1.38e-02   1.03e+00\nsurgery   1.17e-01  -1.10e+00  -1.95e+00  -2.45e-01   4.34e-01   3.34e-01\n         L95%       U95%     \nshape           NA         NA\nscale           NA         NA\nyear      8.03e-01   1.04e+00\nage       1.01e+00   1.06e+00\nsurgery   1.43e-01   7.83e-01\n\nN = 103,  Events: 75,  Censored: 28\nTotal time at risk: 31938\nLog-likelihood = -488.1683, df = 5\nAIC = 986.3366"
  },
  {
    "objectID": "R.html#incidences-cumulées",
    "href": "R.html#incidences-cumulées",
    "title": "R",
    "section": "Incidences cumulées",
    "text": "Incidences cumulées\nOn utilise la fonction cuminc du package cmprsk.\nPas de comparaison\n\nic = cuminc(compet$stime, compet$compet)\nic \n\nEstimates and Variances:\n$est\n          500      1000      1500\n1 1 0.5067598 0.5808345 0.6340038\n1 2 0.1720161 0.2140841 0.2140841\n\n$var\n            500        1000        1500\n1 1 0.002619449 0.003131847 0.003676516\n1 2 0.001473283 0.002203770 0.002203770\n\nplot(ic)\n\n\n\n\n\n\n\n\nAvec survminer\n\nggcompetingrisks(fit = ic)\n\n\n\n\n\n\n\n\nComparaison\nLe test de Gray est automatiquement exécuté.\n\nic = cuminc(compet$stime, compet$compet, group=compet$surgery, rho=1)\nic \n\nTests:\n      stat         pv df\n1 4.604792 0.03188272  1\n2 0.272147 0.60189515  1\nEstimates and Variances:\n$est\n           500      1000      1500\n0 1 0.54917896 0.5940358 0.6604903\n1 1 0.18181818 0.4242424        NA\n0 2 0.18168014 0.2066006 0.2066006\n1 2 0.09090909 0.2121212        NA\n\n$var\n            500        1000        1500\n0 1 0.002955869 0.003335897 0.004199157\n1 1 0.014958678 0.033339569          NA\n0 2 0.001727112 0.002271242 0.002271242\n1 2 0.008449138 0.022024737          NA\n\nplot(ic)\n\n\n\n\n\n\n\n\nAvec survminer. Pour avoir un seul graphique pour toutes les courbes ajouter l’option multiple_panels = F\n\nggcompetingrisks(fit = ic)\n\n\n\n\n\n\n\nggcompetingrisks(fit = ic, multiple_panels = F)"
  },
  {
    "objectID": "R.html#modèles",
    "href": "R.html#modèles",
    "title": "R",
    "section": "Modèles",
    "text": "Modèles\n\nFine-Gray\nAttention à l’interprétation des “Risks/Hazards ratio”. Modèle dont l’utilisation est assez critiquée, il n’est pas impossible que le retire à un moment ou un autre. Comme on travaille\nFonction crr du plackage cmprsk.\nPeu pratique les covariables doivent être introduites sous forme de matrice (n observation * k variables). Pour les covariables discrètes, prévoir la forme binaire et l’omission de la catégorie de référence.\n\nc <- compet[c(\"year\", \"age\", \"surgery\")]\nc = as.matrix(c)\nsummary(crr(compet$stime, compet$compet, c))\n\nCompeting Risks Regression\n\nCall:\ncrr(ftime = compet$stime, fstatus = compet$compet, cov1 = c)\n\n           coef exp(coef) se(coef)     z p-value\nyear    -0.0724     0.930   0.0713 -1.02   0.310\nage      0.0370     1.038   0.0176  2.10   0.036\nsurgery -0.8688     0.419   0.4488 -1.94   0.053\n\n        exp(coef) exp(-coef)  2.5% 97.5%\nyear        0.930      1.075 0.809  1.07\nage         1.038      0.964 1.002  1.07\nsurgery     0.419      2.384 0.174  1.01\n\nNum. cases = 103\nPseudo Log-likelihood = -228 \nPseudo likelihood ratio test = 11.2  on 3 df,\n\n\n\n\nModèle multinomial\nOn va de nouveau utiliser la variable mois (temps discret).\nLe modèle sera estimé à l’aide la fonction multinom du package nnet, les p-values doivent-être programmées, l’output ne donnant que les erreurs-types.\n\n#install.packages(\"nnet\")\nlibrary(nnet)\n\nTransformation de la base\n\ncompet$T = compet$mois\ntd = uncount(compet, mois)\ntd$x=1\ntd$t = ave(td$x, td$id, FUN=cumsum)\ntd$t2 = td$t*td$t\ntd$e = ifelse(td$t<td$T,0, td$compet)\n\nEstimation\nPour estimer le modèle, on utilise la fonction mlogit du package nnet. Les p-values seront calculées à partir d’un test bilatéral (statistique z). Pour avoir un output correct, on peut conseiler le package gtsummary (fonction tlb_regression).\n\ncompetfit = multinom(formula = e ~ t + t2 + year + age + surgery, data = td)\n\n# weights:  21 (12 variable)\ninitial  value 1238.136049 \niter  10 value 579.836377\niter  20 value 387.824428\niter  30 value 277.775477\niter  40 value 275.151304\niter  50 value 275.005454\nfinal  value 275.005419 \nconverged\n\n#summary(competfit)\n#z = summary(competfit)$coefficients/summary(competfit)$standard.errors\n#p <- (1 - pnorm(abs(z), 0, 1)) * 2\n#p\nstargazer(competfit, type = \"text\", ci = TRUE, star.cutoffs = NA, omit.table.layout = \"n\")\n\n\n====================================================\n                         Dependent variable:        \n                  ----------------------------------\n                         1                 2        \n                        (1)               (2)       \n----------------------------------------------------\nt                      -0.203           -0.202      \n                  (-0.284, -0.123) (-0.338, -0.067) \n                                                    \nt2                     0.003             0.003      \n                   (0.001, 0.005)  (-0.00003, 0.006)\n                                                    \nyear                   -0.128           -0.204      \n                  (-0.151, -0.106) (-0.235, -0.172) \n                                                    \nage                    0.044             0.011      \n                   (0.011, 0.077)   (-0.036, 0.058) \n                                                    \nsurgery                -1.147           -0.614      \n                  (-2.190, -0.105)  (-2.106, 0.878) \n                                                    \nConstant               5.647            11.316      \n                   (5.632, 5.662)  (11.290, 11.342) \n                                                    \n----------------------------------------------------\nAkaike Inf. Crit.     574.011           574.011     \n===================================================="
  },
  {
    "objectID": "Sas.html",
    "href": "Sas.html",
    "title": "SAS",
    "section": "",
    "text": "Remarque: Sélection des outputs\nSelon le type d’analyse la totalité des outputs ne seront pas reproduits (ods include ou ods exclude pour la sélection). Un problème spécifique s’observe pour le tableau des estimateurs de Kaplan-Meier qui est particulièrement illisible en présence d’un nombre important d’observations censurées.\nExemple pour proc lifetest: noms des outputs récupérés dans la log\nUtiliser de préférence le nom figurant dans la ligne path: (si comparaison de deux strates, le nom figurant dans la ligne name est identique)."
  },
  {
    "objectID": "Sas.html#méthode-actuarielle",
    "href": "Sas.html#méthode-actuarielle",
    "title": "SAS",
    "section": "Méthode actuarielle",
    "text": "Méthode actuarielle\nAvec une longueur d’intervalle fixe égale à 30 jours.\nLa durée médiane est donnée par la colonne résidual median time. Sur la première ligne, il s’agit de la durée médiane sur toutes les personnes exposées au risque. Dans les lignes suivante, cette durée médiane est recalculée pour les personnes restant exposées au risque dans chaque intervalle.\n\nproc lifetest data=trans method=lifetable width=30;\ntime stime*died(0);run;"
  },
  {
    "objectID": "Sas.html#méthode-kaplan-meier",
    "href": "Sas.html#méthode-kaplan-meier",
    "title": "SAS",
    "section": "Méthode Kaplan-Meier",
    "text": "Méthode Kaplan-Meier\nLe tableau des estimateurs ne sera pas reporté (voir intro du document).\nPour récupérer ces estimateurs, on peut les récupérer via l’instruction output et les exporter, par exemple, dans un tableur.\n\nods exclude Lifetest.Stratum1.ProductLimitEstimates;\nproc lifetest data=trans;\ntime stime*died(0); run;\n\n\n\n\nWarning sur la durée moyenne reportée Sauf exception ne pas interpréter le tableau donnant la durée moyenne. Se reporter à l’estimation des RMST plus bas.\nComparaison des fonctions de survie\nTests du log rank\n\nods exclude Lifetest.Stratum1.ProductLimitEstimates Lifetest.Stratum2.ProductLimitEstimates ;\nproc lifetest data=trans;\ntime stime*died(0);\nstrata surgery / test=all;\nrun;\n\n\n\n\n\nComparaison des RMST\nDisponible avec le dernier module stat de Sas base (Sas-Stat 15.1 novembre 2018).\n\nods exclude Lifetest.Stratum1.ProductLimitEstimates;\nproc lifetest data=trans rmst plots=(rmst);\ntime stime*died(0);\nstrata surgery; run;\n\n  SAS STUDIO: en attente de la mise à jour du module SAS-STAT (on a 5 ans de retard)"
  },
  {
    "objectID": "Sas.html#estimation-du-modèle",
    "href": "Sas.html#estimation-du-modèle",
    "title": "SAS",
    "section": "Estimation du modèle",
    "text": "Estimation du modèle\nproc phreg data=trans;\nmodel stime*died(0) = year age surgery ;\nrun;"
  },
  {
    "objectID": "Sas.html#tests-de-lhypothèse-ph",
    "href": "Sas.html#tests-de-lhypothèse-ph",
    "title": "SAS",
    "section": "Tests de l’hypothèse PH",
    "text": "Tests de l’hypothèse PH\n\nTest de Grambsch Therneau\nDemande au moins l’avant dernière version de Sas/Stat (2016?).\nLe test est exécuté directement dans l’instruction phreg (ajouter zph). L’option global permet de récupérer le résultat du test omnibus (attention rejette facilement \\(H_0\\) - hypothèse PH respectée - lorsque le nombre de degrés de liberté est élevé).\n\nods select PHReg.zphTest;\nproc phreg data=trans zph(global noplot);\nmodel stime*died(0) = year age surgery ;\nrun;\n\n\nPar défaut SAS utilise la transformation \\(f(t)=t\\) (idem Stata). Pour obtenir l’option par défaut de R \\(f(t) = 1 - KM(t)\\):\n\nods select PHReg.zphTest;\nproc phreg data=trans zph(global noplot transform=km);\nmodel stime*died(0) = year age surgery ;\nrun;\n\n\n\n\nInteraction avec la durée\nEstimation d’un modèle avec indicatrices\nLa covariable doit être sous forme d’indicatrice (binaire: (0,1)). Ce qui est le cas ici avec la variable surgery.\n Exemple avec une covariable X à 3 modalités codée 1,2,3.\n Estimation du modèle de Cox avec l’instruction class (ref: X=1)\n\nproc phreg data=base;\nclass X(ref=\"1\");\nmodel variable_dur*variable_cens(0) = X; run;\n\nEstimation du modèle de Cox avec indicatrices\n\ndata base; set base;\nX1 = X=1;\nX2 = X=2;\nX3 = X=3; run;\n\nproc phreg data=base;\nmodel variable_dur*variable_cens(0) = X2 X3; run;\n\nLa variable d’intéraction (\\(surgeryt = surgery\\times stime\\)) est générée, le temps de l’estimation après l’instruction model.\n\nods select PHReg.ParameterEstimates;\nproc phreg data=trans ;\nmodel stime*died(0) = year age surgery surgeryt ;\nsurgeryt = surgery*stime;\nrun;"
  },
  {
    "objectID": "Sas.html#variable-dynamique",
    "href": "Sas.html#variable-dynamique",
    "title": "SAS",
    "section": "Variable dynamique",
    "text": "Variable dynamique\nWarning: opération en ‘aveugle’\nContrairement à R et Stata, la base n’a pas à être splittée, on ne peut pas vérifier si la variable dynamique a été correctement créée. La variable dynamique, qui peut être appréhendée comme une variable en intéraction avec la durée, est générée après l’instruction model.\n Ici la tvc prendra la valeur 1 lorsque stime>wait, 0 sinon.\n\nods select PHReg.ParameterEstimates;\nproc phreg data=trans;\nmodel stime*died(0) = year age surgery tvc ;\ntvc = transplant=1 and stime>=wait;\nrun;"
  },
  {
    "objectID": "Sas.html#mise-en-forme",
    "href": "Sas.html#mise-en-forme",
    "title": "SAS",
    "section": "Mise en forme",
    "text": "Mise en forme\nOn utilise une boucle pour répliquer les lignes sur la valeur de la durée. La nouvelle variable de durée (t) sous forme de compteur est générée automatiquement.\n\ndata td; set trans; \ndo t=1 to mois; \n     output; \n     end; run;\n     \ndata td; set td;\nif t<mois then died=0;\nt2=t*t;\nt3=t2*t; run;"
  },
  {
    "objectID": "Sas.html#durée-continue",
    "href": "Sas.html#durée-continue",
    "title": "SAS",
    "section": "Durée continue",
    "text": "Durée continue\nEstimation du modèle\n\nods select Logistic.FitStatistics;\nproc logistic data=td;\nmodel died(ref=\"0\") = t t2 t3 year age surgery  ; run;"
  },
  {
    "objectID": "Sas.html#durée-discrète",
    "href": "Sas.html#durée-discrète",
    "title": "SAS",
    "section": "Durée discrète",
    "text": "Durée discrète\nPour l’exemple on va regrouper la durée par ses quartiles. Pour chaque individu, on conserve seulement une observation dans chaque quartile.\n\nproc rank data=td out=td2 groups=4;\nvar t;\nranks tq4;\nrun;\n\ndata td2; set td2;\nid2=put(id, 3.);\ntq42=put(tq4, 1.);\ng=id2 || tq42; run;\n\nproc sort data=td2; by id tq4; run;\n\ndata td2; set td2;\nby g;\nif LAST.g; run;\n\nEstimation\n\nproc logistic data=td2;\nclass tq4 / param=ref;\nmodel died(ref=\"0\") = tq4 year age surgery; run;"
  },
  {
    "objectID": "Sas.html#non-paramétrique",
    "href": "Sas.html#non-paramétrique",
    "title": "SAS",
    "section": "Non paramétrique",
    "text": "Non paramétrique\nOn indique en option la cause d’intérêt avec eventcode=valeur , les autres étant considérées commes des risques concurrents.\n\nproc lifetest data=trans plots=CIF;\ntime stime*compet(0) / eventcode=1; run;\n\n\n\n\nPour récupérer le test de Gray, on utilise l’instruction strata.\n\nproc lifetest data=trans plots=CIF;\ntime stime*compet(0) / eventcode=1\nstrata surgery; run;"
  },
  {
    "objectID": "Sas.html#modèles",
    "href": "Sas.html#modèles",
    "title": "SAS",
    "section": "Modèles",
    "text": "Modèles\nModèle de Fine-Gray\n\nproc phreg data=trans;\nmodel stime*compet(0) = year age surgery / eventcode=1  ;\nrun;\n\n\nModèle logistique multinomial à temps discret\n\ndata td; set trans; \ndo t=1 to mois; \n     output; \n     end; \nrun;\ndata td; set td;\nif t<mois then compet=0;\nt2=t*t\nrun;\n\n\nproc logistic data=td;\nmodel compet(ref=\"0\") = t t2 year age surgery / link=glogit;\nrun;"
  },
  {
    "objectID": "Stata.html",
    "href": "Stata.html",
    "title": "Stata",
    "section": "",
    "text": "Ouverture de la base"
  },
  {
    "objectID": "Stata.html#méthode-actuarielle",
    "href": "Stata.html#méthode-actuarielle",
    "title": "Stata",
    "section": "Méthode actuarielle",
    "text": "Méthode actuarielle\nContrairement à la formation, l’estimation sera faite sur des intervalles de 30 jours\n\nltable stime died, interval(30) graph\n\n/*\n                 Beg.                                 Std.\n   Interval     Total   Deaths   Lost    Survival    Error     [95% Conf. Int.]\n-------------------------------------------------------------------------------\n    0    30       103       22      1     0.7854    0.0406     0.6926    0.8531\n   30    60        80       14      2     0.6462    0.0475     0.5449    0.7305\n   60    90        64       12      0     0.5250    0.0498     0.4232    0.6171\n   90   120        52        5      1     0.4741    0.0499     0.3738    0.5677\n  120   150        46        1      1     0.4636    0.0499     0.3637    0.5575\n  150   180        44        2      0     0.4426    0.0498     0.3435    0.5369\n  180   210        42        3      1     0.4106    0.0495     0.3132    0.5053\n  210   240        38        1      0     0.3998    0.0494     0.3030    0.4945\n  240   270        37        1      1     0.3888    0.0492     0.2928    0.4836\n  270   300        35        2      0     0.3666    0.0488     0.2720    0.4614\n  300   330        33        1      0     0.3555    0.0486     0.2618    0.4502\n  330   360        32        3      1     0.3216    0.0478     0.2308    0.4157\n  360   390        28        0      1     0.3216    0.0478     0.2308    0.4157\n  390   420        27        0      1     0.3216    0.0478     0.2308    0.4157\n  420   450        26        0      2     0.3216    0.0478     0.2308    0.4157\n  480   510        24        0      1     0.3216    0.0478     0.2308    0.4157\n  510   540        23        0      1     0.3216    0.0478     0.2308    0.4157\n  540   570        22        0      1     0.3216    0.0478     0.2308    0.4157\n  570   600        21        1      1     0.3059    0.0479     0.2155    0.4010\n  600   630        19        0      1     0.3059    0.0479     0.2155    0.4010\n  660   690        18        1      1     0.2885    0.0483     0.1982    0.3849\n  720   750        16        1      0     0.2704    0.0485     0.1807    0.3681\n  840   870        15        1      1     0.2518    0.0486     0.1629    0.3506\n  900   930        13        0      1     0.2518    0.0486     0.1629    0.3506\n  930   960        12        0      1     0.2518    0.0486     0.1629    0.3506\n  960   990        11        1      0     0.2289    0.0493     0.1404    0.3304\n  990  1020        10        1      0     0.2060    0.0494     0.1192    0.3093\n 1020  1050         9        1      0     0.1831    0.0489     0.0992    0.2873\n 1140  1170         8        0      1     0.1831    0.0489     0.0992    0.2873\n 1320  1350         7        0      1     0.1831    0.0489     0.0992    0.2873\n 1380  1410         6        1      2     0.1465    0.0510     0.0645    0.2602\n 1560  1590         3        0      2     0.1465    0.0510     0.0645    0.2602\n 1770  1800         1        0      1     0.1465    0.0510     0.0645    0.2602\n-------------------------------------------------------------------------------\n*/\n\n\nRécupération des quartiles de la durée\n\nInstallation de la commande qlt\n\nnet install qlt, from(\"https://mthevenin.github.io/analyse_duree/ado/qlt/\") replace\n\n* help qlt\n\n\nltable stime died, interval(30) saving(base, replace)\nuse base, clear\nqlt\n\n\nDuree pour differents quantiles de la fonction de survie\nDefinition des bornes Stata-ltable\nS(t)=0.90: t=        .\nS(t)=0.75: t=    7.623\nS(t)=0.50: t=   74.729\nS(t)=0.25: t=  849.325\nS(t)=0.10: t=        .\n\nAvec la définition des bornes des intervalles de Sas\n\nqlt, sas\n\n/*\nDuree pour differents quantiles de la fonction de survie\nDefinition des bornes Sas-lifetest\nS(t)=0.90: t=   13.977\nS(t)=0.75: t=   37.623\nS(t)=0.50: t=  104.729\nS(t)=0.25: t=  906.993\nS(t)=0.10: t=        .\n*/"
  },
  {
    "objectID": "Stata.html#méthode-kaplan-meier",
    "href": "Stata.html#méthode-kaplan-meier",
    "title": "Stata",
    "section": "Méthode Kaplan-Meier",
    "text": "Méthode Kaplan-Meier\nMode analyse des durées: stset\nLes données doivent être mises en mode analyse de durée (help stset)\n\nqui use \"D:\\Marc\\SMS\\FORMATIONS\\2017\\analyse biographique\\A distribuer\\transplantation.dta\" , clear\nstset stime, f(died)\n\n/*\n     failure event:  (assumed to fail at time=stime)\nobs. time interval:  (0, stime]\n exit on or before:  failure\n\n------------------------------------------------------------------------------\n        103  total observations\n          0  exclusions\n------------------------------------------------------------------------------\n        103  observations remaining, representing\n        103  failures in single-record/single-failure data\n     31,938  total analysis time at risk and under observation\n                                                at risk from t =         0\n                                     earliest observed entry t =         0\n                                          last observed exit t =     1,799\n\nlist in 1/10\n\n     +-----------------------------------------------------------------------------------+\n     | id   year   age   died   stime   surgery   transp~t   wait   _st   _d    _t   _t0 |\n     |-----------------------------------------------------------------------------------|\n  1. |  1     67    30      1      50         0          0      0     1    1    50     0 |\n  2. |  2     68    51      1       6         0          0      0     1    1     6     0 |\n  3. |  3     68    54      1      16         0          1      1     1    1    16     0 |\n  4. |  4     68    40      1      39         0          1     36     1    1    39     0 |\n  5. |  5     68    20      1      18         0          0      0     1    1    18     0 |\n     |-----------------------------------------------------------------------------------|\n  6. |  6     68    54      1       3         0          0      0     1    1     3     0 |\n  7. |  7     68    50      1     675         0          1     51     1    1   675     0 |\n  8. |  8     68    45      1      40         0          0      0     1    1    40     0 |\n  9. |  9     68    47      1      85         0          0      0     1    1    85     0 |\n 10. | 10     68    42      1      58         0          1     12     1    1    58     0 |\n     +-----------------------------------------------------------------------------------+\n*/\n\nEstimation de la fonction de survie\n\nsts list\n\n/*\n         failure _d:  died\n   analysis time _t:  stime\n\n             At                  Survivor      Std.\n  Time     Risk   Fail   Lost    Function     Error     [95% Conf. Int.]\n------------------------------------------------------------------------\n     1      103      1      0      0.9903    0.0097     0.9331    0.9986\n     2      102      3      0      0.9612    0.0190     0.8998    0.9852\n     3       99      3      0      0.9320    0.0248     0.8627    0.9670\n     5       96      2      0      0.9126    0.0278     0.8388    0.9535\n     6       94      2      0      0.8932    0.0304     0.8155    0.9394\n     8       92      1      0      0.8835    0.0316     0.8040    0.9321\n     9       91      1      0      0.8738    0.0327     0.7926    0.9247\n    11       90      0      1      0.8738    0.0327     0.7926    0.9247\n    12       89      1      0      0.8640    0.0338     0.7811    0.9171\n    16       88      3      0      0.8345    0.0367     0.7474    0.8937\n    17       85      1      0      0.8247    0.0375     0.7363    0.8857\n    18       84      1      0      0.8149    0.0383     0.7253    0.8777\n    21       83      2      0      0.7952    0.0399     0.7034    0.8614\n    28       81      1      0      0.7854    0.0406     0.6926    0.8531\n    30       80      1      0      0.7756    0.0412     0.6819    0.8448\n    31       79      0      1      0.7756    0.0412     0.6819    0.8448\n    32       78      1      0      0.7657    0.0419     0.6710    0.8363\n    35       77      1      0      0.7557    0.0425     0.6603    0.8278\n    36       76      1      0      0.7458    0.0431     0.6495    0.8192\n    37       75      1      0      0.7358    0.0436     0.6388    0.8106\n    39       74      1      1      0.7259    0.0442     0.6282    0.8019\n    40       72      2      0      0.7057    0.0452     0.6068    0.7842\n    43       70      1      0      0.6956    0.0457     0.5961    0.7752\n    45       69      1      0      0.6856    0.0461     0.5855    0.7662\n    50       68      1      0      0.6755    0.0465     0.5750    0.7572\n    51       67      1      0      0.6654    0.0469     0.5645    0.7481\n    53       66      1      0      0.6553    0.0472     0.5541    0.7390\n    58       65      1      0      0.6452    0.0476     0.5437    0.7298\n    61       64      1      0      0.6352    0.0479     0.5333    0.7206\n    66       63      1      0      0.6251    0.0482     0.5230    0.7113\n    68       62      2      0      0.6049    0.0487     0.5026    0.6926\n    69       60      1      0      0.5948    0.0489     0.4924    0.6832\n    72       59      2      0      0.5747    0.0493     0.4722    0.6643\n    77       57      1      0      0.5646    0.0494     0.4621    0.6548\n    78       56      1      0      0.5545    0.0496     0.4521    0.6453\n    80       55      1      0      0.5444    0.0497     0.4422    0.6357\n    81       54      1      0      0.5343    0.0498     0.4323    0.6261\n    85       53      1      0      0.5243    0.0499     0.4224    0.6164\n    90       52      1      0      0.5142    0.0499     0.4125    0.6067\n    96       51      1      0      0.5041    0.0499     0.4027    0.5969\n   100       50      1      0      0.4940    0.0499     0.3930    0.5872\n   102       49      1      0      0.4839    0.0499     0.3833    0.5773\n   109       48      0      1      0.4839    0.0499     0.3833    0.5773\n   110       47      1      0      0.4736    0.0499     0.3733    0.5673\n   131       46      0      1      0.4736    0.0499     0.3733    0.5673\n   149       45      1      0      0.4631    0.0499     0.3632    0.5571\n   153       44      1      0      0.4526    0.0499     0.3531    0.5468\n   165       43      1      0      0.4421    0.0498     0.3430    0.5364\n   180       42      0      1      0.4421    0.0498     0.3430    0.5364\n   186       41      1      0      0.4313    0.0497     0.3327    0.5258\n   188       40      1      0      0.4205    0.0497     0.3225    0.5152\n   207       39      1      0      0.4097    0.0495     0.3123    0.5045\n   219       38      1      0      0.3989    0.0494     0.3022    0.4938\n   263       37      1      0      0.3881    0.0492     0.2921    0.4830\n   265       36      0      1      0.3881    0.0492     0.2921    0.4830\n   285       35      2      0      0.3660    0.0488     0.2714    0.4608\n   308       33      1      0      0.3549    0.0486     0.2612    0.4496\n   334       32      1      0      0.3438    0.0483     0.2510    0.4383\n   340       31      1      1      0.3327    0.0480     0.2409    0.4270\n   342       29      1      0      0.3212    0.0477     0.2305    0.4153\n   370       28      0      1      0.3212    0.0477     0.2305    0.4153\n   397       27      0      1      0.3212    0.0477     0.2305    0.4153\n   427       26      0      1      0.3212    0.0477     0.2305    0.4153\n   445       25      0      1      0.3212    0.0477     0.2305    0.4153\n   482       24      0      1      0.3212    0.0477     0.2305    0.4153\n   515       23      0      1      0.3212    0.0477     0.2305    0.4153\n   545       22      0      1      0.3212    0.0477     0.2305    0.4153\n   583       21      1      0      0.3059    0.0478     0.2156    0.4008\n   596       20      0      1      0.3059    0.0478     0.2156    0.4008\n   620       19      0      1      0.3059    0.0478     0.2156    0.4008\n   670       18      0      1      0.3059    0.0478     0.2156    0.4008\n   675       17      1      0      0.2879    0.0483     0.1976    0.3844\n   733       16      1      0      0.2699    0.0485     0.1802    0.3676\n   841       15      0      1      0.2699    0.0485     0.1802    0.3676\n   852       14      1      0      0.2507    0.0487     0.1616    0.3497\n   915       13      0      1      0.2507    0.0487     0.1616    0.3497\n   941       12      0      1      0.2507    0.0487     0.1616    0.3497\n   979       11      1      0      0.2279    0.0493     0.1394    0.3295\n   995       10      1      0      0.2051    0.0494     0.1183    0.3085\n  1032        9      1      0      0.1823    0.0489     0.0985    0.2865\n  1141        8      0      1      0.1823    0.0489     0.0985    0.2865\n  1321        7      0      1      0.1823    0.0489     0.0985    0.2865\n  1386        6      1      0      0.1519    0.0493     0.0713    0.2606\n  1400        5      0      1      0.1519    0.0493     0.0713    0.2606\n  1407        4      0      1      0.1519    0.0493     0.0713    0.2606\n  1571        3      0      1      0.1519    0.0493     0.0713    0.2606\n  1586        2      0      1      0.1519    0.0493     0.0713    0.2606\n  1799        1      0      1      0.1519    0.0493     0.0713    0.2606\n------------------------------------------------------------------------\n*/\n\n\nsts graph\n\n\nComparaison des fonctions de survie\n Tests du log rank\nOn va comparer les fonctions de survie pour la variable surgery.\n\nsts graph, by(surgery)\n\n\nTests du log rank: fonction sts test. On affichera ici plusieurs variantes du test.\n\nlocal test `\" \"l\" \"w\" \"tw\" \"p\" \"'\nforeach test2 of local test {\nsts test surgery, `test2'\n}\n\n/*\nLog-rank test for equality of survivor functions\n\n        |   Events         Events\nsurgery |  observed       expected\n--------+-------------------------\n0       |        69          60.34\n1       |         6          14.66\n--------+-------------------------\nTotal   |        75          75.00\n\n              chi2(1) =       6.59\n              Pr>chi2 =     0.0103\n\n         failure _d:  died\n   analysis time _t:  stime\n\n\nWilcoxon (Breslow) test for equality of survivor functions\n\n        |   Events         Events        Sum of\nsurgery |  observed       expected        ranks\n--------+--------------------------------------\n0       |        69          60.34          623\n1       |         6          14.66         -623\n--------+--------------------------------------\nTotal   |        75          75.00            0\n\n              chi2(1) =       8.99\n              Pr>chi2 =     0.0027\n\n         failure _d:  died\n   analysis time _t:  stime\n\n\nTarone-Ware test for equality of survivor functions\n\n        |   Events         Events        Sum of\nsurgery |  observed       expected        ranks\n--------+--------------------------------------\n0       |        69          60.34    73.105398\n1       |         6          14.66   -73.105398\n--------+--------------------------------------\nTotal   |        75          75.00            0\n\n              chi2(1) =       8.46\n              Pr>chi2 =     0.0036\n\n         failure _d:  died\n   analysis time _t:  stime\n\n\nPeto-Peto test for equality of survivor functions\n\n        |   Events         Events        Sum of\nsurgery |  observed       expected        ranks\n--------+--------------------------------------\n0       |        69          60.34    6.0505875\n1       |         6          14.66   -6.0505875\n--------+--------------------------------------\nTotal   |        75          75.00            0\n\n              chi2(1) =       8.66\n              Pr>chi2 =     0.0032\n*/\n\nComparaison des rmst\n Installation de la commande strmst2:\n\nssc install strmst2\n\narm1 = opération\narm0 = pas d’opération\n\nstrmst2 surgery\n\n\n/*\nRestricted Mean Survival Time (RMST) by arm\n-----------------------------------------------------------\n   Group |  Estimate    Std. Err.      [95% Conf. Interval]\n---------+-------------------------------------------------\n   arm 1 |   734.758     133.478      473.145      996.370\n   arm 0 |   310.168      43.160      225.576      394.760\n-----------------------------------------------------------\n\nBetween-group contrast (arm 1 versus arm 0) \n------------------------------------------------------------------------\n           Contrast  |  Estimate       [95% Conf. Interval]     P>|z|\n---------------------+--------------------------------------------------\nRMST (arm 1 - arm 0) |   424.590      149.641      699.539      0.002\nRMST (arm 1 / arm 0) |     2.369        1.513        3.710      0.000\n------------------------------------------------------------------------\n*/"
  },
  {
    "objectID": "Stata.html#modèle-de-cox",
    "href": "Stata.html#modèle-de-cox",
    "title": "Stata",
    "section": "Modèle de Cox",
    "text": "Modèle de Cox\n\nEstimation du modèle\nAvec la correction d’Efron\n\nstcox year age surgery, nolog noshow efron\n\n/*\nCox regression -- Efron method for ties\n\nNo. of subjects =          103                  Number of obs    =         103\nNo. of failures =           75\nTime at risk    =        31938\n                                                LR chi2(3)       =       17.63\nLog likelihood  =   -289.30639                  Prob > chi2      =      0.0005\n\n------------------------------------------------------------------------------\n          _t | Haz. Ratio   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        year |     0.8872     0.0597    -1.78   0.076       0.7775      1.0124\n         age |     1.0300     0.0139     2.19   0.029       1.0031      1.0577\n     surgery |     0.3726     0.1625    -2.26   0.024       0.1584      0.8761\n------------------------------------------------------------------------------\n*/\n\n\nstcox year age surgery, nolog noshow efron nohr\n\n/*\nCox regression -- Efron method for ties\n\nNo. of subjects =          103                  Number of obs    =         103\nNo. of failures =           75\nTime at risk    =        31938\n                                                LR chi2(3)       =       17.63\nLog likelihood  =   -289.30639                  Prob > chi2      =      0.0005\n\n------------------------------------------------------------------------------\n          _t |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        year |    -0.1196     0.0673    -1.78   0.076      -0.2516      0.0124\n         age |     0.0296     0.0135     2.19   0.029       0.0031      0.0561\n     surgery |    -0.9873     0.4363    -2.26   0.024      -1.8424     -0.1323\n------------------------------------------------------------------------------\n*/\n\n\n\nTest de l’hypothèse PH\nTest Grambsch-Therneau sur les résidus de Schoenfeld\n\n* f(t)=t - par défaut \n\n\nestat phtest, detail\n\n\n/*\n      Test of proportional-hazards assumption\n\n      Time:  Time\n      ----------------------------------------------------------------\n                  |       rho            chi2       df       Prob>chi2\n      ------------+---------------------------------------------------\n      year        |      0.10162         0.80        1         0.3720\n      age         |      0.12937         1.61        1         0.2043\n      surgery     |      0.29664         5.54        1         0.0186\n      ------------+---------------------------------------------------\n      global test |                      8.76        3         0.0327\n      ----------------------------------------------------------------\n*/\n\n\n* f(t)= 1-km - solution par défaut de R\n      \nestat phtest, detail km\n\n/*\n      Test of proportional-hazards assumption\n\n      Time:  Kaplan-Meier\n      ----------------------------------------------------------------\n                  |       rho            chi2       df       Prob>chi2\n      ------------+---------------------------------------------------\n      year        |      0.15920         1.96        1         0.1620\n      age         |      0.10907         1.15        1         0.2845\n      surgery     |      0.25096         3.96        1         0.0465\n      ------------+---------------------------------------------------\n      global test |                      7.99        3         0.0462\n      ----------------------------------------------------------------\n*/\n\nIntéraction avec une fonction de la durée\n\\(f(t)=t\\)\n\nstcox year age surgery, nolog noshow efron nohr tvc(surgery) texp(_t)\n\n/*\nCox regression -- Efron method for ties\n\nNo. of subjects =          103                  Number of obs    =         103\nNo. of failures =           75\nTime at risk    =        31938\n                                                LR chi2(4)       =       21.58\nLog likelihood  =   -287.32903                  Prob > chi2      =      0.0002\n\n------------------------------------------------------------------------------\n          _t |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\nmain         |\n        year |    -0.1231     0.0668    -1.84   0.066      -0.2541      0.0079\n         age |     0.0289     0.0134     2.15   0.032       0.0025      0.0552\n     surgery |    -1.7547     0.6744    -2.60   0.009      -3.0765     -0.4330\n-------------+----------------------------------------------------------------\ntvc          |\n     surgery |     0.0022     0.0011     2.02   0.043       0.0001      0.0044\n------------------------------------------------------------------------------\nNote: Variables in tvc equation interacted with _t.\n*/\n\n\n\nIntroduction d’une variable dynamique (binaire)\nTransformation de la base en format long aux temps d’évènement\n\nEtape 1\n\nstset stime, f(died) id(id)\n\n/*\n                id:  id\n     failure event:  died != 0 & died < .\nobs. time interval:  (stime[_n-1], stime]\n exit on or before:  failure\n\n------------------------------------------------------------------------------\n        103  total observations\n          0  exclusions\n------------------------------------------------------------------------------\n        103  observations remaining, representing\n        103  subjects\n         75  failures in single-failure-per-subject data\n     31,938  total analysis time at risk and under observation\n                                                at risk from t =         0\n                                     earliest observed entry t =         0\n                                          last observed exit t =     1,799\n\n*/\n\nEtape 2\n\nstsplit, at(failure)\n\nstset stime, f(died) id(id)\n\nsort id _t\nlist in 1/23\n\n\n/*\n                id:  id\n     failure event:  died != 0 & died < .\nobs. time interval:  (stime[_n-1], stime]\n exit on or before:  failure\n\n------------------------------------------------------------------------------\n        105  total observations\n          2  ignored because id missing\n------------------------------------------------------------------------------\n        103  observations remaining, representing\n        103  subjects\n         75  failures in single-failure-per-subject data\n     31,938  total analysis time at risk and under observation\n                                                at risk from t =         0\n                                     earliest observed entry t =         0\n                                          last observed exit t =     1,799\n\n     +--------------------------------------------------------------------------------------------------+\n     | id   year   age   died   stime   surgery   transp~t   wait   mois   compet   _st   _d   _t   _t0 |\n     |--------------------------------------------------------------------------------------------------|\n  1. |  1     67    30      .       1         0          0      0      2        1     1    0    1     0 |\n  2. |  1     67    30      .       2         0          0      0      2        1     1    0    2     1 |\n  3. |  1     67    30      .       3         0          0      0      2        1     1    0    3     2 |\n  4. |  1     67    30      .       5         0          0      0      2        1     1    0    5     3 |\n  5. |  1     67    30      .       6         0          0      0      2        1     1    0    6     5 |\n     |--------------------------------------------------------------------------------------------------|\n  6. |  1     67    30      .       8         0          0      0      2        1     1    0    8     6 |\n  7. |  1     67    30      .       9         0          0      0      2        1     1    0    9     8 |\n  8. |  1     67    30      .      12         0          0      0      2        1     1    0   12     9 |\n  9. |  1     67    30      .      16         0          0      0      2        1     1    0   16    12 |\n 10. |  1     67    30      .      17         0          0      0      2        1     1    0   17    16 |\n     |--------------------------------------------------------------------------------------------------|\n 11. |  1     67    30      .      18         0          0      0      2        1     1    0   18    17 |\n 12. |  1     67    30      .      21         0          0      0      2        1     1    0   21    18 |\n 13. |  1     67    30      .      28         0          0      0      2        1     1    0   28    21 |\n 14. |  1     67    30      .      30         0          0      0      2        1     1    0   30    28 |\n 15. |  1     67    30      .      32         0          0      0      2        1     1    0   32    30 |\n     |--------------------------------------------------------------------------------------------------|\n 16. |  1     67    30      .      35         0          0      0      2        1     1    0   35    32 |\n 17. |  1     67    30      .      36         0          0      0      2        1     1    0   36    35 |\n 18. |  1     67    30      .      37         0          0      0      2        1     1    0   37    36 |\n 19. |  1     67    30      .      39         0          0      0      2        1     1    0   39    37 |\n 20. |  1     67    30      .      40         0          0      0      2        1     1    0   40    39 |\n     |--------------------------------------------------------------------------------------------------|\n 21. |  1     67    30      .      43         0          0      0      2        1     1    0   43    40 |\n 22. |  1     67    30      .      45         0          0      0      2        1     1    0   45    43 |\n 23. |  1     67    30      1      50         0          0      0      2        1     1    1   50    45 |\n     +--------------------------------------------------------------------------------------------------+\n*/\n\nEtape 3\n\ngen tvc = transplant==1 & wait<=_t\nsort id _t\nlist id transplant wait tvc _d _t _t0 if id==10  , noobs\n\n/*\n  +--------------------------------------------+\n  | id   transp~t   wait   tvc   _d   _t   _t0 |\n  |--------------------------------------------|\n  | 10          1     12     0    0    1     0 |\n  | 10          1     12     0    0    2     1 |\n  | 10          1     12     0    0    3     2 |\n  | 10          1     12     0    0    5     3 |\n  | 10          1     12     0    0    6     5 |\n  |--------------------------------------------|\n  | 10          1     12     0    0    8     6 |\n  | 10          1     12     0    0    9     8 |\n  | 10          1     12     1    0   12     9 |\n  | 10          1     12     1    0   16    12 |\n  | 10          1     12     1    0   17    16 |\n  |--------------------------------------------|\n  | 10          1     12     1    0   18    17 |\n  | 10          1     12     1    0   21    18 |\n  | 10          1     12     1    0   28    21 |\n  | 10          1     12     1    0   30    28 |\n  | 10          1     12     1    0   32    30 |\n  |--------------------------------------------|\n  | 10          1     12     1    0   35    32 |\n  | 10          1     12     1    0   36    35 |\n  | 10          1     12     1    0   37    36 |\n  | 10          1     12     1    0   39    37 |\n  | 10          1     12     1    0   40    39 |\n  |--------------------------------------------|\n  | 10          1     12     1    0   43    40 |\n  | 10          1     12     1    0   45    43 |\n  | 10          1     12     1    0   50    45 |\n  | 10          1     12     1    0   51    50 |\n  | 10          1     12     1    0   53    51 |\n  |--------------------------------------------|\n  | 10          1     12     1    1   58    53 |\n  +--------------------------------------------+\n*/\n\nEstimation du modèle\n\nstcox year age surgery tvc, nolog noshow efron nohr\n\nCox regression -- Efron method for ties\n\nNo. of subjects =          103                  Number of obs    =       3,573\nNo. of failures =           75\nTime at risk    =        31938\n                                                LR chi2(4)       =       17.70\nLog likelihood  =   -289.27014                  Prob > chi2      =      0.0014\n\n------------------------------------------------------------------------------\n          _t |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        year |    -0.1203     0.0673    -1.79   0.074      -0.2523      0.0117\n         age |     0.0304     0.0139     2.19   0.029       0.0032      0.0577\n     surgery |    -0.9829     0.4366    -2.25   0.024      -1.8385     -0.1273\n         tvc |    -0.0822     0.3048    -0.27   0.787      -0.6797      0.5153\n------------------------------------------------------------------------------"
  },
  {
    "objectID": "Stata.html#modèle-à-temps-discret",
    "href": "Stata.html#modèle-à-temps-discret",
    "title": "Stata",
    "section": "Modèle à temps discret",
    "text": "Modèle à temps discret\nVariable de durée = mois\n\nMise en forme\n\nexpand mois\nbysort id: gen t=_n\ngen e = died\nreplace e=0 if t<mois\n\n* list in 1/31\n\n/*\n\n     +-------------------------------------------------------------------------------------+\n     | id   year   age   died   stime   surgery   transp~t   wait   mois   compet    t   e |\n     |-------------------------------------------------------------------------------------|\n  1. |  1     67    30      1      50         0          0      0      2        1    1   0 |\n  2. |  1     67    30      1      50         0          0      0      2        1    2   1 |\n  3. |  2     68    51      1       6         0          0      0      1        1    1   1 |\n  4. |  3     68    54      1      16         0          1      1      1        1    1   1 |\n  5. |  4     68    40      1      39         0          1     36      2        2    1   0 |\n     |-------------------------------------------------------------------------------------|\n  6. |  4     68    40      1      39         0          1     36      2        2    2   1 |\n  7. |  5     68    20      1      18         0          0      0      1        1    1   1 |\n  8. |  6     68    54      1       3         0          0      0      1        2    1   1 |\n  9. |  7     68    50      1     675         0          1     51     23        1    1   0 |\n 10. |  7     68    50      1     675         0          1     51     23        1    2   0 |\n     |-------------------------------------------------------------------------------------|\n 11. |  7     68    50      1     675         0          1     51     23        1    3   0 |\n 12. |  7     68    50      1     675         0          1     51     23        1    4   0 |\n 13. |  7     68    50      1     675         0          1     51     23        1    5   0 |\n 14. |  7     68    50      1     675         0          1     51     23        1    6   0 |\n 15. |  7     68    50      1     675         0          1     51     23        1    7   0 |\n     |-------------------------------------------------------------------------------------|\n 16. |  7     68    50      1     675         0          1     51     23        1    8   0 |\n 17. |  7     68    50      1     675         0          1     51     23        1    9   0 |\n 18. |  7     68    50      1     675         0          1     51     23        1   10   0 |\n 19. |  7     68    50      1     675         0          1     51     23        1   11   0 |\n 20. |  7     68    50      1     675         0          1     51     23        1   12   0 |\n     |-------------------------------------------------------------------------------------|\n 21. |  7     68    50      1     675         0          1     51     23        1   13   0 |\n 22. |  7     68    50      1     675         0          1     51     23        1   14   0 |\n 23. |  7     68    50      1     675         0          1     51     23        1   15   0 |\n 24. |  7     68    50      1     675         0          1     51     23        1   16   0 |\n 25. |  7     68    50      1     675         0          1     51     23        1   17   0 |\n     |-------------------------------------------------------------------------------------|\n 26. |  7     68    50      1     675         0          1     51     23        1   18   0 |\n 27. |  7     68    50      1     675         0          1     51     23        1   19   0 |\n 28. |  7     68    50      1     675         0          1     51     23        1   20   0 |\n 29. |  7     68    50      1     675         0          1     51     23        1   21   0 |\n 30. |  7     68    50      1     675         0          1     51     23        1   22   0 |\n     |-------------------------------------------------------------------------------------|\n 31. |  7     68    50      1     675         0          1     51     23        1   23   1 |\n     +-------------------------------------------------------------------------------------+\n\n*/\n\n\n\nParamétrisation avec durée continue\nLes critères d’information\n\ngen t2=t^2\ngen t3=t^3\n\nlogit e  t ,  nolog \nestat ic\n\nlogit e  t t2 ,  nolog \nestat ic\n\nlogit e  t2 t3 ,  nolog \nestat ic\n\n\n\nLogistic regression                             Number of obs     =      1,127\n                                                LR chi2(1)        =      50.85\n                                                Prob > chi2       =     0.0000\nLog likelihood = -250.26058                     Pseudo R2         =     0.0922\n\n------------------------------------------------------------------------------\n           e |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n           t |    -0.1007     0.0185    -5.45   0.000      -0.1370     -0.0645\n       _cons |    -1.6436     0.1724    -9.53   0.000      -1.9815     -1.3057\n------------------------------------------------------------------------------\n\n\nAkaike's information criterion and Bayesian information criterion\n\n-----------------------------------------------------------------------------\n       Model |          N   ll(null)  ll(model)      df        AIC        BIC\n-------------+---------------------------------------------------------------\n           . |      1,127  -275.6841  -250.2606       2   504.5212   514.5758\n-----------------------------------------------------------------------------\nNote: BIC uses N = number of observations. See [R] BIC note.\n\n*****************************************************************************\n\nLogistic regression                             Number of obs     =      1,127\n                                                LR chi2(2)        =      65.25\n                                                Prob > chi2       =     0.0000\nLog likelihood = -243.05761                     Pseudo R2         =     0.1183\n\n------------------------------------------------------------------------------\n           e |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n           t |    -0.2172     0.0357    -6.08   0.000      -0.2872     -0.1471\n          t2 |     0.0034     0.0008     4.50   0.000       0.0019      0.0049\n       _cons |    -1.2326     0.1925    -6.40   0.000      -1.6098     -0.8554\n------------------------------------------------------------------------------\n\n\nAkaike's information criterion and Bayesian information criterion\n\n-----------------------------------------------------------------------------\n       Model |          N   ll(null)  ll(model)      df        AIC        BIC\n-------------+---------------------------------------------------------------\n           . |      1,127  -275.6841  -243.0576       3   492.1152   507.1972\n-----------------------------------------------------------------------------\nNote: BIC uses N = number of observations. See [R] BIC note.\n\n       \n*****************************************************************************       \n\nLogistic regression                             Number of obs     =      1,127\n                                                LR chi2(3)        =      72.86\n                                                Prob > chi2       =     0.0000\nLog likelihood = -239.25267                     Pseudo R2         =     0.1321\n\n------------------------------------------------------------------------------\n           e |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n           t |    -0.4038     0.0819    -4.93   0.000      -0.5643     -0.2434\n          t2 |     0.0157     0.0050     3.14   0.002       0.0059      0.0254\n          t3 |    -0.0002     0.0001    -2.31   0.021      -0.0003     -0.0000\n       _cons |    -0.8250     0.2406    -3.43   0.001      -1.2965     -0.3536\n------------------------------------------------------------------------------\n\nAkaike's information criterion and Bayesian information criterion\n\n-----------------------------------------------------------------------------\n       Model |          N   ll(null)  ll(model)      df        AIC        BIC\n-------------+---------------------------------------------------------------\n           . |      1,127  -275.6841  -239.2527       4   486.5053   506.6146\n-----------------------------------------------------------------------------\nNote: BIC uses N = number of observations. See [R] BIC note.\n\nEstimation du modèle\n\nlogit e  t t2 t3 year age surgery, nolog\n\n/*\n\nLogistic regression                             Number of obs     =      1,127\n                                                LR chi2(6)        =      90.69\n                                                Prob > chi2       =     0.0000\nLog likelihood = -230.33671                     Pseudo R2         =     0.1645\n\n------------------------------------------------------------------------------\n           e |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n           t |    -0.3721     0.0824    -4.52   0.000      -0.5335     -0.2106\n          t2 |     0.0142     0.0050     2.83   0.005       0.0044      0.0241\n          t3 |    -0.0002     0.0001    -2.11   0.035      -0.0003     -0.0000\n        year |    -0.1327     0.0738    -1.80   0.072      -0.2773      0.0119\n         age |     0.0333     0.0147     2.27   0.023       0.0046      0.0621\n     surgery |    -1.0109     0.4486    -2.25   0.024      -1.8902     -0.1317\n       _cons |     7.0827     5.3077     1.33   0.182      -3.3203     17.4856\n------------------------------------------------------------------------------\n*/"
  },
  {
    "objectID": "Stata.html#paramétrisation-avec-durée-discrète",
    "href": "Stata.html#paramétrisation-avec-durée-discrète",
    "title": "Stata",
    "section": "Paramétrisation avec durée discrète",
    "text": "Paramétrisation avec durée discrète\nPour l’exemple seulement, on prendra des intervalles découpés sur les quartiles de la durée\n\nxtile ct4=t, n(4)\nbysort id ct4: keep if _n==_N\n\ntab  ct4 e\n\nlogit e i.ct4  year age surgery,  nolog\n\n/*\n\n         4 |\n quantiles |           e\n      of t |         0          1 |     Total\n-----------+----------------------+----------\n         1 |        50         53 |       103 \n         2 |        35         11 |        46 \n         3 |        27          5 |        32 \n         4 |        10          6 |        16 \n-----------+----------------------+----------\n     Total |       122         75 |       197 \n\n\n\nLogistic regression                             Number of obs     =        197\n                                                LR chi2(6)        =      39.30\n                                                Prob > chi2       =     0.0000\nLog likelihood = -111.23965                     Pseudo R2         =     0.1501\n\n------------------------------------------------------------------------------\n           e |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         ct4 |\n          2  |    -1.0334     0.4189    -2.47   0.014      -1.8543     -0.2124\n          3  |    -1.6152     0.5449    -2.96   0.003      -2.6831     -0.5473\n          4  |    -0.4789     0.5993    -0.80   0.424      -1.6535      0.6957\n             |\n        year |    -0.2032     0.0932    -2.18   0.029      -0.3859     -0.0206\n         age |     0.0469     0.0185     2.53   0.011       0.0106      0.0831\n     surgery |    -1.1102     0.5026    -2.21   0.027      -2.0952     -0.1252\n       _cons |    12.4467     6.6537     1.87   0.061      -0.5943     25.4877\n------------------------------------------------------------------------------\n*/"
  },
  {
    "objectID": "Stata.html#modèle-aft",
    "href": "Stata.html#modèle-aft",
    "title": "Stata",
    "section": "Modèle AFT",
    "text": "Modèle AFT\nWeibull\nPar défaut, le modèle de Weibull est exécuté sous paramétrisation PH. Pour une paramétrisation type AFT, ajouter l’option time.\n\nwebuse set \"https://raw.githubusercontent.com//mthevenin/analyse_duree/master/bases/\"\nwebuse  \"transplantation_m\", clear\nwebuse set\n\nstset stime, f(died)\nstreg year age surgery , dist(weibull) time nolog noshow\nestat ic\n\n/*\n\nWeibull AFT regression\n\nNo. of subjects =          103                  Number of obs    =         103\nNo. of failures =           75\nTime at risk    =        31938\n                                                LR chi2(3)       =       18.87\nLog likelihood  =    -188.6278                  Prob > chi2      =      0.0003\n\n------------------------------------------------------------------------------\n          _t |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        year |     0.1620     0.1218     1.33   0.184      -0.0768      0.4008\n         age |    -0.0615     0.0247    -2.49   0.013      -0.1100     -0.0130\n     surgery |     1.9703     0.7794     2.53   0.011       0.4427      3.4980\n       _cons |    -3.0220     8.7284    -0.35   0.729     -20.1294     14.0854\n-------------+----------------------------------------------------------------\n       /ln_p |    -0.5868     0.0927    -6.33   0.000      -0.7685     -0.4051\n-------------+----------------------------------------------------------------\n           p |     0.5561     0.0516                        0.4637      0.6669\n         1/p |     1.7983     0.1667                        1.4995      2.1566\n------------------------------------------------------------------------------\n\nAkaike's information criterion and Bayesian information criterion\n\n-----------------------------------------------------------------------------\n       Model |          N   ll(null)  ll(model)      df        AIC        BIC\n-------------+---------------------------------------------------------------\n           . |        103  -198.0632  -188.6278       5   387.2556   400.4292\n-----------------------------------------------------------------------------\nNote: BIC uses N = number of observations. See [R] BIC note.\n\n*/\n\nLoglogistique\n\nstreg year age surgery , dist(loglog) nolog noshow \nestat ic\n\n/*\nLoglogistic AFT regression\n\nNo. of subjects =          103                  Number of obs    =         103\nNo. of failures =           75\nTime at risk    =        31938\n                                                LR chi2(3)       =       21.69\nLog likelihood  =   -183.03937                  Prob > chi2      =      0.0001\n\n------------------------------------------------------------------------------\n          _t |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        year |     0.2408     0.1172     2.05   0.040       0.0110      0.4705\n         age |    -0.0427     0.0213    -2.00   0.045      -0.0845     -0.0010\n     surgery |     2.2747     0.6913     3.29   0.001       0.9198      3.6296\n       _cons |   -10.4034     8.3410    -1.25   0.212     -26.7515      5.9446\n-------------+----------------------------------------------------------------\n    /lngamma |     0.1805     0.0970     1.86   0.063      -0.0095      0.3706\n-------------+----------------------------------------------------------------\n       gamma |     1.1979     0.1161                        0.9906      1.4486\n------------------------------------------------------------------------------\n\n\nAkaike's information criterion and Bayesian information criterion\n\n-----------------------------------------------------------------------------\n       Model |          N   ll(null)  ll(model)      df        AIC        BIC\n-------------+---------------------------------------------------------------\n           . |        103  -193.8865  -183.0394       5   376.0787   389.2524\n-----------------------------------------------------------------------------\n*/"
  },
  {
    "objectID": "Stata.html#non-paramétrique-estimation-des-ic",
    "href": "Stata.html#non-paramétrique-estimation-des-ic",
    "title": "Stata",
    "section": "Non paramétrique: estimation des IC",
    "text": "Non paramétrique: estimation des IC\nInstaller les commandes stcompet et stcomlist\n\nssc install stcompet\nssc install stcomlist\n\nLe risque d’intérêt est compet=1, le risque concurrent est compet=2\n\nstset stime, failure(compet==1)\nstcomlist, compet1(2)\n\n/*\n            failure:  compet == 1\n competing failures:  compet == 2\n\n    Time       CIF         SE     [95% Conf. Int.]\n--------------------------------------------------\n       1    0.0097     0.0097     0.0009    0.0477\n       2    0.0388     0.0190     0.0127    0.0892\n       3    0.0583     0.0231     0.0239    0.1149\n       5    0.0777     0.0264     0.0363    0.1395\n       6    0.0874     0.0278     0.0429    0.1515\n       8    0.0971     0.0292     0.0497    0.1634\n       9    0.1068     0.0304     0.0566    0.1751\n      12    0.1166     0.0316     0.0638    0.1868\n      16    0.1362     0.0338     0.0785    0.2099\n      18    0.1461     0.0349     0.0860    0.2212\n      21    0.1657     0.0367     0.1014    0.2437\n      32    0.1756     0.0376     0.1093    0.2550\n      37    0.1856     0.0384     0.1173    0.2662\n      40    0.1957     0.0393     0.1254    0.2775\n      43    0.2058     0.0400     0.1337    0.2888\n      45    0.2158     0.0408     0.1420    0.2999\n      50    0.2259     0.0415     0.1503    0.3110\n      51    0.2360     0.0422     0.1588    0.3221\n      53    0.2461     0.0428     0.1673    0.3330\n      58    0.2562     0.0434     0.1759    0.3439\n      61    0.2662     0.0440     0.1845    0.3548\n      66    0.2763     0.0445     0.1932    0.3656\n      69    0.2864     0.0450     0.2020    0.3763\n      72    0.3066     0.0459     0.2197    0.3976\n      77    0.3167     0.0464     0.2286    0.4082\n      78    0.3267     0.0467     0.2376    0.4187\n      81    0.3368     0.0471     0.2466    0.4292\n      85    0.3469     0.0475     0.2556    0.4396\n      90    0.3570     0.0478     0.2648    0.4500\n      96    0.3671     0.0481     0.2739    0.4604\n     102    0.3771     0.0484     0.2831    0.4707\n     110    0.3874     0.0487     0.2925    0.4812\n     149    0.3980     0.0489     0.3021    0.4920\n     165    0.4085     0.0492     0.3118    0.5027\n     186    0.4193     0.0495     0.3217    0.5137\n     188    0.4301     0.0497     0.3316    0.5246\n     207    0.4408     0.0499     0.3417    0.5354\n     219    0.4516     0.0501     0.3517    0.5462\n     263    0.4624     0.0502     0.3618    0.5570\n     285    0.4846     0.0505     0.3826    0.5791\n     308    0.4957     0.0506     0.3931    0.5900\n     340    0.5068     0.0507     0.4037    0.6009\n     583    0.5221     0.0514     0.4171    0.6168\n     675    0.5401     0.0524     0.4322    0.6361\n     733    0.5580     0.0532     0.4477    0.6548\n     995    0.5808     0.0548     0.4659    0.6795\n    1032    0.6036     0.0559     0.4851    0.7031\n    1386    0.6340     0.0583     0.5083    0.7357\n\n            failure:  compet == 2\n competing failures:  compet == 1\n\n    Time       CIF         SE     [95% Conf. Int.]\n--------------------------------------------------\n       3    0.0097     0.0097     0.0009    0.0477\n       6    0.0194     0.0136     0.0038    0.0619\n      16    0.0292     0.0166     0.0079    0.0761\n      17    0.0391     0.0191     0.0128    0.0897\n      28    0.0489     0.0213     0.0182    0.1029\n      30    0.0587     0.0232     0.0240    0.1157\n      35    0.0686     0.0250     0.0302    0.1286\n      36    0.0786     0.0267     0.0367    0.1411\n      39    0.0885     0.0282     0.0435    0.1534\n      40    0.0986     0.0296     0.0504    0.1658\n      68    0.1188     0.0322     0.0650    0.1901\n      80    0.1288     0.0334     0.0724    0.2020\n     100    0.1389     0.0345     0.0800    0.2138\n     153    0.1495     0.0356     0.0880    0.2261\n     334    0.1605     0.0368     0.0964    0.2392\n     342    0.1720     0.0381     0.1052    0.2526\n     852    0.1913     0.0417     0.1175    0.2787\n     979    0.2141     0.0460     0.1320    0.3094\n*/"
  },
  {
    "objectID": "Stata.html#modèle-cause-specific-cox",
    "href": "Stata.html#modèle-cause-specific-cox",
    "title": "Stata",
    "section": "Modèle cause-specific (Cox)",
    "text": "Modèle cause-specific (Cox)\nAttention: non relié aux IC\n\nstcox year age surgery,  nohr\n\n/*\nIteration 0:   log likelihood = -222.40766\nIteration 1:   log likelihood = -214.66912\nIteration 2:   log likelihood = -214.47069\nIteration 3:   log likelihood = -214.46905\nIteration 4:   log likelihood = -214.46905\nRefining estimates:\nIteration 0:   log likelihood = -214.46905\n\nCox regression -- Breslow method for ties\n\nNo. of subjects =          103                  Number of obs    =         103\nNo. of failures =           56\nTime at risk    =        31938\n                                                LR chi2(3)       =       15.88\nLog likelihood  =   -214.46905                  Prob > chi2      =      0.0012\n\n------------------------------------------------------------------------------\n          _t |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        year |    -0.1033     0.0774    -1.33   0.182      -0.2550      0.0485\n         age |     0.0385     0.0163     2.36   0.018       0.0065      0.0704\n     surgery |    -1.1099     0.5290    -2.10   0.036      -2.1468     -0.0730\n------------------------------------------------------------------------------\n*/"
  },
  {
    "objectID": "Stata.html#modèle-de-fine-gray",
    "href": "Stata.html#modèle-de-fine-gray",
    "title": "Stata",
    "section": "Modèle de Fine-Gray",
    "text": "Modèle de Fine-Gray\nLa commande stcrreg est installée avec les commandes de base. Relié directement aux IC, la définition du risque diffère du risque instantané usuel (risque de sous répartition).\n\nstcrreg year age surgery, compete(compet=2) nohr\n\n\n/*\n         failure _d:  compet == 1\n   analysis time _t:  stime\n\nIteration 0:   log pseudolikelihood = -227.92407  \nIteration 1:   log pseudolikelihood = -227.69764  \nIteration 2:   log pseudolikelihood = -227.69531  \nIteration 3:   log pseudolikelihood = -227.69531  \n\nCompeting-risks regression                       No. of obs       =        103\n                                                 No. of subjects  =        103\nFailure event  : compet == 1                     No. failed       =         56\nCompeting event: compet == 2                     No. competing    =         19\n                                                 No. censored     =         28\n\n                                                 Wald chi2(3)     =      11.42\nLog pseudolikelihood = -227.69531                Prob > chi2      =     0.0097\n\n------------------------------------------------------------------------------\n             |               Robust\n          _t |      Coef.   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n        year |    -0.0724     0.0716    -1.01   0.312      -0.2128      0.0679\n         age |     0.0370     0.0177     2.09   0.037       0.0022      0.0718\n     surgery |    -0.8688     0.4510    -1.93   0.054      -1.7528      0.0153\n------------------------------------------------------------------------------\n\n       */"
  },
  {
    "objectID": "Stata.html#modèle-logistique-multinomial",
    "href": "Stata.html#modèle-logistique-multinomial",
    "title": "Stata",
    "section": "Modèle logistique multinomial",
    "text": "Modèle logistique multinomial\nAttention: non relié aux IC Pour la variable de durée on utilise la variable mois\n\nexpand mois\nbysort id: gen t=_n\ngen t2=t*t\n\ngen e = compet\nreplace e=0 if t<mois\nmlogit e t t2 year age surgery\n\n/*\nIteration 0:   log likelihood = -318.13171  \nIteration 1:   log likelihood = -285.78811  \nIteration 2:   log likelihood = -275.20206  \nIteration 3:   log likelihood = -275.00574  \nIteration 4:   log likelihood = -275.00542  \nIteration 5:   log likelihood = -275.00542  \n\nMultinomial logistic regression                 Number of obs     =      1,127\n                                                LR chi2(10)       =      86.25\n                                                Prob > chi2       =     0.0000\nLog likelihood = -275.00542                     Pseudo R2         =     0.1356\n\n------------------------------------------------------------------------------\n           e |        RRR   Std. Err.      z    P>|z|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n0            |  (base outcome)\n-------------+----------------------------------------------------------------\n1            |\n           t |     0.8159     0.0338    -4.91   0.000       0.7522      0.8850\n          t2 |     1.0032     0.0009     3.53   0.000       1.0014      1.0049\n        year |     0.8795     0.0718    -1.57   0.116       0.7494      1.0321\n         age |     1.0449     0.0183     2.51   0.012       1.0097      1.0813\n     surgery |     0.3175     0.1711    -2.13   0.033       0.1104      0.9129\n* Constante non reportée     \n-------------+----------------------------------------------------------------\n2            |\n           t |     0.8168     0.0565    -2.93   0.003       0.7134      0.9353\n          t2 |     1.0030     0.0015     1.94   0.052       1.0000      1.0060\n        year |     0.8158     0.1127    -1.47   0.141       0.6223      1.0695\n         age |     1.0111     0.0248     0.45   0.654       0.9635      1.0610\n     surgery |     0.5412     0.4221    -0.79   0.431       0.1173      2.4959\n* Constante non reportée      \n------------------------------------------------------------------------------\n\n       */"
  },
  {
    "objectID": "Theorie.html",
    "href": "Theorie.html",
    "title": "La théorie",
    "section": "",
    "text": "L’analyse des durées peut être vue comme l’étude d’une variable aléatoire \\(T\\) qui décrit la durée d’attente jusqu’à l’occurence d’un évènement.\nLa principale caractéristique de l’analyse des durées est le traitement des informations dites censurées, lorsque la durée d’observation est plus courte que la durée d’exposition au risque."
  },
  {
    "objectID": "Theorie.html#censure-à-droite",
    "href": "Theorie.html#censure-à-droite",
    "title": "La théorie",
    "section": "Censure à droite",
    "text": "Censure à droite\nDéfinition\n Certains individus n’auront pas (encore) connu l’évènement à la date de l’enquête après une certaine durée d’exposition. On a donc besoin d’un marqueur permettant de déterminer que les individus n’ont pas observé l’évènement sur la période d’étude.\nPourquoi une information est-elle censurée (à droite)?\n\nFin de l’étude, date de l’enquête.\nPerdu de vue, décès si autre évènement étudié.\n\nEn pratique (important)\n\nNe pas exclure ces observations, sinon on surestime la survenue de l’évènement.\nNe pas les considérer a-priori comme sorties de l’exposition sans avoir connu l’évènement. Elles peuvent connaître l’évènement après la date de l’enquête ou en étant perdues de vue. Sinon on sous-estime la durée moyenne de survenue de l’évènement.\n\nExemple\nOn effectue une enquête auprès de femmes : On souhaite mesurer l’âge à la première naissance. Au moment de l’enquête, une femme est âgée de 29 ans n’a pas (encore) d’enfant.\nCette information sera dite «censurée».\nElle est clairement encore soumise au risque après la date de l’enquête. Au niveau de l’analyse, elle sera soumise au risque à partir de ses premières règles jusqu’au moment de l’enquête.\n\nHypothèse fondamentale\nLes observations censurées ont vis à vis du phénomène observé le même comportement que les observations non censurées. On dit que la censure est non informative. Elle ne dépend pas de l’évènement analysé. Normalement le problème ne se pose pas dans les recueil retrospectif. \nProblème posé par la censure informative\nPar exemple en analysant des décès avec un recueil prospectif, si un individu est perdu de vue en raison d’une dégradation de son état de santé, l’indépendance entre la cause de la censure et le décès ne peut plus être assurée.\nA l’Ined l’exploitation du registre des personnes atteintes de mucoviscidose (G.Bellis) donne une autre illustration de ce phénomène. Chaque année un nombre significatif de personnes sortent du registre (pas de résultats aux examens annuels). Si certain.e.s perdu.e.s de vue s’expliquent par des déménagements, émigration ou par un simple problème d’enregistrement des informations, on note qu’ils/elles sont nombreu.se.s à présenter une forme « légère » de la maladie. L’information pouvant être donnée ici par la mutation du gène. Comme il n’est pas recommandé de supprimer ou de traiter ces observations comme des censures à droite non informative, on peut les appréhender comme un risque concurrent au décès ou à tout autre évènement analysé à partir de ce registre (voir section dédiée).\n\nLes graphiques suivant représentent, en temps calendaire et après sa transformation en durée, la logique des censures à droite. Le recueil des informations est ici de nature prospectives.\n\nTrait plein : durée observée\nPointillés : durée censurés\nBulle : moment de l’évènement"
  },
  {
    "objectID": "Theorie.html#censure-à-gauche-troncature-et-censure-par-intervalle",
    "href": "Theorie.html#censure-à-gauche-troncature-et-censure-par-intervalle",
    "title": "La théorie",
    "section": "Censure à gauche, troncature et censure par intervalle",
    "text": "Censure à gauche, troncature et censure par intervalle\nCensure à gauche\nL’évènement s’est produit avant le début période d’observation. Typique des données prospectives, de type registre, avec des âges d’inclusion différenciés. \nCensure par intervalle Un évènement peut avoir lieux entre 2 temps d’observations sans qu’on puisse les observer (ex: en criminologie récidive d’un delit entre deux arrestations). Un phénomène de censure à droite avec perdu.e de vue peut se transformer en censure par intervalle lorsque la personne réapparait et est de nouveau incluse dans les données.  Troncature\nPar l’exemple, on analyse la survie d’une population. Seule la survie des individus vivants à l’inclusion peut être analysée (troncature à gauche). On peut également trouver un phénomène de troncature lorsqu’on mesure la durée à partir ou jusqu’à un certain seuil niveau.\n\nCes situations sont généralement plutôt bien contrôlées dans les recueils rétrospectifs. Elles sont assez courantes lorsque le recueil est de type prospectif.\n\nDurée d’observation supérieure à la durée d’exposition\nA l’inverse des individus peuvent sortir de l’exposition avant la fin de la période d’observation, et il convient donc de corriger la durée de cette sortie.\nUn exemple simple : si au moment de l’enquête une femme sans enfant a 70 ans, cela n’a pas de sens de continuer de l’exposer au risque au-delà d’un certain âge. Si on ne dispose pas d’information sur l’âge à la ménopause on peut tronquer la durée un peu au-delà de l’âge le plus élevé à la première naissance observée dans les données."
  },
  {
    "objectID": "Theorie.html#les-grandeurs-utilisées",
    "href": "Theorie.html#les-grandeurs-utilisées",
    "title": "La théorie",
    "section": "Les grandeurs utilisées",
    "text": "Les grandeurs utilisées\nLa fonction de survie \\(S(t)\\)\nLa fonction de répartition \\(F(t)\\)\nLa fonction de densité \\(f(t)\\)\nLe risque “instantané” \\(h(t)\\)\nLe risque “instantané” cumulé \\(H(t)\\)\n\nRemarques:\n\nToutes ces grandeurs sont mathématiquement liées les unes par rapport aux autres. En connaître une permet d’obtenir les autres.\n\nAu niveau formel on se placera ici du point de vue où la durée mesurée est strictement continue. Cela se traduit, entre autre, par l’absence d’évènements dits “simultanés”.\nLes expressions qui vont suivre ne sont pas des techniques de calcul, mais des grandeurs dont on précisera seulement les propriétés."
  },
  {
    "objectID": "Theorie.html#la-fonction-de-survie-st",
    "href": "Theorie.html#la-fonction-de-survie-st",
    "title": "La théorie",
    "section": "La fonction de Survie \\(S(t)\\)",
    "text": "La fonction de Survie \\(S(t)\\)\nDans ce type d’analyse, il est courant d’analyser la courbe de survie (ou de séjour).\nLa fonction de survie donne la proportion de la population qui n’a pas encore connue l’évènement après une certaine durée \\(t\\). Elle y a “survécu”.\nFormellement, la fonction de survie est la probabilité de survivre au-delà de \\(t\\), soit:\n\n\\[S(t) = P(T>t)\\]\n Propriétés: \\(S(0)=1\\) et \\(\\lim\\limits_{t\\to{\\propto}}S(t)=0\\)\n La fonction de survie est strictement non croissante."
  },
  {
    "objectID": "Theorie.html#la-fonction-de-répartition-ft",
    "href": "Theorie.html#la-fonction-de-répartition-ft",
    "title": "La théorie",
    "section": "La fonction de répartition \\(F(t)\\)",
    "text": "La fonction de répartition \\(F(t)\\)\nC’est la probabilité de connaitre l’évènement jusqu’en \\(t\\), soit:\n\n\\[F(t)=P(T\\leq{t})\\]\n\\(t\\geq{0}\\) Soit \\(F(t) = 1 - S(t)\\)\n La fonction de survie et la fonction de répartition sont deux grandeurs strictement complémentaires.\n Propriétés: \\(F(0)=0\\) et \\(\\lim\\limits_{t\\to{\\propto}}F(t)=1\\)"
  },
  {
    "objectID": "Theorie.html#la-fonction-de-densité-ft",
    "href": "Theorie.html#la-fonction-de-densité-ft",
    "title": "La théorie",
    "section": "La fonction de densité \\(f(t)\\)",
    "text": "La fonction de densité \\(f(t)\\)\n\nPour une valeur de \\(t\\) donnée, la fonction de densité de l’évènement donne la distribution des moments où les évènement ont eu lieu. Elle est donnée dans un premier temps par la probabilité de connaitre l’évènement dans un petit intervalle de temps \\(dt\\). Si \\(dt\\) est proche de 0 (temps continu) alors cette probabilité tend également vers 0. On norme donc cette probabilité par \\(dt\\). Rappel: on est toujours ici dans la théorie.\n\nEn temps continu, la fonction de densité est donnée par la dérivée de la fonction de répartition: \\(f(t)=F'(t)=-S'(t)\\).  Formellement la fonction de densité \\(f(t)\\) s’écrit:\n\n\\[f(t)=\\frac{P(t\\leq{T}< t + dt)}{dt}\\]"
  },
  {
    "objectID": "Theorie.html#le-risque-instantané-ht",
    "href": "Theorie.html#le-risque-instantané-ht",
    "title": "La théorie",
    "section": "Le risque instantané \\(h(t)\\)",
    "text": "Le risque instantané \\(h(t)\\)\nConcept fondamental de l’analyse des durées:\n\\[h(t)=\\frac{P(t\\leq{T}< t + dt | T\\geq{t})}{dt}\\] \n\n\\(P(t\\leq{T}< t + dt | T\\geq{t})\\) donne la probabilité de survenue de l’évènement sur l’intervalle \\([t,t+dt[\\) conditionnellement à la survie au temps \\(t\\).\n\nLa quantité obtenue donne alors un nombre moyen d’évènements que connaîtrait un individu durant une unité de temps choisie.\nA priori cette quantité n’est pas une probabilité. C’est la nature de l’évènement, en particulier sa non récurrence, et la métrique temporelle choisie ou disponible qui peut la rendre assimilable à une probabilité. Tout comme la densité, on est plutôt dans la définition d’un taux (d’où l’expression hazard rate en anglais).\n\n On peut également écrire: \\(h(t)=\\frac{f(t)}{S(t)}=\\frac{F'(t)}{S(t)}=-\\frac{S'(t)}{S(t)}\\)\n\nOn voit ici clairement que la fonction de risque n’est pas une probabilité : \\(\\frac{f(t)}{S(t)}\\) ne peut pas contraindre la valeur à ne pas être supérieure à 1."
  },
  {
    "objectID": "Theorie.html#le-risque-cumulé-ht",
    "href": "Theorie.html#le-risque-cumulé-ht",
    "title": "La théorie",
    "section": "Le risque cumulé \\(H(t)\\)",
    "text": "Le risque cumulé \\(H(t)\\)\nLe risque cumulé est égal à : \\(H(t)=\\int_{0}^{t} h(u)du = -log(S(t))\\)\n On peut alors le réécrire toutes les autres quantités:\n\n\n\\(S(t)=e^{-H(t)}\\)\n\n\\(F(t)=1-e^{-H(t)}\\)\n\n\\(f(t)=h(t)\\times{e^{-H(t)}}\\)\n\n Exemple avec la loi exponentiel (risque constant)\nSi on pose que le risque est strictement constant au cours du temps: \\(h(t)=a\\) (on parle de loi exponentielle - cf partie sur les modèles AFT - typique des processus sans mémoire comme la durée de vie des ampoules):\n\n\\(h(t)=a\\)\n\n\\(H(t)=a\\times{t}\\)\n\n\\(S(t)=e^{-a\\times{t}}\\)\n\n\\(F(t)=1-e^{-a\\times{t}}\\)\n\n\\(f(t)=a\\times{e^{-a\\times{t}}}\\)\n\n\nApplication: risque et échelles temporelles:\n Fortement inspiré, pour ne pas dire copié, de l’excellent cours de Gilbert Colletaz: https://www.univ-orleans.fr/deg/masters/ESA/GC/sources/Econometrie%20des%20Donnees%20de%20Survie.pdf\nAttention on sort ici très clairement du temps continu, il s’agit seulement de manipuler les concepts et de voir la dépendance de la mesure du risque à l’échelle temporelle choisie/disponible.\n\n\nDurant les mois d’hiver, entre le 1er janvier et le 1er avril (3 mois), la probabilité d’attraper un rhume chaque mois est de 48% (il s’agit bien d’un risque). Quelle est le risque d’attraper le rhume durant la saison froide?\n\\(\\frac{0.48}{1/3}=1.44\\). On peut donc s’attendre a attraper 1.44 rhume durant la période d’hiver.\nOn passe une année en vacances dans une région où la probabilité de décéder chaque mois est évaluée à 33%. Quelle est le risque de décéder pendant cette année sabbatique? \\(\\frac{0.33}{1/12}=3.96\\)\n\n\nLe risque peut donc être supérieur à 1 (c’est donc plutôt un taux tel qu’on le défini généralement). En soit cela ne pose pas de problème comme il s’agit d’un nombre moyen d’évènements espérés (exemple: “taux” de fécondité), mais pour des évènements qui ne peuvent pas se répéter, évènements dits “absorbants”, l’interprétation n’est pas très intuitive.\n On peut donc prendre l’inverse du risque qui mesure la durée moyenne (espérée) jusqu’à l’occurence de l’évènement.\nOn retrouve donc un concept classique en analyse démographique comme l’espérance de vie (survie): la question n’est pas de savoir si “on” va mourir ou non, le risque indépendement du temps étant par définition égal à 1, mais jusqu’à quand on peut espérer survivre.\n - Pour le rhume, la durée moyenne est de \\(1/1.44=0.69\\) du trimestre hivernal, soit approximativement le début du mois de mars. - Pour l’année sabbatique, la durée moyenne de survie (l’espérance de vie) est de \\(1/3.96=0.25\\) d’une année soit 3 mois après l’arrivée dans la région.\n\nExercice \n\nOn a une population de 100 cochons d’Inde.\nOn analyse leur mortalité (naturelle).\n\nIci l’analyse est en temps discret.\nLa durée représente le nombre d’année de vie.\nIl n’y a pas de censure à droite.\n\n\n\n\nDurée\nNombre de décès\n\n\n\n\n1\n1\n\n\n2\n1\n\n\n3\n3\n\n\n4\n9\n\n\n5\n30\n\n\n6\n40\n\n\n7\n10\n\n\n8\n3\n\n\n9\n2\n\n\n10\n1\n\n\n\nN=100\nA quel âge le risque de mourir des cochons d’Inde est-il le plus élevé? Quelle est la valeur de ce risque?"
  },
  {
    "objectID": "Theorie.html#forme-des-fonctions-de-survie",
    "href": "Theorie.html#forme-des-fonctions-de-survie",
    "title": "La théorie",
    "section": "Forme des fonctions de survie",
    "text": "Forme des fonctions de survie\nUne des propriétés de la fonction de survie ou de séjour est qu’elles tendent vers 0. A la lecture du graphique suivant, cela peut correspondre à la forme de la courbe S2, bien que le % de survivant tend à baisser de moins en moins à mesure que la durée augmente. Deux cas limites doivent être considéré.\n \n\nS1: très peu d’évènements et la fonction de séjour suit une asymptote nettement supérieur à 0 ( \\(\\lim\\limits_{t\\to{\\propto}}S(t)=a\\) avec \\(a>0\\)). La question est plus délicate car on interroge l’exposition au risque d’une partie de l’échantillon ou, dit autrement on peut penser qu’une fraction est immunisé au risque. Cette problématique est rapidement posée en fin de formation.\nS2: la situation attendue\nS3: La survie tombe à 0 très/trop rapidement: il n’y a donc pas ou presque pas de durée (par exemple presque tout l’échantillon observe l’évènement la première année de l’exposition). Les méthodes en temps continue ne sont a priori pas adaptées à ce genre de situation. Si on dispose d’une information plus fine pour dater les évènements, la fonction de séjour pourra reprendre une forme plus “standard”. Dans le graphique, \\(S(t=1)=0.4\\) , \\(S(t=2)=0.025\\), mais si on dispose par exemple de 10 points d’observations supplémentaires dans chaque durée groupée:"
  },
  {
    "objectID": "Theorie.html#absence-de-censures-à-droites",
    "href": "Theorie.html#absence-de-censures-à-droites",
    "title": "La théorie",
    "section": "Absence de censures à droites",
    "text": "Absence de censures à droites\nLes méthodes qui vont être présentées gèrent la présence de censures à droite. En leur absence, elle restent néanmoins parfaitement valables. L’absence de censure facilite certaines analyses, par exemple celles des fonctions de séjour où le calcul direct des durées moyennes est rendu possible."
  },
  {
    "objectID": "Theorie.html#utilisation-des-pondérations",
    "href": "Theorie.html#utilisation-des-pondérations",
    "title": "La théorie",
    "section": "Utilisation des pondérations",
    "text": "Utilisation des pondérations\nUne question assez récurrente concerne l’utilisation des poids de sondage dans les analyses de durées avec longueurs biographiques souvent assez longues. Appartenant à l’école du bon sens (Eva Levièvre), leur utilisation ne me semble pas recommandée voire à exclure sauf exceptions. En effet les pondérations sont générées au moment de l’enquête, alors que les évènements étudiés peuvent remonter dans un passé plus ou moins lointain pour une partie de la population analysée. Si on regarde de plus près, la création de poids longitudinaux ne résoudrait pas grand chose ,les pondérations devant être recalculées à chaque moment d’observation ou à chaque moment où des évènements se produisent. Par ailleurs on mélangerait à un instant donné des personnes issues de générations différentes ce qui rend impossible tout calage sur des caractéristiques d’un population. Supposons une personne âgée de 25 ans et un personne âgée de 70 ans au moment de l’enquête en 2022, avec un début d’observation à l’âge de 18 ans . A 20 ans (\\(t=2\\)), pour la première personne les caractéristiques de la population sont celles de 2017, pour celle de 70 ans celles de 1972. On fait comment??????"
  },
  {
    "objectID": "docs/theorie.html",
    "href": "docs/theorie.html",
    "title": "La théorie",
    "section": "",
    "text": "L’analyse des durées peut être vue comme l’étude d’une variable aléatoire \\(T\\) qui décrit la durée d’attente jusqu’à l’occurence d’un évènement.\nLa principale caractéristique de l’analyse des durées est le traitement des informations dites censurées, lorsque la durée d’observation est plus courte que la durée d’exposition au risque."
  },
  {
    "objectID": "docs/theorie.html#censure-à-droite",
    "href": "docs/theorie.html#censure-à-droite",
    "title": "La théorie",
    "section": "Censure à droite",
    "text": "Censure à droite\nDéfinition\n Certains individus n’auront pas (encore) connu l’évènement à la date de l’enquête après une certaine durée d’exposition. On a donc besoin d’un marqueur permettant de déterminer que les individus n’ont pas observé l’évènement sur la période d’étude.\nPourquoi une information est-elle censurée (à droite)?\n\nFin de l’étude, date de l’enquête.\nPerdu de vue, décès si autre évènement étudié.\n\nEn pratique (important)\n\nNe pas exclure ces observations, sinon on surestime la survenue de l’évènement.\nNe pas les considérer a-priori comme sorties de l’exposition sans avoir connu l’évènement. Elles peuvent connaître l’évènement après la date de l’enquête ou en étant perdues de vue. Sinon on sous-estime la durée moyenne de survenue de l’évènement.\n\nExemple\nOn effectue une enquête auprès de femmes : On souhaite mesurer l’âge à la première naissance. Au moment de l’enquête, une femme est âgée de 29 ans n’a pas (encore) d’enfant.\nCette information sera dite «censurée».\nElle est clairement encore soumise au risque après la date de l’enquête. Au niveau de l’analyse, elle sera soumise au risque à partir de ses premières règles jusqu’au moment de l’enquête.\n\nHypothèse fondamentale\nLes observations censurées ont vis à vis du phénomène observé le même comportement que les observations non censurées. On dit que la censure est non informative. Elle ne dépend pas de l’évènement analysé. Normalement le problème ne se pose pas dans les recueil retrospectif. \nProblème posé par la censure informative\nPar exemple en analysant des décès avec un recueil prospectif, si un individu est perdu de vue en raison d’une dégradation de son état de santé, l’indépendance entre la cause de la censure et le décès ne peut plus être assurée.\nA l’Ined l’exploitation du registre des personnes atteintes de mucoviscidose (G.Bellis) donne une autre illustration de ce phénomène. Chaque année un nombre significatif de personnes sortent du registre (pas de résultats aux examens annuels). Si certain.e.s perdu.e.s de vue s’expliquent par des déménagements, émigration ou par un simple problème d’enregistrement des informations, on note qu’ils/elles sont nombreu.se.s à présenter une forme « légère » de la maladie. L’information pouvant être donnée ici par la mutation du gène. Comme il n’est pas recommandé de supprimer ou de traiter ces observations comme des censures à droite non informative, on peut les appréhender comme un risque concurrent au décès ou à tout autre évènement analysé à partir de ce registre (voir section dédiée).\n\nLes graphiques suivant représentent, en temps calendaire et après sa transformation en durée, la logique des censures à droite. Le recueil des informations est ici de nature prospectives.\n\nTrait plein : durée observée\nPointillés : durée censurés\nBulle : moment de l’évènement"
  },
  {
    "objectID": "docs/theorie.html#censure-à-gauche-troncature-et-censure-par-intervalle",
    "href": "docs/theorie.html#censure-à-gauche-troncature-et-censure-par-intervalle",
    "title": "La théorie",
    "section": "Censure à gauche, troncature et censure par intervalle",
    "text": "Censure à gauche, troncature et censure par intervalle\nCensure à gauche\nL’évènement s’est produit avant le début période d’observation. Typique des données prospectives, de type registre, avec des âges d’inclusion différenciés. \nCensure par intervalle Un évènement peut avoir lieux entre 2 temps d’observations sans qu’on puisse les observer (ex: en criminologie récidive d’un delit entre deux arrestations). Un phénomène de censure à droite avec perdu.e de vue peut se transformer en censure par intervalle lorsque la personne réapparait et est de nouveau incluse dans les données.  Troncature\nPar l’exemple, on analyse la survie d’une population. Seule la survie des individus vivants à l’inclusion peut être analysée (troncature à gauche). On peut également trouver un phénomène de troncature lorsqu’on mesure la durée à partir ou jusqu’à un certain seuil niveau.\n\nCes situations sont généralement plutôt bien contrôlées dans les recueils rétrospectifs. Elles sont assez courantes lorsque le recueil est de type prospectif.\n\nDurée d’observation supérieure à la durée d’exposition\nA l’inverse des individus peuvent sortir de l’exposition avant la fin de la période d’observation, et il convient donc de corriger la durée de cette sortie.\nUn exemple simple : si au moment de l’enquête une femme sans enfant a 70 ans, cela n’a pas de sens de continuer de l’exposer au risque au-delà d’un certain âge. Si on ne dispose pas d’information sur l’âge à la ménopause on peut tronquer la durée un peu au-delà de l’âge le plus élevé à la première naissance observée dans les données."
  },
  {
    "objectID": "docs/theorie.html#les-grandeurs-utilisées",
    "href": "docs/theorie.html#les-grandeurs-utilisées",
    "title": "La théorie",
    "section": "Les grandeurs utilisées",
    "text": "Les grandeurs utilisées\nLa fonction de survie \\(S(t)\\)\nLa fonction de répartition \\(F(t)\\)\nLa fonction de densité \\(f(t)\\)\nLe risque “instantané” \\(h(t)\\)\nLe risque “instantané” cumulé \\(H(t)\\)\n\nRemarques:\n\nToutes ces grandeurs sont mathématiquement liées les unes par rapport aux autres. En connaître une permet d’obtenir les autres.\n\nAu niveau formel on se placera ici du point de vue où la durée mesurée est strictement continue. Cela se traduit, entre autre, par l’absence d’évènements dits “simultanés”.\nLes expressions qui vont suivre ne sont pas des techniques de calcul, mais des grandeurs dont on précisera seulement les propriétés."
  },
  {
    "objectID": "docs/theorie.html#la-fonction-de-survie-st",
    "href": "docs/theorie.html#la-fonction-de-survie-st",
    "title": "La théorie",
    "section": "La fonction de Survie \\(S(t)\\)",
    "text": "La fonction de Survie \\(S(t)\\)\nDans ce type d’analyse, il est courant d’analyser la courbe de survie (ou de séjour).\nLa fonction de survie donne la proportion de la population qui n’a pas encore connue l’évènement après une certaine durée \\(t\\). Elle y a “survécu”.\nFormellement, la fonction de survie est la probabilité de survivre au-delà de \\(t\\), soit:\n\n\\[S(t) = P(T>t)\\]\n Propriétés: \\(S(0)=1\\) et \\(\\lim\\limits_{t\\to{\\propto}}S(t)=0\\)\n La fonction de survie est strictement non croissante."
  },
  {
    "objectID": "docs/theorie.html#la-fonction-de-répartition-ft",
    "href": "docs/theorie.html#la-fonction-de-répartition-ft",
    "title": "La théorie",
    "section": "La fonction de répartition \\(F(t)\\)",
    "text": "La fonction de répartition \\(F(t)\\)\nC’est la probabilité de connaitre l’évènement jusqu’en \\(t\\), soit:\n\n\\[F(t)=P(T\\leq{t})\\]\n\\(t\\geq{0}\\) Soit \\(F(t) = 1 - S(t)\\)\n La fonction de survie et la fonction de répartition sont deux grandeurs strictement complémentaires.\n Propriétés: \\(F(0)=0\\) et \\(\\lim\\limits_{t\\to{\\propto}}F(t)=1\\)"
  },
  {
    "objectID": "docs/theorie.html#la-fonction-de-densité-ft",
    "href": "docs/theorie.html#la-fonction-de-densité-ft",
    "title": "La théorie",
    "section": "La fonction de densité \\(f(t)\\)",
    "text": "La fonction de densité \\(f(t)\\)\n\nPour une valeur de \\(t\\) donnée, la fonction de densité de l’évènement donne la distribution des moments où les évènement ont eu lieu. Elle est donnée dans un premier temps par la probabilité de connaitre l’évènement dans un petit intervalle de temps \\(dt\\). Si \\(dt\\) est proche de 0 (temps continu) alors cette probabilité tend également vers 0. On norme donc cette probabilité par \\(dt\\). Rappel: on est toujours ici dans la théorie.\n\nEn temps continu, la fonction de densité est donnée par la dérivée de la fonction de répartition: \\(f(t)=F'(t)=-S'(t)\\).  Formellement la fonction de densité \\(f(t)\\) s’écrit:\n\n\\[f(t)=\\frac{P(t\\leq{T}< t + dt)}{dt}\\]"
  },
  {
    "objectID": "docs/theorie.html#le-risque-instantané-ht",
    "href": "docs/theorie.html#le-risque-instantané-ht",
    "title": "La théorie",
    "section": "Le risque instantané \\(h(t)\\)",
    "text": "Le risque instantané \\(h(t)\\)\nConcept fondamental de l’analyse des durées:\n\\[h(t)=\\frac{P(t\\leq{T}< t + dt | T\\geq{t})}{dt}\\] \n\n\\(P(t\\leq{T}< t + dt | T\\geq{t})\\) donne la probabilité de survenue de l’évènement sur l’intervalle \\([t,t+dt[\\) conditionnellement à la survie au temps \\(t\\).\n\nLa quantité obtenue donne alors un nombre moyen d’évènements que connaîtrait un individu durant une unité de temps choisie.\nA priori cette quantité n’est pas une probabilité. C’est la nature de l’évènement, en particulier sa non récurrence, et la métrique temporelle choisie ou disponible qui peut la rendre assimilable à une probabilité. Tout comme la densité, on est plutôt dans la définition d’un taux (d’où l’expression hazard rate en anglais).\n\n On peut également écrire: \\(h(t)=\\frac{f(t)}{S(t)}=\\frac{F'(t)}{S(t)}=-\\frac{S'(t)}{S(t)}\\)\n\nOn voit ici clairement que la fonction de risque n’est pas une probabilité : \\(\\frac{f(t)}{S(t)}\\) ne peut pas contraindre la valeur à ne pas être supérieure à 1."
  },
  {
    "objectID": "docs/theorie.html#le-risque-cumulé-ht",
    "href": "docs/theorie.html#le-risque-cumulé-ht",
    "title": "La théorie",
    "section": "Le risque cumulé \\(H(t)\\)",
    "text": "Le risque cumulé \\(H(t)\\)\nLe risque cumulé est égal à : \\(H(t)=\\int_{0}^{t} h(u)du = -log(S(t))\\)\n On peut alors le réécrire toutes les autres quantités:\n\n\n\\(S(t)=e^{-H(t)}\\)\n\n\\(F(t)=1-e^{-H(t)}\\)\n\n\\(f(t)=h(t)\\times{e^{-H(t)}}\\)\n\n Exemple avec la loi exponentiel (risque constant)\nSi on pose que le risque est strictement constant au cours du temps: \\(h(t)=a\\) (on parle de loi exponentielle - cf partie sur les modèles AFT - typique des processus sans mémoire comme la durée de vie des ampoules):\n\n\\(h(t)=a\\)\n\n\\(H(t)=a\\times{t}\\)\n\n\\(S(t)=e^{-a\\times{t}}\\)\n\n\\(F(t)=1-e^{-a\\times{t}}\\)\n\n\\(f(t)=a\\times{e^{-a\\times{t}}}\\)\n\n\nApplication: risque et échelles temporelles:\n Fortement inspiré, pour ne pas dire copié, de l’excellent cours de Gilbert Colletaz: https://www.univ-orleans.fr/deg/masters/ESA/GC/sources/Econometrie%20des%20Donnees%20de%20Survie.pdf\nAttention on sort ici très clairement du temps continu, il s’agit seulement de manipuler les concepts et de voir la dépendance de la mesure du risque à l’échelle temporelle choisie/disponible.\n\n\nDurant les mois d’hiver, entre le 1er janvier et le 1er avril (3 mois), la probabilité d’attraper un rhume chaque mois est de 48% (il s’agit bien d’un risque). Quelle est le risque d’attraper le rhume durant la saison froide?\n\\(\\frac{0.48}{1/3}=1.44\\). On peut donc s’attendre a attraper 1.44 rhume durant la période d’hiver.\nOn passe une année en vacances dans une région où la probabilité de décéder chaque mois est évaluée à 33%. Quelle est le risque de décéder pendant cette année sabbatique? \\(\\frac{0.33}{1/12}=3.96\\)\n\n\nLe risque peut donc être supérieur à 1 (c’est donc plutôt un taux tel qu’on le défini généralement). En soit cela ne pose pas de problème comme il s’agit d’un nombre moyen d’évènements espérés (exemple: “taux” de fécondité), mais pour des évènements qui ne peuvent pas se répéter, évènements dits “absorbants”, l’interprétation n’est pas très intuitive.\n On peut donc prendre l’inverse du risque qui mesure la durée moyenne (espérée) jusqu’à l’occurence de l’évènement.\nOn retrouve donc un concept classique en analyse démographique comme l’espérance de vie (survie): la question n’est pas de savoir si “on” va mourir ou non, le risque indépendement du temps étant par définition égal à 1, mais jusqu’à quand on peut espérer survivre.\n - Pour le rhume, la durée moyenne est de \\(1/1.44=0.69\\) du trimestre hivernal, soit approximativement le début du mois de mars. - Pour l’année sabbatique, la durée moyenne de survie (l’espérance de vie) est de \\(1/3.96=0.25\\) d’une année soit 3 mois après l’arrivée dans la région.\n\nExercice \n\nOn a une population de 100 cochons d’Inde.\nOn analyse leur mortalité (naturelle).\n\nIci l’analyse est en temps discret.\nLa durée représente le nombre d’année de vie.\nIl n’y a pas de censure à droite.\n\n\n\n\nDurée\nNombre de décès\n\n\n\n\n1\n1\n\n\n2\n1\n\n\n3\n3\n\n\n4\n9\n\n\n5\n30\n\n\n6\n40\n\n\n7\n10\n\n\n8\n3\n\n\n9\n2\n\n\n10\n1\n\n\n\nN=100\nA quel âge le risque de mourir des cochons d’Inde est-il le plus élevé? Quelle est la valeur de ce risque?"
  },
  {
    "objectID": "docs/theorie.html#forme-des-fonctions-de-survie",
    "href": "docs/theorie.html#forme-des-fonctions-de-survie",
    "title": "La théorie",
    "section": "Forme des fonctions de survie",
    "text": "Forme des fonctions de survie\nUne des propriétés de la fonction de survie ou de séjour est qu’elles tendent vers 0. A la lecture du graphique suivant, cela peut correspondre à la forme de la courbe S2, bien que le % de survivant tend à baisser de moins en moins à mesure que la durée augmente. Deux cas limites doivent être considéré.\n \n\nS1: très peu d’évènements et la fonction de séjour suit une asymptote nettement supérieur à 0 ( \\(\\lim\\limits_{t\\to{\\propto}}S(t)=a\\) avec \\(a>0\\)). La question est plus délicate car on interroge l’exposition au risque d’une partie de l’échantillon ou, dit autrement on peut penser qu’une fraction est immunisé au risque. Cette problématique est rapidement posée en fin de formation.\nS2: la situation attendue\nS3: La survie tombe à 0 très/trop rapidement: il n’y a donc pas ou presque pas de durée (par exemple presque tout l’échantillon observe l’évènement la première année de l’exposition). Les méthodes en temps continue ne sont a priori pas adaptées à ce genre de situation. Si on dispose d’une information plus fine pour dater les évènements, la fonction de séjour pourra reprendre une forme plus “standard”. Dans le graphique, \\(S(t=1)=0.4\\) , \\(S(t=2)=0.025\\), mais si on dispose par exemple de 10 points d’observations supplémentaires dans chaque durée groupée:"
  },
  {
    "objectID": "docs/theorie.html#absence-de-censures-à-droites",
    "href": "docs/theorie.html#absence-de-censures-à-droites",
    "title": "La théorie",
    "section": "Absence de censures à droites",
    "text": "Absence de censures à droites\nLes méthodes qui vont être présentées gèrent la présence de censures à droite. En leur absence, elle restent néanmoins parfaitement valables. L’absence de censure facilite certaines analyses, par exemple celles des fonctions de séjour où le calcul direct des durées moyennes est rendu possible."
  },
  {
    "objectID": "docs/theorie.html#utilisation-des-pondérations",
    "href": "docs/theorie.html#utilisation-des-pondérations",
    "title": "La théorie",
    "section": "Utilisation des pondérations",
    "text": "Utilisation des pondérations\nUne question assez récurrente concerne l’utilisation des poids de sondage dans les analyses de durées avec longueurs biographiques souvent assez longues. Appartenant à l’école du bon sens (Eva Levièvre), leur utilisation ne me semble pas recommandée voire à exclure sauf exceptions. En effet les pondérations sont générées au moment de l’enquête, alors que les évènements étudiés peuvent remonter dans un passé plus ou moins lointain pour une partie de la population analysée. Si on regarde de plus près, la création de poids longitudinaux ne résoudrait pas grand chose ,les pondérations devant être recalculées à chaque moment d’observation ou à chaque moment où des évènements se produisent. Par ailleurs on mélangerait à un instant donné des personnes issues de générations différentes ce qui rend impossible tout calage sur des caractéristiques d’un population. Supposons une personne âgée de 25 ans et un personne âgée de 70 ans au moment de l’enquête en 2022, avec un début d’observation à l’âge de 18 ans . A 20 ans (\\(t=2\\)), pour la première personne les caractéristiques de la population sont celles de 2017, pour celle de 70 ans celles de 1972. On fait comment??????"
  }
]