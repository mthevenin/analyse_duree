{
  "hash": "3eaa33534876165cbd8f2d66d9c4c040",
  "result": {
    "markdown": "---\nfilters:\n  - lightbox\nlightbox: auto\n---\n\n\n\n# **Annexes**\n\n\n## Tests Grambsch-Therneau OLS sur les résidus de Schoenfeld\n\n::: callout-important\nAttention il ne s'agit pas du test actuellement implémenté dans la nouvelle version de `survival` (v3) qui, malheureusement, lui a substitué la version dite *exacte* (moindres carrés généralisés). Le programme de la fonction  du test OLS est néanmoins facilement récupérable et exécutable. [lien](https://github.com/mthevenin/analyse_duree/blob/main/cox.zphold/cox.zphold.R).  \n\nJe continue de préconiser l'utilisation de cette version OLS du test, reproductible avec les autres applications statistiques (Stata,Sas,Python).\n:::\n\n* Le test dit \"simplifié\", qui n'apparait pas dans le texte original de P.Gramsch et T.Thernau [lien](https://www.jstor.org/stable/2337123#metadata_info_tab_contents), répond à un soucis d'instabilité des variances des résidus de Schoenfeld en fin de durée d'observation lorsque peu d'observation restent soumises au risque. Cet argument est soulevé dans leur ouvrage de 2022  [lien](https://link.springer.com/book/10.1007/978-1-4757-3294-8) avant d'en présenter sa version.\n* Il est simplifié car on applique à tous les résidus bruts la variance du paramètre ($b$) estimés par le modèle de Cox. \n* Le test devient alors un simple test de corrélation entre les résidus et une fonction de la durée (centrée). Dans l'esprit, il peut être également approché par une regression linéaire par les moindre carrés ordinaires entre les résidus et une fonction de la durée (voir page 134 de l'ouvrage de Grambsch et Therneau).\n\n\nSoit les données suivantes, avec *t* la variable de durées, *Y* la variable de censure et *X* la seule et unique covariable.\n\n* Pas d'évènement simultané (donc pas de correction de la vraisemblance)\n* Covariable de type indicatrice\n\n| $t_i$ | $Y_i$ | $X_i$ |\n|-----|-----|-----|\n| 1   | 1   | 1   |\n| 2   | 0   | 0   |\n| 3   | 0   | 0   |\n| 4   | 1   | 1   |\n| 5   | 1   | 1   |\n| 6   | 1   | 0   |\n| 7   | 0   | 1   |\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest = data.frame(time=  c(1,2,3,4,5,6,7),\n                    Y=c(1,0,0,1,1,1,0),\n                    X=     c(1,0,0,1,1,0,1))\n```\n:::\n\n\nEstimation du modèle de Cox:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(survival)\nfit = coxph(formula = Surv(time, Y) ~ X, data=test)\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\ncoxph(formula = Surv(time, Y) ~ X, data = test)\n\n    coef exp(coef) se(coef)    z     p\nX 0.6217    1.8622   1.1723 0.53 0.596\n\nLikelihood ratio test=0.31  on 1 df, p=0.5797\nn= 7, number of events= 4 \n```\n:::\n:::\n\n\nCalcul des résidus brut (si et seulement si $Y=1$) dans le cas d'une seule covariable avec $b$ égal à **0.62**:  \n\n$$rs_{i}=X_{i}- \\sum_{j\\in R_i}X_{i}\\frac{e^{0.62\\times X}}{\\sum_{j\\in R_i}e^{0.62\\times X}}= X_{i} - E(X_{j\\in R_i})$$\nIl y a ici 4 résidus à calculer, pour $t=(1,4,5,6)$   \n\n**Résidus pour $t=1$**    \n\n* $a_1= \\sum_{j\\in R_i}e^{0.62\\times X} = e^{0.62} + 1 + 1 + e^{0.62} + 1 + e^{0.62}= 10.43$\n* $b_1= \\sum_{j\\in R_i}X_{i}\\frac{e^{0.62\\times X}}{\\sum_{j\\in R_i}e^{0.62\\times X}} = 4\\times\\frac{e^{0.62}}{10.43} = 0.71$ \n* $r_1 = 1 - 0.71 = 0.29$\n\n**Résidus pour $t=4$**  \n\n* $a_4 = e^{0.62} + e^{0.62} + 1 + e^{0.62} = 6.58$\n* $b_4 =  4\\times\\frac{e^{0.62}}{6.58} = 0.84$ \n* $r_4 = 1 - 0.84 = 0.15$\n\n**Résidus pour $t=5$**  \n\n* $a_5 = e^{0.62} + e^{0.62} + 1 = 4.71$\n* $b_5 = 2\\times\\frac{e^{0.62}}{4.71} = 0.78$\n* $r_5 = 1 - 0.78 = 0.21$\n\n**Résidus pour $t=6$**  \n\n* $a_6 = e^{0.62} + 1 = 2.86$\n* $b_6 = \\frac{e^{0.62}}{2.86} = 0.65$\n* $r_6 = 0 - 0.65 = -0.65$\n\nLes résidus \"standardisés\", ou plutôt *scaled residuals* (je cale sur une traduction correcte en français) sont égaux à: \n\n$$sr_i = b + nd \\times Var(b) \\times r_i$$\n Avec $nd= \\sum Y_i$\n \n  \n* $\\sum Y_i = 4$\n* $Var(b) = (1.1723)^2=1.37$\n\n* $sr_1 = 0.62 + 4\\times 1.37 \\times 0.29 = 2.20$\n* $sr_4 = 0.62 + 4\\times 1.37 \\times 0.15 = 1.47$\n* $sr_5 = 0.62 + 4\\times 1.37 \\times 0.21 = 1.78$\n* $sr_6 = 0.62 + 4\\times 1.37 \\times (-0.65) = -2.95$\n\n\nAvec $g(t_i)$ une fonction de la durée ($g(t_i)=t_i$, $g(t_i)=1-KM(t_i)$...) et $\\overline{g(t)}$ sa valeur moyenne, la statistique  du test score simplifié pour une covariable est égale à :   \n\n$$\\frac{[\\sum_i(g(t_i) - \\overline{g(t_i)}\\times sr_i)]^2}{nd \\times Var(b) \\times (\\sum_i(g(t_i) - \\overline{g(t_i)})^2}$$\nEt suis un $\\chi^2$ à 1 degré de liberté.  \n\nAvec $\\overline{g(t_i)}=t_i$, le calcul de la statistique de test est:  \n\n*  $\\overline{g(t_i)}= \\frac{28}{7}=4$\n\n* $\\frac{[(1-4)\\times 2.20] + [(4-4)\\times 1.47 + (5-4)\\times 1.78 + (6-4)\\times (-2.95)]^2 }{4\\times 1.37 \\times [(1-4)^2 + (4-4)^2 + (5-4)^2 + (6-4)^2] } = \\frac{114.9}{76.72} = 1.49$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#source(\"D:/D/Marc/SMS/FORMATIONS/analyse_duree/cox.zphold/cox.zphold.R\")\n\nsource(\"https://raw.githubusercontent.com/mthevenin/analyse_duree/master/cox.zphold/cox.zphold.R\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncox.zphold(fit, transform=\"identity\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     rho chisq     p\nX -0.688  1.49 0.222\n```\n:::\n:::\n\n\n\n\n## Fragilité et immunité\n\n\nSeulement quelques remarques, le traitement de ces problématiques dépassant largement le contenu de la formation.\n\n### Fragilité (Frailty)\n\nPour la *fragilité*, je conseille fortement de lire la dernière section du document de travail de ***Simon Quantin** (cf bibliographie), il n'y a pas meilleure présentation du problème que la sienne ^[petite maj par rapport à la version précédente: il ne traite que la fragilité individuelle stricto sensu et non la fragilité plus connu sous le terme de *shared frailty* (proche modèle multiniveau). Problèmatique importante, car une des origines de la non proportionnalité des risques réside dans l’omission de variables. Ici on va être confronté une omission sur des traits non observables ou latents, qui **accélèrent** dès le début de la période d’exposition la survenue de l’évènement. L’introduction d’un facteur de fragilité se fait par l’introduction d’un effet aléatoire dans le modèle, de nature plus complexe, et rendant l’interprétation des modèles plus compliquée. \n\nOn peut distinguer deux types de modèles:\n\n- les modèles à *fragilité partagée*, c'est la situation la plus simple car la logique se rapproche des modèles multiniveaux, des groupes d'individus, identifiables, partagent une même *fragilité*, par exemple  géographique.\n- les modèles à *fragilité non partagée*, avec des caractéristiques latentes non observable comme les préférences, ou en médecine certains traits génétiques non identifiés.\n\n### Immunité (Cure fraction)\n\nLe phénomène d’immunité est un cas particulier du précédent, et a été étudié dès le début des années 1950, en questionnant l’exposition au risque d’une partie des observations. On s'interrogeait par exemple sur les risques de rechute et de décès après le traitement d'un premier cancer. \nVisuellement on peut commencer à se proser des questions sur la présence d'une fraction *immunisée* ou non *susceptible* de connaître l'évènement lorsque la fonction de séjour ne tend pas vers 0 mais présente une longue asymptote (plateau) sur une valeur  supérieure à 0: $\\lim_{t \\to \\infty}S(t)=a$. \n\nLes modèles avec une fraction immunisée peuvent être de ***type mixte*** en associant une probabilité d'être immunisé aux observations censurées à droite à un modèle de durée ^[Le plus classique utilise un algorithme *Expectation Maximisation* utilisé en imputation: on estime une probabilité d'être susceptible de connaitre l'évènement aux observations censurées à droite, qui intervient comme facteur de pondération dans le modèle de durée. Cette probabilité et le modèle de durée qui lui est associé est réévalué à chaque boucle de l'algorithme jusqu'à convergence. Le principale problème de cette méthode résite dans l'estimation de la variance, souvent effectué par bootstrap. Cette méthode à l'avantage d'être implémentable en durée discrète, bien qu'à ma connaissance aucun logiciel ne la propose (j'ai une commande Stata encore perfectible sous le coude). On trouve en revanche ce type d'estimation sous R, pour les modèles de Cox ou les modèles paramétriques dans le package **`smcure`**].  Plus dans le vent je crois, on a également des modèles de **type non mixte**, avec il me semble une connotation bayesienne qui semble s'accroître.  Il n’y a donc pas de méthode unifiée à ce jour [[Si vous voulez vous en convaincre](https://www.annualreviews.org/doi/10.1146/annurev-statistics-031017-100101)]. \n\n On peut également noter, c'est important, que cette problématique affecte les analyses avec des évènements dits récurrents. Ici, la stratégie classique qui consiste à introduire dans un modèle un simple effet aléatoire de type fragilité partagée (shared frailty) pour contrôler risque d'être insuffisante. Ici le *groupe* est constitué de chaque séquence de remise dans le risque set. Exemple pour la fécondité: une personne ayant eu un enfant est exposée au risque d'en avoir un autre, l'horloge temporelle étant alors simplement réinitialisée. Et donc, quid des préférences individuelles en terme de fécondité ^[en situation de récurrence, toujours penser à *remettre à jour* les conditions initiales, par exemple pour la fécondite l'âge de la mère à la naissance de l'enfant pour les rang supérieur à 1]. \n\nEnfin, les modèles à fragilité ou à fraction immunisée repose tous sur une hypothèse très forte. La fragilité ou le degré d'immunité est toujours défini (estimé) en début d'exposition, et il ne varie pas. Cela peut ne pas toujours faire sens, en particulier pour les préférences, pas forcément stables ou fixes dans le temps.\n\n\n## Exemple d'analyse de durées ***Canada Dry***\n\nEn remettant en cause l'utilisation abusive de concepts empruntés à l'analyse de survie dans une étude publiée récemment, ce qui suit pourra paraître un peu polémique. Dans ce qui suit, il s'agit seulement de contester l'utilisation abusive d'une terminologie très typée et conditionnée au respect de certaines hypothèses de base, et non de remettre en cause l'ensemble des résultats obtenus, bien qu'à la lecture des quelques informations descriptives contenues dans l'étude ont peu s'interroger sur la présence assez substentielles d'affet de sélection...Ce qui n'est pas étonnant vu la manière dont a été créée l'échantillon.\nL'étude en question [[Lien](https://www.cairn.info/revue-population-2022-3-page-411.htm)] mobilise des données de suivi de stock, à savoir l'EDP (Echantillon Démographique Permanent). L'étude cherche à mettre à profit l'ajout par l'Insee de données fiscales à cet échantillon pour analyser la fécondité des femmes nullipares **au sein des couples**.  L'inclusion de cette données fiscale à cet échantillon a été initiée pour l'année 2011.\n\n***Sélection initiale de l'échantillon sur la première année d'inclusion***\nEn sélectionnant l'échantillon de départ sur 2011, année d'ajout d'une nouvelle information, aucune durée d'exposition ne peut être  clairement définie pour cette première année. Ces le béaba des analyses de durée/survie, une origine commune doit être clairement définie ce qui ne peut pas être ici. Cette origine commune permet lors du suivi de construire une population initiale soumise au risque sur la ***même ligne de départ***^[Ceci ayant été écrit pendant des mondiaux d'athlétisme, dont les épreuves de courses sont pas définition des épreuves de durée, on pourra dire ici que les *faux départs* sont généralisés et autorisés]. Il n'y avait pas d'autre choix compte tenu de la problématique de prendre l'année de mise en couple (cohabitant) pour définir le point d'origine, et donc demarrer l'observation à l'année 2012. Cela aurait permis en comparant les données fiscales d'une année sur l'autre de repérer les femmes qui se sont mis en couple. Et c'est ce qui est fait pour les inclusions sur les années suivantes. \n\nOn peut noter que ce biais d'inclusion joue de manière différenciée selon l'âge de la femme en 2011: surement nul ou très faible pour les femmes les plus jeunes, et selon le rang de l'union, plus ou moins élevé pour les femmes plus âgées. Sur ce dernier point le rang de l'union qui n'est visiblement pas mesurable avec les données utilisées, constituait également une dimension de contrôle importante. \n\n***Sélection  les années suivantes (jusqu'en 2017)***  \nComme indiqué juste juste au dessus, les inclusions suivantes se font sur la base d'une origine commune à savoir l'année de mise en couple.\n\nAu final, on se retrouve avec un échantillon dont la logique d'inclusion repose sur des critères différents: ajout des données fiscales pour 2011, et pour les années suivantes les mises en couples observées durant l'année écoulée.\n\n***Une fausse variable de durée***\nEn conservant la mise en couple comme origine, et en analysant la fécondité des femmes nullipares sur les 5 premières années, une analyse de survie reposant sur un suivi sur les 5 premières années de vie en couple cohabitant était facilement réalisable. En retranchant au âges observés durant les années de suivi l'âge de 18 ans, l'autrice, je pense en allant piocher maladroitement et partiellement dans un ouvrage de méthodologie très connu (voir encadré ci-dessous) a été conduit à créer une pseudo variable de durée dont la longueur est artificielle, et ne permet pas de mesurer proprement les durées d'exposition observées. \n\n\n![](images/image22.png)\n\nAu final, et toujours sous l'angle revendiqué d'une analyse des durée ou de survie, aucune fonction de séjour sous jacente n'est correctement estimable, et donc par définition aucune interprétation en termes de risques instantané n'est possible. Sur le modèle présenté, la construction de la vraisemblance n'est pas compatible avec ce type d'analyse.\n\nMais malheureusement on peut lire:\n\n* \"***des modèles de risque instantané à temps discret***\" [page 419].\n\n*  En note de bas de page: ***Le risque instantané est la probabilité conditionnelle que l’événement survienne entre le moment t et t + Δt, sachant qu’il n’a pas eu lieu avant t***  [page 419].\n   * Cette définition est très légère. Le risque *instantané* n'est formellement une probabilité qu'avec des durées discrètes, ce qui est effectivement le cas ici. La conditionnalité repose sur  une fonction de séjour qui n'est pas ici correctement estimable, et c'est la relation fondamentale qui lie risque (hazard rate), survie et densité qui identifie la vraisemblance d'un modèle de durée ou de survie. Cette relation est incontournable et elle n'est pas respectée ici.\n\n* \"***Le deuxième modèle peut se passer de l’hypothèse des risques instantanés proportionnels***\" [page 420]. \n  - Comment peut on soulever l'hypothèse de proportionnalité lorsque les individus ne sont pas observé à des durées identiques, ????\n  - Dans l'article cette correction est faite également sur les variables dynamique. Il s'agit d'un emprunt à l'ouvrage de Singer et Willet. L'autrice n'en ait pas responsable, mais l'idée même de cette hypothèse est très contestable avec des covariables non fixes^[Avec une variable fixe une des méthodes de correction, via l'introduction d'une intéraction, consiste justement à la transformer en variable dynamique].   \n\n* \"***Le quatrième modèle enfin inclut l’interaction des revenus relatifs avec le temps d’exposition au risque***\" [page 421]. \n  * Il n'y a malheureusement pas de durée d'exposition au risque clairement défini dans l'étude. On peut néanmoins concéder que pour les couples dont la femme est très jeune (18-20 ans), l'inclusion en 2011 pourrait correspondre à l'année de mis en couple. \n  * Voir le point précédent pour l'introduction de cette intéraction.\n\n* \"***L’équation suivante représente le calcul du risque continu cumulé***\" [pages 421], alors qu'aucune fonction de survie, et donc de risque cumulé ne peut être correctement obtenu (je sais je me répète dans la répétition). Mais cela permet à l'autrice de justifier l'utilisation de la fonction de lien *complémentaire log-log*. Et donc de faire, sous un angle paramétrique un modèle une variante d'un modèle de Cox à durée discrète. Mais en présence de censure à gauche, massive dans l'étude, ou de troncature à gauche, il est fortement biaisé. \n\nAu final:  \nAprès de brefs échanges avec la rédaction en chef de l'Ined^[Nous étions 3 à avoir exprimé de l'étonnement sur le contenu de cet article], il a été convenu qu'il ne s'agissait pas en effet d'une analyse de durée ou de survie....Ouf!....  Mais que l'autrice, via une réponse peu inspirée d'un spécialiste de l'Ined sur les questions de fécondité, n'avait pas souhaité faire une analyse de ce type... Ha bon ^[Argument lunaire: l'autrice n'utilise pas la durée comme variable dépendante mais comme une simple covariable. Alors 1) en présence de censure à droite, c'est quand même compliquer d'introduire la durée directement comme variable dépendante, c'est le béaba et 2) l'introduction de la durée ou d'une fonction de celle-ci comme covariable ou variable d'ajustement est une caractéristique commune à toute type de modèle de durée de type paramétrique. Sur les modèles usuels, seul le modèle de Cox figure comme exception]...\n\n\n::: callout-warning \n\n### L'ouvrage de Singer et Willet \n\nClairement revendiqué dans l'article, la *stratégie méthodologique* repose sur une section du fameux ouvrage de J.Singer et J.Willet ***Applied longitudinal data analysis: Modeling change and event occurrence***. Considéré comme une véritable bible des modèles à durée discrètes, il a reposé plusieurs années dans mon bureau et donc été utilisé régulièrement. Il n'en reste pas moins exempt de défauts, avec un chapitrage peu conventionnel, un côté supermarché à modèles excessif, mais également des partis pris méthodologiques non discutés comme le test et la correction contestable de la non proportionnalité des risques pour des variables dynamiques. \n\nIl n'en reste pas moins, qu'à l'exception du dernier chapitre dédié aux temps continu, toutes la méthodologie et les applications sont réalisés dans le cadre stricte de la censure à droite non informative, et les auteurs prennent clairement le soins d'alerter les lecteurs des problèmes posés par présence de censure ou de troncature à gauche avec les données prospectives ou de suvi de stock, tel que l'EDP: \n\n> pages 320 et 321: ***In what follows, we typically assume that all censoring occurs on the right***. In section 15.6, however, we describe what you can do when your data set includes what are known as late entrants into the risk set. You are most likely to encounter late entrants if you study stock samples, age-heterogeneous groups of people who already occupy the initial state when data collection begins—for example, a random sample of adults who have yet to experience a depressive episode. Your plan is to follow everyone for a fixed period of time—say ten years— and record whether and when sample participants experience their first episode. Because each sample member was a different age when data collection began, however, the ten years you cover do not cover the same ten years of peoples’ lives: you follow the 20-year-olds until they are 30, the 21-year-olds until they are 31, the 22-year-olds until they are 32, and so on. For an outcome like depression onset, it makes little sense to clock “time” using chronological years (2009,2010, etc). Instead, you would like to clock “time” in terms of age, but you do not observe everyone during the identical set of ages.]\n\nOn trouvera également les programmes associés aux exemples de l'ouvrage [[ici](https://stats.oarc.ucla.edu/other/examples/alda/)]. \nOn trouvera dans l'ouvrage, mot pour mot, le modèle utilisé par l'autrice. Mais l'exemple utilisait des données retrospéctives avec une stricte censure à droite non informative^[On pourrait également questionner cet article paru dans Population, sur la nature non informative des ruptures d'union par rapport à la fécondité].\n\n:::\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}