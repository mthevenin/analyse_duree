{
  "hash": "1f5df8854954414106e5ed20ebc21fb3",
  "result": {
    "markdown": "---\nfilters:\n  - lightbox\nlightbox: auto\n---\n\n\n\n# **Annexes**\n\n\n## Tests Grambsch-Therneau OLS sur les résidus de Schoenfeld\n\n::: callout-important\nAttention il ne s'agit pas du test actuellement implémenté dans la nouvelle version de `survival` (v3) qui, malheureusement, lui a substitué la version dite *exacte* (moindres carrés généralisés). Le programme de la fonction  du test OLS est néanmoins facilement récupérable et exécutable. [lien](https://github.com/mthevenin/analyse_duree/blob/main/cox.zphold/cox.zphold.R).  \n\nJe continue de préconiser l'utilisation de cette version OLS du test, reproductible avec les autres applications statistiques (Stata,Sas,Python).\n:::\n\n* Le test dit \"simplifié\", qui n'apparait pas dans le texte original de P.Gramsch et T.Thernau [lien](https://www.jstor.org/stable/2337123#metadata_info_tab_contents), répond à un soucis d'instabilité des variances des résidus de Schoenfeld en fin de durée d'observation lorsque peu d'observation restent soumises au risque. Cet argument est soulevé dans leur ouvrage de 2022  [lien](https://link.springer.com/book/10.1007/978-1-4757-3294-8) avant d'en présenter sa version.\n* Il est simplifié car on applique à tous les résidus bruts la variance du paramètre ($b$) estimés par le modèle de Cox. \n* Le test devient alors un simple test de corrélation entre les résidus et une fonction de la durée (centrée). Dans l'esprit, il peut être également approché par une regression linéaire par les moindre carrés ordinaires entre les résidus et une fonction de la durée (voir page 134 de l'ouvrage de Grambsch et Therneau).\n\n\nSoit les données suivantes, avec *t* la variable de durées, *Y* la variable de censure et *X* la seule et unique covariable.\n\n* Pas d'évènement simultané (donc pas de correction de la vraisemblance)\n* Covariable de type indicatrice\n\n| $t_i$ | $Y_i$ | $X_i$ |\n|-----|-----|-----|\n| 1   | 1   | 1   |\n| 2   | 0   | 0   |\n| 3   | 0   | 0   |\n| 4   | 1   | 1   |\n| 5   | 1   | 1   |\n| 6   | 1   | 0   |\n| 7   | 0   | 1   |\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntest = data.frame(time=  c(1,2,3,4,5,6,7),\n                    Y=c(1,0,0,1,1,1,0),\n                    X=     c(1,0,0,1,1,0,1))\n```\n:::\n\n\nEstimation du modèle de Cox:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(survival)\nfit = coxph(formula = Surv(time, Y) ~ X, data=test)\nfit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCall:\ncoxph(formula = Surv(time, Y) ~ X, data = test)\n\n    coef exp(coef) se(coef)    z     p\nX 0.6217    1.8622   1.1723 0.53 0.596\n\nLikelihood ratio test=0.31  on 1 df, p=0.5797\nn= 7, number of events= 4 \n```\n:::\n:::\n\n\nCalcul des résidus brut (si et seulement si $Y=1$) dans le cas d'une seule covariable avec $b$ égal à **0.62**:  \n\n$$rs_{i}=X_{i}- \\sum_{j\\in R_i}X_{i}\\frac{e^{0.62\\times X}}{\\sum_{j\\in R_i}e^{0.62\\times X}}= X_{i} - E(X_{j\\in R_i})$$\nIl y a ici 4 résidus à calculer, pour $t=(1,4,5,6)$   \n\n**Résidus pour $t=1$**    \n\n* $a_1= \\sum_{j\\in R_i}e^{0.62\\times X} = e^{0.62} + 1 + 1 + e^{0.62} + 1 + e^{0.62}= 10.43$\n* $b_1= \\sum_{j\\in R_i}X_{i}\\frac{e^{0.62\\times X}}{\\sum_{j\\in R_i}e^{0.62\\times X}} = 4\\times\\frac{e^{0.62}}{10.43} = 0.71$ \n* $r_1 = 1 - 0.71 = 0.29$\n\n**Résidus pour $t=4$**  \n\n* $a_4 = e^{0.62} + e^{0.62} + 1 + e^{0.62} = 6.58$\n* $b_4 =  4\\times\\frac{e^{0.62}}{6.58} = 0.84$ \n* $r_4 = 1 - 0.84 = 0.15$\n\n**Résidus pour $t=5$**  \n\n* $a_5 = e^{0.62} + e^{0.62} + 1 = 4.71$\n* $b_5 = 2\\times\\frac{e^{0.62}}{4.71} = 0.78$\n* $r_5 = 1 - 0.78 = 0.21$\n\n**Résidus pour $t=6$**  \n\n* $a_6 = e^{0.62} + 1 = 2.86$\n* $b_6 = \\frac{e^{0.62}}{2.86} = 0.65$\n* $r_6 = 0 - 0.65 = -0.65$\n\nLes résidus \"standardisés\", ou plutôt *scaled residuals* (je cale sur une traduction correcte en français) sont égaux à: \n\n$$sr_i = b + nd \\times Var(b) \\times r_i$$\n Avec $nd= \\sum Y_i$\n \n  \n* $\\sum Y_i = 4$\n* $Var(b) = (1.1723)^2=1.37$\n\n* $sr_1 = 0.62 + 4\\times 1.37 \\times 0.29 = 2.20$\n* $sr_4 = 0.62 + 4\\times 1.37 \\times 0.15 = 1.47$\n* $sr_5 = 0.62 + 4\\times 1.37 \\times 0.21 = 1.78$\n* $sr_6 = 0.62 + 4\\times 1.37 \\times (-0.65) = -2.95$\n\n\nAvec $g(t_i)$ une fonction de la durée ($g(t_i)=t_i$, $g(t_i)=1-KM(t_i)$...) et $\\overline{g(t)}$ sa valeur moyenne, la statistique  du test score simplifié pour une covariable est égale à :   \n\n$$\\frac{[\\sum_i(g(t_i) - \\overline{g(t_i)}\\times sr_i)]^2}{nd \\times Var(b) \\times (\\sum_i(g(t_i) - \\overline{g(t_i)})^2}$$\nEt suis un $\\chi^2$ à 1 degré de liberté.  \n\nAvec $\\overline{g(t_i)}=t_i$, le calcul de la statistique de test est:  \n\n*  $\\overline{g(t_i)}= \\frac{28}{7}=4$\n\n* $\\frac{[(1-4)\\times 2.20] + [(4-4)\\times 1.47 + (5-4)\\times 1.78 + (6-4)\\times (-2.95)]^2 }{4\\times 1.37 \\times [(1-4)^2 + (4-4)^2 + (5-4)^2 + (6-4)^2] } = \\frac{114.9}{76.72} = 1.49$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#source(\"D:/D/Marc/SMS/FORMATIONS/analyse_duree/cox.zphold/cox.zphold.R\")\n\nsource(\"https://raw.githubusercontent.com/mthevenin/analyse_duree/master/cox.zphold/cox.zphold.R\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncox.zphold(fit, transform=\"identity\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     rho chisq     p\nX -0.688  1.49 0.222\n```\n:::\n:::\n\n\n\n\n## Fragilité et immunité\n\n\nSeulement quelques remarques, le traitement de ces problématiques dépassant largement le contenu de la formation.\n\n### Fragilité (Frailty)\n\nPour la *fragilité*, je conseille fortement de lire la dernière section du document de travail de ***Simon Quantin** (cf bibliographie), il n'y a pas meilleure présentation du problème que la sienne ^[petite maj par rapport à la version précédente: il ne traite que la fragilité individuelle stricto sensu et non la fragilité plus connu sous le terme de *shared frailty* (proche modèle multiniveau). Problèmatique importante, car une des origines de la non proportionnalité des risques réside dans l’omission de variables. Ici on va être confronté une omission sur des traits non observables ou latents, qui **accélèrent** dès le début de la période d’exposition la survenue de l’évènement. L’introduction d’un facteur de fragilité se fait par l’introduction d’un effet aléatoire dans le modèle, de nature plus complexe, et rendant l’interprétation des modèles plus compliquée. \n\nOn peut distinguer deux types de modèles:\n\n- les modèles à *fragilité partagée*, c'est la situation la plus simple car la logique se rapproche des modèles multiniveaux, des groupes d'individus, identifiables, partagent une même *fragilité*, par exemple  géographique.\n- les modèles à *fragilité non partagée*, avec des caractéristiques latentes non observable comme les préférences, ou en médecine certains traits génétiques non identifiés.\n\n### Immunité (Cure fraction)\n\nLe phénomène d’immunité est un cas particulier du précédent, et a été étudié dès le début des années 1950, en questionnant l’exposition au risque d’une partie des observations. On s'interrogeait par exemple sur les risques de rechute et de décès après le traitement d'un premier cancer. \nVisuellement on peut commencer à se proser des questions sur la présence d'une fraction *immunisée* ou non *susceptible* de connaître l'évènement lorsque la fonction de séjour ne tend pas vers 0 mais présente une longue asymptote (plateau) sur une valeur  supérieure à 0: $\\lim_{t \\to \\infty}S(t)=a$. \n\nLes modèles avec une fraction immunisée peuvent être de ***type mixte*** en associant une probabilité d'être immunisé aux observations censurées à droite à un modèle de durée ^[Le plus classique utilise un algorithme *Expectation Maximisation* utilisé en imputation: on estime une probabilité d'être susceptible de connaitre l'évènement aux observations censurées à droite, qui intervient comme facteur de pondération dans le modèle de durée. Cette probabilité et le modèle de durée qui lui est associé est réévalué à chaque boucle de l'algorithme jusqu'à convergence. Le principale problème de cette méthode résite dans l'estimation de la variance, souvent effectué par bootstrap. Cette méthode à l'avantage d'être implémentable en durée discrète, bien qu'à ma connaissance aucun logiciel ne la propose (j'ai une commande Stata encore perfectible sous le coude). On trouve en revanche ce type d'estimation sous R, pour les modèles de Cox ou les modèles paramétriques dans le package **`smcure`**].  Plus dans le vent je crois, on a également des modèles de **type non mixte**, avec il me semble une connotation bayesienne qui semble s'accroître.  Il n’y a donc pas de méthode unifiée à ce jour [[Si vous voulez vous en convaincre](https://www.annualreviews.org/doi/10.1146/annurev-statistics-031017-100101)]. \n\n Il est également à noter, c'est important, que cette problématique affecte les analyses avec des évènements dits récurrents. Ici, la stratégie classique qui consiste à introduire dans un modèle un simple effet aléatoire de type fragilité partagée (shared frailty) pour contrôler risque d'être insuffisante. Ici le *groupe* est constitué de chaque séquence de remise dans le risque. Exemple pour la fécondité: une personne ayant eu un enfant est exposée au risque d'en avoir un autre, l'horloge temporelle étant alors simplement réinitialisée. Et donc, quid des préférences individuelles en terme de fécondité ^[en situation de récurrence, toujours penser à *remettre à jour* des conditions initiales, par exemple pour la fécondite l'âge de la mère à la naissance de l'enfant pour les rang supérieur à 1]. \n\nEnfin, les modèles à fragilité ou à fraction immunisée repose tous sur une hypothèse très forte. La fragilité ou le degré d'immunité est toujours défini (estimé) en début d'exposition, et il ne varie pas. Cela peut ne pas toujours faire sens, en particulier pour les préférences, pas forcément stables ou fixes dans le temps.\n\n\n## Exemple d'analyse de durées ***Canada Dry***\n\n\n::: callout-important\nCe qui suit pourra apparaître un peu polémique. Il conteste, dans une étude publiée, l'utilisation abusive des concepts empruntés au type d'analyse développé dans ce support. Il ne juge ni de l'intérêt de l'étude, ni des résultats obtenus peut-être parfaitement valide dans une autre perspective. \nNous estimons, contrairement à ce qu'un chercheur de l'Ined a pu avancé, que l'autrice revendique pleinement l'utilisation d'une méthodologie empruntée à l'analyse de durée ou de survie.\n\nIl n'est par ailleurs pas conseillé de lire ce qui suit sans maîtriser a minima les concepts de bases en analyses de durée: durée d'exposition, censure et troncature, taux de risque etc....\n\n:::\n\nUne étude récente prétend reposer, par la terminologie utilisée, sur une analyse de durée a été publiée  dans la revue *Population* [[Lien](https://www.cairn.info/revue-population-2022-3-page-411.htm)]. Sans surprise les données mobilisées sont de type prospectives, à savoir l'EDP. L'étude cherche à mettre à profit l'ajout de données fiscales à cet échantillon pour analyser la fécondité des femmes nullipares au sein des couples. L'inclusion de ces données fiscale  débute pour l'année 2011, et la sélection de l'échantillon de départ a été faite sur cette année. Au final aucune durée d'exposition n'est clairement définie pour cette première année, alors que les inclusions pour les années suivantes (jusqu'en 2017) se font et de manière plutôt cohérente sur des nouveaux couples cohabitants. De même aucune variable de durée n'a été définie, forcément pour les inclusions de 2011 mais aussi pour les inclusions postérieures. On a simplement retranché 18 à l'âge de la personne de 2011 à 2017. Mais quel est réellement le début de l'exposition alors que c'est dans une situation de couple, dont on ne connait pas la durée en 2011 et qui a moins d'un an pour les inclusions suivante?\n\nPour les inclusions de 2011, le graphique ci-dessous décrit la construction de la durée construite dans l'article pour les femmes âgées de 18 à 26 ans: pas d'origine commune, pas de mesure de risque au sens de l'analyse de la survie, pas de fonction de séjour etc....\n\n![](images/image22.png)\n\n\nIci ce ne sont pas les résultats en soit qui posent vraiment problème, mais les termes utilisés lors de la présentation de la méthode et  tout du long de l'analyse des résultats.  \n\nOn peut lire par exemple dans l'article:\n\n* \"***des modèles de risque instantané à temps discret***\" [page 419].\n\n*  En note de bas de page: ***Le risque instantané est la probabilité conditionnelle que l’événement survienne entre le moment t et t + Δt, sachant qu’il n’a pas eu lieu avant t.^[Cette définition est très légère. Le risque instantané est lié à la fonction de survie comme cela a été présneté dans le chapitre \"Théorie\". Mais aucune fonction de séjour n'est ici estimable dans cet article]*** [page 419]\n\n* \"***Le deuxième modèle peut se passer de l’hypothèse des risques instantanés proportionnels***\" [page 420]. \n\n* \"***Le quatrième modèle enfin inclut l’interaction des revenus relatifs avec le temps d’exposition au risque***\" [page 421]. Il n'y a malheureusement pas de temps d'exposition au risque clairement défini dans l'étude. On peut néanmoins remarquer que pour les couples dont la femmes est très jeune (18-20 ans), l'inclusion en 2011 pourrait correspondre à l'année de mis en couple. \n\n* Dans le même ordre d'idée: \"***La spécification quadratique de la durée d’exposition indique une augmentation initiale de la fonction de risque...**\" [page 423]. \n\n* \"***L’équation suivante représente le calcul du risque continu cumulé***\", alors qu'aucune fonction de survie n'est estimable. Ce qui par ailleurs justifie l'utilisation d'une fonction de lien de type complémentaire log-log dans l'article (on passera sur le fait que les incidences sont quand même bien trop élevées dans l'article pour utiliser cette transformation, plutôt réservé à des situations de rareté/) [pages 421].\n\nAu final:  \nAprès de brefs échanges avec la rédaction en chef de l'Ined^[Nous étions 3 à avoir exprimé de l'étonnement sur le contenu de cet article], il a été convenu qu'il ne s'agissait pas en effet d'une analyse de durée ou de survie....Ouf!.... Mais que l'autrice, via une réponse peu inspirée^[D'un spécialiste de l'Ined sur les questions de fécondité...pas un.e méthodologue], n'avait pas souhaité faire une analyse de ce type... Ha bon?  Je laisserai juge à toute personne de se positionner sur cette dernière affirmation. \n\n::: callout-warning \n\n### L'ouvrage de Singer et Willet \n\nClairement présenté dans l'article, la *stratégie méthodologique* repose sur une section du fameux ouvrage de J.Singer et J.Willet ***Applied longitudinal data analysis: Modeling change and event occurrence***. Considéré comme une véritable bible des modèles à durée discrètes, il a reposé plusieurs années dans mon bureau et donc été utilisé régulièrement. Il n'en reste pas moins exempt de défauts, avec une structure peu conventionnelle, à côté supermarché à modèle excessif, mais également des partis pris méthodologiques peu discutés comme le test et la correction contestable de la non proportionnalité des risques pour des variables dynamiques. \n\nIl n'en reste pas moins, qu'à l'exception du dernier chapitre dédié aux temps continu, toutes la méthodologie et les applications sont réalisés dans le cadre stricte de la censure à droite non informative. Les auteurs avaient pourtant  pris le soins d'alerter très clairement les lecteurs des problèmes posés par présence de censure ou de troncature à gauche: \n\n> pages 320 et 321: ***In what follows, we typically assume that all censoring occurs on the right***. In section 15.6, however, we describe what you can do when your data set includes what are known as late entrants into the risk set. You are most likely to encounter late entrants if you study stock samples, age-heterogeneous groups of people who already occupy the initial state when data collection begins—for example, a random sample of adults who have yet to experience a depressive episode. Your plan is to follow everyone for a fixed period of time—say ten years— and record whether and when sample participants experience their first episode. Because each sample member was a different age when data collection began, however, the ten years you cover do not cover the same ten years of peoples’ lives: you follow the 20-year-olds until they are 30, the 21-year-olds until they are 31, the 22-year-olds until they are 32, and so on. For an outcome like depression onset, it makes little sense to clock “time” using chronological years (2009,2010, etc). Instead, you would like to clock “time” in terms of age, but you do not observe everyone during the identical set of ages.]\n\nOn trouvera également les programmes associés aux exemples de l'ouvrage [[ici](https://stats.oarc.ucla.edu/other/examples/alda/)]\n\n:::\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}