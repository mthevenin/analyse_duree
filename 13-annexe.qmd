---
filters:
  - lightbox
lightbox: auto
---


# **Annexes**


## Tests Grambsch-Therneau OLS sur les résidus de Schoenfeld

::: callout-important
Attention il ne s'agit pas du test actuellement implémenté dans la nouvelle version de `survival` (v3) qui, malheureusement, lui a substitué la version dite *exacte* (moindres carrés généralisés). Le programme de la fonction  du test OLS est néanmoins facilement récupérable et exécutable. [lien](https://github.com/mthevenin/analyse_duree/blob/main/cox.zphold/cox.zphold.R).  

Je continue de préconiser l'utilisation de cette version OLS du test, reproductible avec les autres applications statistiques (Stata,Sas,Python).
:::

* Le test dit "simplifié", qui n'apparait pas dans le texte original de P.Gramsch et T.Thernau [lien](https://www.jstor.org/stable/2337123#metadata_info_tab_contents), répond à un soucis d'instabilité des variances des résidus de Schoenfeld en fin de durée d'observation lorsque peu d'observation restent soumises au risque. Cet argument est soulevé dans leur ouvrage de 2022  [lien](https://link.springer.com/book/10.1007/978-1-4757-3294-8) avant d'en présenter sa version.
* Il est simplifié car on applique à tous les résidus bruts la variance du paramètre ($b$) estimés par le modèle de Cox. 
* Le test devient alors un simple test de corrélation entre les résidus et une fonction de la durée (centrée). Dans l'esprit, il peut être également approché par une regression linéaire par les moindre carrés ordinaires entre les résidus et une fonction de la durée (voir page 134 de l'ouvrage de Grambsch et Therneau).


Soit les données suivantes, avec *t* la variable de durées, *Y* la variable de censure et *X* la seule et unique covariable.

* Pas d'évènement simultané (donc pas de correction de la vraisemblance)
* Covariable de type indicatrice

| $t_i$ | $Y_i$ | $X_i$ |
|-----|-----|-----|
| 1   | 1   | 1   |
| 2   | 0   | 0   |
| 3   | 0   | 0   |
| 4   | 1   | 1   |
| 5   | 1   | 1   |
| 6   | 1   | 0   |
| 7   | 0   | 1   |

```{r}
test = data.frame(time=  c(1,2,3,4,5,6,7),
                    Y=c(1,0,0,1,1,1,0),
                    X=     c(1,0,0,1,1,0,1))
```

Estimation du modèle de Cox:

```{r}
library(survival)
fit = coxph(formula = Surv(time, Y) ~ X, data=test)
fit
```

Calcul des résidus brut (si et seulement si $Y=1$) dans le cas d'une seule covariable avec $b$ égal à **0.62**:  

$$rs_{i}=X_{i}- \sum_{j\in R_i}X_{i}\frac{e^{0.62\times X}}{\sum_{j\in R_i}e^{0.62\times X}}= X_{i} - E(X_{j\in R_i})$$
Il y a ici 4 résidus à calculer, pour $t=(1,4,5,6)$   

**Résidus pour $t=1$**    

* $a_1= \sum_{j\in R_i}e^{0.62\times X} = e^{0.62} + 1 + 1 + e^{0.62} + 1 + e^{0.62}= 10.43$
* $b_1= \sum_{j\in R_i}X_{i}\frac{e^{0.62\times X}}{\sum_{j\in R_i}e^{0.62\times X}} = 4\times\frac{e^{0.62}}{10.43} = 0.71$ 
* $r_1 = 1 - 0.71 = 0.29$

**Résidus pour $t=4$**  

* $a_4 = e^{0.62} + e^{0.62} + 1 + e^{0.62} = 6.58$
* $b_4 =  4\times\frac{e^{0.62}}{6.58} = 0.84$ 
* $r_4 = 1 - 0.84 = 0.15$

**Résidus pour $t=5$**  

* $a_5 = e^{0.62} + e^{0.62} + 1 = 4.71$
* $b_5 = 2\times\frac{e^{0.62}}{4.71} = 0.78$
* $r_5 = 1 - 0.78 = 0.21$

**Résidus pour $t=6$**  

* $a_6 = e^{0.62} + 1 = 2.86$
* $b_6 = \frac{e^{0.62}}{2.86} = 0.65$
* $r_6 = 0 - 0.65 = -0.65$

Les résidus "standardisés", ou plutôt *scaled residuals* (je cale sur une traduction correcte en français) sont égaux à: 

$$sr_i = b + nd \times Var(b) \times r_i$$
 Avec $nd= \sum Y_i$
 
  
* $\sum Y_i = 4$
* $Var(b) = (1.1723)^2=1.37$

* $sr_1 = 0.62 + 4\times 1.37 \times 0.29 = 2.20$
* $sr_4 = 0.62 + 4\times 1.37 \times 0.15 = 1.47$
* $sr_5 = 0.62 + 4\times 1.37 \times 0.21 = 1.78$
* $sr_6 = 0.62 + 4\times 1.37 \times (-0.65) = -2.95$


Avec $g(t_i)$ une fonction de la durée ($g(t_i)=t_i$, $g(t_i)=1-KM(t_i)$...) et $\overline{g(t)}$ sa valeur moyenne, la statistique  du test score simplifié pour une covariable est égale à :   

$$\frac{[\sum_i(g(t_i) - \overline{g(t_i)}\times sr_i)]^2}{nd \times Var(b) \times (\sum_i(g(t_i) - \overline{g(t_i)})^2}$$
Et suis un $\chi^2$ à 1 degré de liberté.  

Avec $\overline{g(t_i)}=t_i$, le calcul de la statistique de test est:  

*  $\overline{g(t_i)}= \frac{28}{7}=4$

* $\frac{[(1-4)\times 2.20] + [(4-4)\times 1.47 + (5-4)\times 1.78 + (6-4)\times (-2.95)]^2 }{4\times 1.37 \times [(1-4)^2 + (4-4)^2 + (5-4)^2 + (6-4)^2] } = \frac{114.9}{76.72} = 1.49$


```{r}
#source("D:/D/Marc/SMS/FORMATIONS/analyse_duree/cox.zphold/cox.zphold.R")

source("https://raw.githubusercontent.com/mthevenin/analyse_duree/master/cox.zphold/cox.zphold.R")

```

```{r}
cox.zphold(fit, transform="identity")
```



## Fragilité et immunité


Seulement quelques remarques, le traitement de ces problématiques dépassant largement le contenu de la formation.

### Fragilité (Frailty)

Pour la *fragilité*, je conseille fortement de lire la dernière section du document de travail de ***Simon Quantin** (cf bibliographie), il n'y a pas meilleure présentation du problème que la sienne ^[petite maj par rapport à la version précédente: il ne traite que la fragilité individuelle stricto sensu et non la fragilité plus connu sous le terme de *shared frailty* (proche modèle multiniveau). Problèmatique importante, car une des origines de la non proportionnalité des risques réside dans l’omission de variables. Ici on va être confronté une omission sur des traits non observables ou latents, qui **accélèrent** dès le début de la période d’exposition la survenue de l’évènement. L’introduction d’un facteur de fragilité se fait par l’introduction d’un effet aléatoire dans le modèle, de nature plus complexe, et rendant l’interprétation des modèles plus compliquée. 

On peut distinguer deux types de modèles:

- les modèles à *fragilité partagée*, c'est la situation la plus simple car la logique se rapproche des modèles multiniveaux, des groupes d'individus, identifiables, partagent une même *fragilité*, par exemple  géographique.
- les modèles à *fragilité non partagée*, avec des caractéristiques latentes non observable comme les préférences, ou en médecine certains traits génétiques non identifiés.

### Immunité (Cure fraction)

Le phénomène d’immunité est un cas particulier du précédent, et a été étudié dès le début des années 1950, en questionnant l’exposition au risque d’une partie des observations. On s'interrogeait par exemple sur les risques de rechute et de décès après le traitement d'un premier cancer. 
Visuellement on peut commencer à se proser des questions sur la présence d'une fraction *immunisée* ou non *susceptible* de connaître l'évènement lorsque la fonction de séjour ne tend pas vers 0 mais présente une longue asymptote (plateau) sur une valeur  supérieure à 0: $\lim_{t \to \infty}S(t)=a$. 

Les modèles avec une fraction immunisée peuvent être de ***type mixte*** en associant une probabilité d'être immunisé aux observations censurées à droite à un modèle de durée ^[Le plus classique utilise un algorithme *Expectation Maximisation* utilisé en imputation: on estime une probabilité d'être susceptible de connaitre l'évènement aux observations censurées à droite, qui intervient comme facteur de pondération dans le modèle de durée. Cette probabilité et le modèle de durée qui lui est associé est réévalué à chaque boucle de l'algorithme jusqu'à convergence. Le principale problème de cette méthode résite dans l'estimation de la variance, souvent effectué par bootstrap. Cette méthode à l'avantage d'être implémentable en durée discrète, bien qu'à ma connaissance aucun logiciel ne la propose (j'ai une commande Stata encore perfectible sous le coude). On trouve en revanche ce type d'estimation sous R, pour les modèles de Cox ou les modèles paramétriques dans le package **`smcure`**].  Plus dans le vent je crois, on a également des modèles de **type non mixte**, avec il me semble une connotation bayesienne qui semble s'accroître.  Il n’y a donc pas de méthode unifiée à ce jour [[Si vous voulez vous en convaincre](https://www.annualreviews.org/doi/10.1146/annurev-statistics-031017-100101)]. 

 On peut également noter, c'est important, que cette problématique affecte les analyses avec des évènements dits récurrents. Ici, la stratégie classique qui consiste à introduire dans un modèle un simple effet aléatoire de type fragilité partagée (shared frailty) pour contrôler risque d'être insuffisante. Ici le *groupe* est constitué de chaque séquence de remise dans le risque set. Exemple pour la fécondité: une personne ayant eu un enfant est exposée au risque d'en avoir un autre, l'horloge temporelle étant alors simplement réinitialisée. Et donc, quid des préférences individuelles en terme de fécondité ^[en situation de récurrence, toujours penser à *remettre à jour* les conditions initiales, par exemple pour la fécondite l'âge de la mère à la naissance de l'enfant pour les rang supérieur à 1]. 

Enfin, les modèles à fragilité ou à fraction immunisée repose tous sur une hypothèse très forte. La fragilité ou le degré d'immunité est toujours défini (estimé) en début d'exposition, et il ne varie pas. Cela peut ne pas toujours faire sens, en particulier pour les préférences, pas forcément stables ou fixes dans le temps.


## Exemple d'analyse de durées ***Canada Dry***

En remettant en cause l'utilisation abusive de concepts empruntés à l'analyse de survie dans une étude publiée récemment, ce qui suit peut paraître un peu polémique. En revanche il est important de préciser que les résultats obtenus avec une modélisation assez classique, ne sont pas, à quelques détails près, remis en cause. Il s'agit seulement de contester l'utilisation d'une terminologie très typée et conditionnée au respect de certaines hypothèses de base.  

Cette étude [[Lien](https://www.cairn.info/revue-population-2022-3-page-411.htm)] mobilise  des données de suivi, à savoir l'EDP (Echantillon Démographique Permanent). 
L'étude cherche à mettre à profit l'ajout par l'Insee de données fiscales à cet échantillon pour analyser la fécondité des femmes nullipares **au sein des couples**.  L'inclusion de cette données fiscale est initiée pour l'année 2011.

***Sélection initiale de l'échantillon sur la première année d'inclusion***
En sélectionnant l'échantillon de départ sur 2011, année d'ajout d'une nouvelle information, aucune durée d'exposition ne peut être  clairement définie pour cette première année. Ces le béaba des analyses de durée/survie, une origine commune doit être clairement définie ce qui ne peut pas être ici. Cette origine commune permet lors du suivi de construire une population initiale soumise au risque sur la ***même ligne de départ***^[Ceci ayant été écrit pendant des mondiaux d'athlétisme, dont les épreuves de courses sont pas définition des épreuves de durée, on pourra dire ici que les *faux départs* sont généralisés et autorisés]. Il n'y avait pas d'autre choix compte tenu de la problématique de prendre l'année de mise en couple (cohabitant) pour définir le point d'origine, et donc demarrer l'observation à l'année 2012. Cela aurait permis en comparant les données fiscales d'une année sur l'autre de repérer les femmes qui se sont mis en couple. Et c'est ce qui est fait pour les inclusions sur les années suivantes. 

On peut noter que ce biais d'inclusion joue de manière différenciée selon l'âge de la femme en 2011: surement nul ou très faible pour les femmes les plus jeunes, et selon le rang de l'union plus ou moins élevé pour les plus âgés. Sur ce dernier point le rang de l'union qui n'est peut-être pas mesurable avec les données utilisées, constitue surement une dimension de contrôle importante. 

***Sélection  les années suivantes (jusqu'en 2017)***  
Comme cela a été noter juste au-dessus, les inclusions suivantes se font sur la base d'une origine commune à savoir l'année de mise en couple.

Au final, on se retrouve donc avec un échantillon d'inclusion fait sur des critères différents: inclusions des nouvelles données pour 2011, et sur les mises en couples pour les années suivantes.

***Une fausse variable de durée***
En conservant la mise en couple comme origine, et en analysant la fécondité des femmes nullipares sur les 5 premières années, une analyse de survie reposant sur un suivi sur les 5 premières années de vie en couple cohabitant était facilement réalisable. L'autrice, je pense en allant piocher maladroitement et partiellement dans un ouvrage de méthodologie très connu (voir encadré ci-dessous), a été conduit à créer une pseudo variable de durée dont la longueur est artificielle et qui ne permet pas de mesurer proprement les durées d'exposition.
Elle a en effet retrancher au âges observés durant les années de suivi l'âge de 18 ans. 

Pour l'année 2011 et pour des âges d'inclusion allant de 18 à 26 ans, la construction de la durée est donc:

![](images/image22.png)

Au final, et toujours sous l'angle revendiqué d'une analyse des durée ou de survie, aucune fonction de séjour sous jacente n'est estimable, et donc par définition aucune interprétation en termes de risques instantané n'est admissible. Sur le modèle, la construction de la vraisemblance est incompatible avec ce type d'analyse.
Mais malheureusement on peut lire:

* "***des modèles de risque instantané à temps discret***" [page 419].

*  En note de bas de page: ***Le risque instantané est la probabilité conditionnelle que l’événement survienne entre le moment t et t + Δt, sachant qu’il n’a pas eu lieu avant t***  [page 419].
   * Cette définition est très légère. Le risque instantané est lié à la fonction de survie comme cela classiquement. Mais aucune fonction de séjour n'est ici estimable dans cet article, sachant que la survie est une probabilité. C'est la relation fondamentale qui lie risque (hazard rate), survie et densité qui permet de construire la vraisemblance d'un modèle de durée/survie. Cette relation est incontournable et elle n'est pas respectée ici.

* "***Le deuxième modèle peut se passer de l’hypothèse des risques instantanés proportionnels***" [page 420]. 
  - Comment peut on soulever l'hypothèse de proportionnalité lorsque les individus ne sont pas observé à des durées identique????
  - Dans l'article cette correction est faite également sur les variables dynamique. Il s'agit d'un emprunt à l'ouvrage de Singer et Willet. L'autrice n'en ait pas responsable, mais l'existence même de cette hypothèse très contestable avec des covariables non fixes^[Avec une variable fixe une des méthodes de correction, via l'introduction d'une intéraction, consiste justement à la transformaer en variable dynamique].  

* "***Le quatrième modèle enfin inclut l’interaction des revenus relatifs avec le temps d’exposition au risque***" [page 421]. 
  * Il n'y a malheureusement pas de temps d'exposition au risque clairement défini dans l'étude. On peut néanmoins remarquer que pour les couples dont la femme est très jeune (18-20 ans), l'inclusion en 2011 pourrait correspondre à l'année de mis en couple. 
  * Voir le point précédent pour l'introduction de cette intéraction .

* Dans le même ordre d'idée: "***La spécification quadratique de la durée d’exposition indique une augmentation initiale de la fonction de risque...**" [page 423]. Il n'y a malheureusement pas de fonction de risque estimable au sens de l'analyse de la survie.

* "***L’équation suivante représente le calcul du risque continu cumulé***" [pages 421], alors qu'aucune fonction de survie, et donc de risque cumulé ne peut être obtenu (je sais je me répète), mais cela permet à l'autrice de justifier l'utilisation de la fonction de lien *complémentaire log-log*. On pourra se reporter au cours de German Rodriguez pour une démonstration rigoureuse (lien en début de document). 


Au final:  
Après de brefs échanges avec la rédaction en chef de l'Ined^[Nous étions 3 à avoir exprimé de l'étonnement sur le contenu de cet article], il a été convenu qu'il ne s'agissait pas en effet d'une analyse de durée ou de survie....Ouf!....  Mais que l'autrice, via une réponse peu inspirée d'un spécialiste de l'Ined sur les questions de fécondité, n'avait pas souhaité faire une analyse de ce type ^[Argument lunaire: l'autrice n'utilise par la durée comme variable dépendante mais comme une simple covariable. Alors 1) en présence de censure à droite, c'est quand même compliquer d'introduire la durée directement comme variable dépendante et 2) l'introduction de la durée ou d'une fonction de celle-ci comme covariable ou variable d'ajustement est une caractéristique commune à toute type de moèle de durée de tpye parmétrique. Sur les modèles usuels, seul le modèle de Cox figure comme exception]... Ha bon?


::: callout-warning 

### L'ouvrage de Singer et Willet 

Clairement revendiqué dans l'article, la *stratégie méthodologique* repose sur une section du fameux ouvrage de J.Singer et J.Willet ***Applied longitudinal data analysis: Modeling change and event occurrence***. Considéré comme une véritable bible des modèles à durée discrètes, il a reposé plusieurs années dans mon bureau et donc été utilisé régulièrement. Il n'en reste pas moins exempt de défauts, avec un chapitrage peu conventionnel, un côté supermarché à modèles excessif, mais également des partis pris méthodologiques non discutés comme le test et la correction contestable de la non proportionnalité des risques pour des variables dynamiques. 

Il n'en reste pas moins, qu'à l'exception du dernier chapitre dédié aux temps continu, toutes la méthodologie et les applications sont réalisés dans le cadre stricte de la censure à droite non informative, et les auteurs prennent clairement le soins d'alerter très clairement les lecteurs des problèmes posés par présence de censure ou de troncature à gauche avec les données prospectives ou de suvi de stock, tel que l'EDP: 

> pages 320 et 321: ***In what follows, we typically assume that all censoring occurs on the right***. In section 15.6, however, we describe what you can do when your data set includes what are known as late entrants into the risk set. You are most likely to encounter late entrants if you study stock samples, age-heterogeneous groups of people who already occupy the initial state when data collection begins—for example, a random sample of adults who have yet to experience a depressive episode. Your plan is to follow everyone for a fixed period of time—say ten years— and record whether and when sample participants experience their first episode. Because each sample member was a different age when data collection began, however, the ten years you cover do not cover the same ten years of peoples’ lives: you follow the 20-year-olds until they are 30, the 21-year-olds until they are 31, the 22-year-olds until they are 32, and so on. For an outcome like depression onset, it makes little sense to clock “time” using chronological years (2009,2010, etc). Instead, you would like to clock “time” in terms of age, but you do not observe everyone during the identical set of ages.]

On trouvera également les programmes associés aux exemples de l'ouvrage [[ici](https://stats.oarc.ucla.edu/other/examples/alda/)]. 
On trouvera dans l'ouvrage, mot pour mot, le modèle utilisé par l'autrice. Mais l'exemple utilisait des données retrospéctives avec une stricte censure à droite non informative^[On pourrait également questionner cet article paru dans Population, sur la nature non informative des ruptures d'union par rapport à la fécondité].

:::



